{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog","text":""},{"location":"#azure","title":"Azure","text":"<ul> <li>Certificate Based Authentication</li> <li>Download Subscription Bill</li> <li>Download Azure Resource Listing</li> <li>Query Recovery Services Vault</li> </ul>"},{"location":"#cisco-meraki-nagios-xi","title":"Cisco Meraki &amp; Nagios XI","text":"<ul> <li>Cisco Meraki Devices Discovery and Nagios Integration</li> </ul>"},{"location":"#microsoft-365","title":"Microsoft 365","text":"<ul> <li>Outlook Actionable Adaptive Card - Part 1</li> <li>Outlook Actionable Adaptive Card - Part 2</li> <li>Sharepoint Sites Enumeration</li> <li>Sharepoint Document Library Enumeration</li> <li>Sharepoint Document Library Copy to Azure File Share</li> </ul>"},{"location":"#sap","title":"SAP","text":"<ul> <li>Setup PyRFC in your Container</li> <li>Concur Expense Report Aggregation</li> <li>Calling SAP RFC Function Modules from Python</li> </ul>"},{"location":"#ukg","title":"UKG","text":"<ul> <li>Secure File Transfer with UKG Dimensions</li> <li>Employee Verification via API</li> <li>Automating Integration Execution and Dataview extract</li> </ul>"},{"location":"#proof-point-essentials","title":"Proof Point Essentials","text":"<ul> <li>Proof Point User Management</li> </ul>"},{"location":"#active-directory-domain-services","title":"Active Directory Domain Services","text":"<ul> <li>Create User and assign groups</li> <li>Update User</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/","title":"Reviewing Consultants via Adaptive Card (Actionable Outlook Messages) \u2013 Part 1","text":"<p>In this article, we\u2019ll walk through a real-world Python implementation for reviewing consultants using Adaptive Cards in Outlook. This solution enables managers to receive an actionable email, review their consultants, and submit decisions directly from their inbox. We'll cover the end-to-end process, focusing on how to build and send an actionable Adaptive Card email using Python and Microsoft Graph.</p> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part1/#overview","title":"Overview","text":"<p>The workflow consists of the following steps:</p> <ol> <li>Fetch consultants grouped by manager using <code>fetch_manager_consultants</code>.</li> <li>Build an Adaptive Card for each manager using <code>create_adaptive_card_outlook</code>.</li> <li>Send the Adaptive Card email using <code>send_adaptive_card_email</code>.</li> </ol> <p>Let\u2019s dive into each step and the code behind it.</p>"},{"location":"adaptive-card-consultant-review-part1/#1-fetching-consultants-grouped-by-manager","title":"1. Fetching Consultants Grouped by Manager","text":"<p>The function <code>fetch_manager_consultants(frequency)</code> retrieves consultants from the database and groups them by their manager\u2019s email.</p> <pre><code>def fetch_manager_consultants(frequency):\n    \"\"\"Fetch consultants grouped by their manager's email.\"\"\"\n    get_consultants_sql_statement = get_consultants_sql(frequency)\n    consultants_data = execute_Select_SQL_statement(get_consultants_sql_statement)[0]\n    manager_to_consultants = {}\n\n    try:\n        for row in consultants_data:\n            manager_email = row[5]\n            consultant_info = {\n                \"in_adp\": row[0],\n                \"name\": row[1],\n                \"last_logon\": row[2],\n                \"email\": row[3],\n                \"last_password_change\": row[4],\n                \"hire_date\": row[6],\n            }\n            manager_to_consultants.setdefault(manager_email, []).append(consultant_info)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, Exception)\n\n    return manager_to_consultants\n</code></pre> <ul> <li>Key Points:</li> <li>The function queries the database for consultant data.</li> <li>It organizes consultants by their manager\u2019s email, returning a dictionary mapping each manager to their consultants.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#2-building-the-adaptive-card","title":"2. Building the Adaptive Card","text":"<p>The function <code>create_adaptive_card_outlook(manager_email, consultants)</code> constructs an Adaptive Card JSON payload for Outlook. This card allows managers to review each consultant and select an action (keep active or deactivate).</p> <pre><code>def create_adaptive_card_outlook(manager_email, consultants):\n    \"\"\"Create an Adaptive Card with consultant details and actions.\"\"\"\n    try:\n        manager_name = manager_email.split('@')[0].split('.')[0]  # Extract manager's first name\n        inputs = []\n        action_data = {}\n\n        for consultant in consultants:\n            consultant_id = consultant[\"email\"].replace(\"@\", \"_\").replace(\".\", \"_\")\n            last_logon = consultant['last_logon'] or 'N/A'\n            hire_date = consultant['hire_date'] or 'N/A'\n\n            inputs.extend([\n                {\n                    \"type\": \"TextBlock\",\n                    \"wrap\": True,\n                    \"weight\": \"Bolder\",\n                    \"color\": \"Warning\",\n                    \"spacing\": \"Medium\",\n                    \"text\": \"****\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"padding\": \"None\",\n                    \"spacing\": \"None\",\n                    \"items\": [\n                        {\n                            \"type\": \"TextBlock\",\n                            \"text\": f\"**{consultant['name']}** ({consultant['email']}), with Last Logon: {last_logon} and Hire Date: {hire_date}\",\n                            \"weight\": \"Bolder\",\n                            \"wrap\": True\n                        },\n                        {\n                            \"type\": \"Input.ChoiceSet\",\n                            \"id\": f\"decision_{consultant_id}\",\n                            \"isMultiSelect\": False,\n                            \"value\": \"keep\",\n                            \"choices\": [\n                                {\"title\": \"Keep Active\", \"value\": \"keep\"},\n                                {\"title\": \"Deactivate\", \"value\": \"deactivate\"}\n                            ],\n                            \"style\": \"expanded\",\n                            \"spacing\": \"None\",\n                        }\n                    ]\n                }\n            ])\n            action_data[consultant_id] = {\n                \"decision\": f\"{{{{decision_{consultant_id}.value}}}}\",\n                \"email\": consultant[\"email\"],\n                \"manageremail\": manager_email,\n                \"managername\": manager_name,\n            }\n\n        inputs.append({\n            \"type\": \"TextBlock\",\n            \"wrap\": True,\n            \"weight\": \"Bolder\",\n            \"color\": \"Warning\",\n            \"spacing\": \"Medium\",\n            \"text\": \"****\"\n        })\n\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.0\",\n            \"originator\": ORGINATOR_ID,\n            \"body\": [\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Consultant Review\",\n                    \"weight\": \"bolder\",\n                    \"size\": \"extraLarge\",\n                    \"color\": \"attention\",\n                    \"separator\": True,\n                    \"horizontalAlignment\": \"center\",\n                    \"spacing\": \"small\",\n                    \"wrap\": True\n                },\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": (\n                        f\"Hello {manager_name}, please review the details of your consultants and select the appropriate action. \"\n                        \"Some consultants may not be in the HR system as they were set up directly as Guest accounts, so their hire date will show as N/A. \"\n                        \"Please review all consultants and provide feedback so that appropriate action can be taken if they no longer require network access.\"\n                    ),\n                    \"wrap\": True,\n                    \"color\": \"Default\",\n                    \"spacing\": \"Medium\",\n                    \"weight\": \"Bolder\"\n                }\n            ] + inputs,\n            \"actions\": [\n                {\n                    \"type\": \"Action.Http\",\n                    \"title\": \"Submit Consultant Actions\",\n                    \"headers\": [\n                        {\"name\": \"Content-Type\", \"value\": \"application/json\"},\n                        {\"name\": \"Authorization\", \"value\": \"\"}\n                    ],\n                    \"method\": \"POST\",\n                    \"url\": \"https://api.example.com/consultant-review-confirmation\",\n                    \"body\": \"\"\n                }\n            ],\n            \"style\": \"default\"\n        }\n\n        # Prepare the action data for the body\n        action_data_str = json.dumps(action_data)\n        adaptive_card['actions'][0]['body'] = action_data_str\n\n        email_payload = {\n            \"message\": {\n            \"subject\": \"Consultant Review - Action Required\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n            \"bccRecipients\": [{\"emailAddress\": {\"address\": \"audit@example.com\"}}]\n            }\n        }\n\n        return email_payload\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        return None\n</code></pre> <ul> <li>Key Points:</li> <li>The card is dynamically built for each manager and their consultants.</li> <li>Each consultant has a choice set for the manager to select \"Keep Active\" or \"Deactivate\".</li> <li>The card is embedded in the email as a <code>&lt;script type='application/adaptivecard+json'&gt;...&lt;/script&gt;</code> block, which is required for actionable messages in Outlook.</li> <li>The card uses an <code>Action.Http</code> action to POST the manager\u2019s decisions to a specified endpoint.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#3-sending-the-adaptive-card-email","title":"3. Sending the Adaptive Card Email","text":"<p>The function <code>send_adaptive_card_email(email_payload)</code> sends the constructed Adaptive Card email using the Microsoft Graph API.</p> <pre><code>def send_adaptive_card_email(email_payload):\n    \"\"\"Send an email with an embedded Adaptive Card using Microsoft Graph API.\"\"\"\n    try:\n        user_id = \"your-user-guid\"\n        graph_api_url = f\"https://graph.microsoft.com/v1.0/users/{user_id}/sendMail\"\n        access_token = get_access_token_API_Access_AAD()\n\n        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n\n        response = requests.post(graph_api_url, json=email_payload, headers=headers)\n\n        if response.status_code == 202:\n            print(\"Email sent successfully!\")\n        else:\n            print(f\"Failed to send email: {response.status_code}, {response.text}\")\n\n    except requests.exceptions.RequestException as req_error:\n        handle_global_exception(sys._getframe().f_code.co_name, req_error)\n        print(f\"Request error occurred: {req_error}\")\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        print(f\"An unexpected error occurred: {error}\")\n</code></pre> <ul> <li>Key Points:</li> <li>The function authenticates using an Azure AD access token.</li> <li>It sends the email via the Microsoft Graph <code>/sendMail</code> endpoint.</li> <li>The Adaptive Card is delivered as an actionable message in Outlook.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#end-to-end-example","title":"End-to-End Example","text":"<p>Here\u2019s how you might orchestrate the process:</p> <pre><code>def process_consultants(frequency):\n    manager_to_consultants = fetch_manager_consultants(frequency)\n    for manager_email, consultants in manager_to_consultants.items():\n        email_payload = create_adaptive_card_outlook(manager_email, consultants)\n        if email_payload:\n            send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part1/#conclusion","title":"Conclusion","text":"<p>This article (Part 1) demonstrated how to:</p> <ul> <li>Fetch consultants grouped by manager.</li> <li>Build an Adaptive Card for actionable review in Outlook.</li> <li>Send the Adaptive Card email using Microsoft Graph.</li> </ul> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part2/","title":"Company-Agnostic Adaptive Card Consultant Review Blog (Part 2, Deep Dive)","text":""},{"location":"adaptive-card-consultant-review-part2/#introduction","title":"Introduction","text":"<p>In Part 1, we covered how to send actionable Adaptive Card emails for consultant review. In this Part 2, we focus on the backend: how to securely receive, verify, and process the manager's response when the Adaptive Card is submitted. This article recursively examines each function involved in the request processing chain, providing a complete, end-to-end understanding of how an incoming Adaptive Card request is handled\u2014with all relevant Python code included and all company-specific references replaced with generic placeholders (e.g., <code>mycompany.com</code>).</p>"},{"location":"adaptive-card-consultant-review-part2/#1-endpoint-consultant-review-confirmation","title":"1. Endpoint: <code>/consultant-review-confirmation</code>","text":"<p>When a manager submits the Adaptive Card, the card's action posts the data to the <code>/consultant-review-confirmation</code> endpoint:</p> <pre><code>@app.route('/consultant-review-confirmation', methods=['POST'])\ndef consultant_review_confirmation():\n    try:\n        payload, client_ip, error_response, status_code = process_request_headers_and_payload(request)\n        if error_response:\n            return error_response, status_code\n        process_adaptive_card_payload(payload, client_ip)\n        return jsonify({\"status\": \"success\", \"message\": \"Actions processed successfully\"}), 200\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>This route does two things: 1. Verifies the request and extracts the payload using <code>process_request_headers_and_payload</code>. 2. Processes the submitted data using <code>process_adaptive_card_payload</code>.</p>"},{"location":"adaptive-card-consultant-review-part2/#2-deep-dive-process_request_headers_and_payload","title":"2. Deep Dive: <code>process_request_headers_and_payload</code>","text":"<p>This function is responsible for: - Extracting and logging request headers. - Validating the JWT Bearer token in the <code>Action-Authorization</code> header. - Decoding the token and verifying its authenticity. - Extracting the JSON payload from the request.</p> <pre><code>def process_request_headers_and_payload(request):\n    headers = dict(request.headers)\n    logger.info(f\"Request headers: {headers}\")\n    action_auth_header = headers.get(\"Action-Authorization\", \"\")\n    client_ip = headers.get(\"X-Forwarded-For\", \"\")\n    logger.info(f\"Incoming request from IP: {client_ip}\")\n    logger.info(f\"Action Authorization: {action_auth_header}\")\n    if not action_auth_header.startswith(\"Bearer \"):\n        logger.error(f\"Missing or invalid Bearer token in Action-Authorization header from {client_ip}\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Missing Bearer token\"}), 401\n    token = action_auth_header.split(\" \", 1)[1]\n    log_jwt_payload(token)\n    public_key = fetch_public_key(token)\n    if not public_key:\n        logger.error(\"Public key not found!\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    if not validate_token(token, public_key):\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    payload = request.get_json()\n    logger.info(f\"Payload: {payload}\")\n    return payload, client_ip, None, None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#21-log_jwt_payloadtoken","title":"2.1. <code>log_jwt_payload(token)</code>","text":"<p>Logs the decoded JWT payload (without verifying the signature) for debugging and traceability.</p> <pre><code>def log_jwt_payload(token):\n    \"\"\"Logs the decoded JWT payload without verification.\"\"\"\n    payload = jwt.decode(token, options={\"verify_signature\": False})\n    for key, value in payload.items():\n        logger.info(f\"{key}: {value}\")\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#22-fetch_public_keytoken","title":"2.2. <code>fetch_public_key(token)</code>","text":"<p>Extracts the key ID (<code>kid</code>) from the JWT header, fetches the public keys from the identity provider's JWKS endpoint, and finds the matching key for signature verification.</p> <pre><code>def fetch_public_key(token):\n    \"\"\"Fetches the public key for the given token.\"\"\"\n    try:\n        header = jwt.get_unverified_header(token)\n        key_id = header.get(\"kid\")\n        jwks_url = 'https://substrate.office.com/sts/common/discovery/keys'  # Replace with your IdP's JWKS endpoint if needed\n        jwks = requests.get(jwks_url).json()\n        for key in jwks[\"keys\"]:\n            if key[\"kid\"] == key_id:\n                return RSAAlgorithm.from_jwk(json.dumps(key))\n    except Exception as e:\n        raise Exception(f\"Error fetching public key: {e}\")\n    return None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#23-validate_tokentoken-public_key","title":"2.3. <code>validate_token(token, public_key)</code>","text":"<p>Decodes and verifies the JWT signature using the public key, checks the token's issuer and audience, and raises an error if the token is expired or invalid.</p> <pre><code>def validate_token(token, public_key):\n    \"\"\"Validates the JWT token using the public key.\"\"\"\n    try:\n        decoded_token = jwt.decode(\n            token, public_key, algorithms=[\"RS256\"], audience=\"https://api.mycompany.com\"\n        )\n        if decoded_token.get(\"iss\") != \"https://substrate.office.com/sts/\":  # Replace with your IdP's issuer if needed\n            raise Exception(\"Invalid issuer!\")\n        return True\n    except jwt.ExpiredSignatureError:\n        raise Exception(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        raise Exception(\"Invalid token!\")\n</code></pre> <p>Summary: Only requests with a valid JWT token (issued by your identity provider) are accepted. The payload is only processed if authentication passes. All actions are logged for traceability.</p>"},{"location":"adaptive-card-consultant-review-part2/#3-deep-dive-process_adaptive_card_payload","title":"3. Deep Dive: <code>process_adaptive_card_payload</code>","text":"<p>This function is responsible for: - Iterating through the submitted consultant actions. - Extracting manager and consultant details from the payload. - Taking the appropriate action (e.g., sending confirmation emails, saving to disk, triggering downstream automation).</p> <pre><code>def process_adaptive_card_payload(payload, client_ip):\n    for consultant_id, values in payload.items():\n        manager_email = values.get(\"manageremail\")\n        manager_name = values.get(\"managername\")\n        # ... process each consultant's action ...\n    send_email_to_manager(payload, manager_email, manager_name)\n    save_payload_to_disk(payload, manager_email)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#31-send_email_to_managerpayload-manager_email-manager_name","title":"3.1. <code>send_email_to_manager(payload, manager_email, manager_name)</code>","text":"<p>Builds an HTML summary of the manager's actions for all consultants and sends a confirmation email to the manager with a table of decisions (keep/deactivate).</p> <pre><code>def send_email_to_manager(payload, manager_email, manager_name):\n    \"\"\"Sends an HTML formatted email to the manager.\"\"\"\n    try:\n        subject = \"Consultant Review Actions Summary\"\n        body = f\"\"\"\n        &lt;html&gt;\n        &lt;body style=\\\"font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;p&gt;Dear {manager_name},&lt;/p&gt;\n            &lt;p&gt;Thank you for reviewing the consultants. Below is a summary of your actions:&lt;/p&gt;\n            &lt;table border=\\\"1\\\" style=\\\"border-collapse: collapse; width: 100%; font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Consultant Email&lt;/th&gt;\n                &lt;th&gt;Decision&lt;/th&gt;\n            &lt;/tr&gt;\n        \"\"\"\n        for consultant_id, values in payload.items():\n            body += f\"&lt;tr&gt;&lt;td&gt;{values.get('email')}&lt;/td&gt;&lt;td&gt;{values.get('decision')}&lt;/td&gt;&lt;/tr&gt;\"\n        body += \"\"\"\n            &lt;/table&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n        send_email(recipients=[manager_email], subject=subject, html_message=body)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#32-save_payload_to_diskpayload-manager_email","title":"3.2. <code>save_payload_to_disk(payload, manager_email)</code>","text":"<p>Serializes the entire payload to a JSON file and saves it to a mounted share or persistent storage for auditing and further processing.</p> <pre><code>def save_payload_to_disk(payload, manager_email):\n    \"\"\"Saves the entire payload to the mounted share as a single JSON file.\"\"\"\n    try:\n        import os, json, datetime\n        filename = f\"{manager_email}_consultant_review_{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}.json\"\n        path = os.path.join(UNPROCESSED_PATH, filename)\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(payload, f, ensure_ascii=False, indent=2)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#4-downstream-automation-process_deactived_consultants","title":"4. Downstream Automation: <code>process_deactived_consultants</code>","text":"<p>This function is typically run on a schedule to process all submitted consultant reviews: - Loads all unprocessed review files from disk. - For each consultant marked for deactivation, adds them to a deactivation list. - Sends a summary email to HR and IT for further action. - Moves processed files to an archive location.</p> <pre><code>def process_deactived_consultants():\n    deactivate_list = []\n    manager_consultants_files = fetch_and_ignore_unprocessed_review_files()\n    for file in manager_consultants_files:\n        file_path, file_name = file.rsplit('/', 1)\n        file_time_utc = os.path.getmtime(file)\n        file_time = datetime.fromtimestamp(file_time_utc, pytz.utc).astimezone(pytz.timezone('America/Chicago'))\n        with open(file, 'r') as f:\n            file_content = f.read()\n        consultants_data = json.loads(file_content)\n        for consultant, details in consultants_data.items():\n            if details.get('decision') == 'deactivate':\n                deactivate_list.append({\n                    'manager_email': details.get('manageremail'),\n                    'consultant_email': details.get('email'),\n                    'approval_time': file_time.strftime('%Y-%m-%d %H:%M:%S')\n                })\n    if deactivate_list:\n        send_email_to_hr_and_it(deactivate_list)\n    for file in manager_consultants_files:\n        file_name = os.path.basename(file)\n        processed_file_path = os.path.join(PROCESSED_PATH, file_name)\n        os.rename(file, processed_file_path)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#5-error-handling-handle_global_exception","title":"5. Error Handling: <code>handle_global_exception</code>","text":"<p>All major functions use <code>handle_global_exception</code> to log and report errors, ensuring that issues are traceable and do not silently fail.</p> <pre><code>def handle_global_exception(function_name, exception_obj):\n    logger.error(f\"Exception in {function_name}: {exception_obj}\")\n    # Optionally, send an alert email or take other action\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#6-recap-full-request-processing-chain","title":"6. Recap: Full Request Processing Chain","text":"<ol> <li>Adaptive Card submission posts to <code>/consultant-review-confirmation</code>.</li> <li><code>process_request_headers_and_payload</code> authenticates and extracts the payload.<ul> <li>Calls <code>log_jwt_payload</code>, <code>fetch_public_key</code>, <code>validate_token</code>.</li> </ul> </li> <li><code>process_adaptive_card_payload</code> processes the payload.<ul> <li>Calls <code>send_email_to_manager</code>, <code>save_payload_to_disk</code>.</li> </ul> </li> <li><code>process_deactived_consultants</code> (scheduled) processes all reviews and notifies HR/IT.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#7-example-end-to-end-flow","title":"7. Example: End-to-End Flow","text":"<ol> <li>Manager receives Adaptive Card, reviews consultants, and submits actions.</li> <li>Submission is POSTed to <code>/consultant-review-confirmation</code> with a JWT Bearer token.</li> <li>The backend verifies the token, extracts the payload, and logs all actions.</li> <li>The manager receives a confirmation email summarizing their decisions.</li> <li>The payload is saved for auditing and further automation (e.g., account deactivation).</li> <li>HR/IT are notified of deactivation approvals as needed.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#conclusion","title":"Conclusion","text":"<p>By recursively examining each function and providing the full code, you can see how the system securely and reliably processes Adaptive Card submissions. This approach is company-agnostic and can be adapted to any workflow requiring secure, actionable messaging in Outlook.</p> <ul> <li>Always validate and log incoming requests.</li> <li>Process and audit all actions.</li> <li>Automate downstream actions as needed.</li> </ul> <p>This completes the deep dive into the end-to-end workflow for actionable consultant review using Adaptive Cards and Python.</p>"},{"location":"adds-user-creation/","title":"Automating Active Directory User Creation and Group Assignment","text":""},{"location":"adds-user-creation/#introduction","title":"Introduction","text":"<p>Automating user provisioning in Active Directory Domain Services (ADDS) is a common requirement for IT teams managing large organizations. Python, with its rich ecosystem of libraries, makes it possible to programmatically create users and assign them to groups in ADDS. This article provides a detailed walkthrough of a working Python implementation for creating new ADDS users and adding them to groups, using the <code>ldap3</code> library and related tools.</p>"},{"location":"adds-user-creation/#overview-of-the-workflow","title":"Overview of the Workflow","text":"<p>The core function, <code>create_new_users_adds</code>, orchestrates the process of:</p> <ol> <li>Establishing a secure connection to the ADDS server.</li> <li>Creating a new user account with the required attributes.</li> <li>Setting the user's password and enabling the account.</li> <li>Adding the user to one or more ADDS groups.</li> </ol> <p>This workflow is modular, with each step handled by a dedicated function or library call, making it easy to adapt for different environments.</p>"},{"location":"adds-user-creation/#step-1-establishing-a-connection-to-adds","title":"Step 1: Establishing a Connection to ADDS","text":"<p>The function <code>get_adds_Connection</code> uses the <code>ldap3</code> library to connect to the ADDS server over SSL. Credentials are securely retrieved (in this codebase, from Azure Key Vault, but you can use environment variables or other secure stores):</p> <pre><code>server = ldap3.Server(dc_ip, use_ssl=True)\nconn = ldap3.Connection(server, user=LDAP_USER_ID, password=LDAP_USER_PASSWORD)\nif not conn.bind():\n    print('Error in bind', conn.result)\n</code></pre> <p>This returns a connection object used for all subsequent LDAP operations.</p>"},{"location":"adds-user-creation/#step-2-creating-a-new-user-in-adds","title":"Step 2: Creating a New User in ADDS","text":"<p>The function <code>create_adds_user</code> (called within <code>create_new_users_adds</code>) performs the following:</p> <ul> <li> <p>Adds the user object: <pre><code>conn.add(\n    distinguished_name,\n    ['top', 'person', 'organizationalPerson', 'user'],\n    {\n        'givenName': first_name,\n        'sn': last_name,\n        'sAMAccountName': sam_account_name,\n        'userPrincipalName': upn_name,\n        'mail': upn_name\n    }\n)\n</code></pre>   The <code>distinguished_name</code> (DN) specifies the user's location in the directory tree (OU). For generalization, replace any organization-specific OUs with your own structure.</p> </li> <li> <p>Enables the account and sets the password: <pre><code>conn.modify(\n    distinguished_name,\n    {\n        'userAccountControl': [(ldap3.MODIFY_REPLACE, [512])],  # Enable the account\n        'unicodePwd': [(ldap3.MODIFY_REPLACE, [f'\"{default_password}\"'.encode('utf-16-le')])]\n    }\n)\n</code></pre>   The password must be encoded in UTF-16-LE and quoted. The <code>userAccountControl</code> value of 512 enables the account.</p> </li> </ul>"},{"location":"adds-user-creation/#step-3-adding-the-user-to-adds-groups","title":"Step 3: Adding the User to ADDS Groups","text":"<p>After the user is created, the code assigns them to one or more groups using the <code>add_members_to_group</code> function from <code>ldap3.extend.microsoft.addMembersToGroups</code>:</p> <p><pre><code>add_members_to_group(conn, [distinguished_name], group_dns, fix=True)\n</code></pre> - <code>conn</code>: The active LDAP connection. - <code>[distinguished_name]</code>: A list of user DNs to add. - <code>group_dns</code>: A list of group DNs (distinguished names) to which the user should be added. - <code>fix=True</code>: Ensures the function will attempt to fix any inconsistencies in group membership.</p> <p>This function performs the necessary LDAP modifications to add the user as a member of each specified group. It is robust and handles group membership updates according to Microsoft's AD schema.</p>"},{"location":"adds-user-creation/#error-handling-and-best-practices","title":"Error Handling and Best Practices","text":"<ul> <li>Error Handling: Each step is wrapped in try/except blocks, and errors are logged or emailed to administrators. This is critical for production automation.</li> <li>Security: Credentials are not hardcoded. Use secure storage for service accounts and passwords.</li> <li>Generalization: Replace any organization-specific OUs or group names with your own. The logic is portable to any ADDS environment.</li> </ul>"},{"location":"adds-user-creation/#example-creating-and-assigning-a-user","title":"Example: Creating and Assigning a User","text":"<p>Here is a simplified, generalized version of the workflow:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\nfrom ldap3.extend.microsoft.addMembersToGroups import ad_add_members_to_groups as add_members_to_group\n\n# Connect to ADDS\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\n# Create user\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nconn.add(dn, ['top', 'person', 'organizationalPerson', 'user'], {\n    'givenName': 'John',\n    'sn': 'Doe',\n    'sAMAccountName': 'jdoe',\n    'userPrincipalName': 'jdoe@example.com',\n    'mail': 'jdoe@example.com'\n})\n\n# Enable account and set password\nconn.modify(dn, {\n    'userAccountControl': [(MODIFY_REPLACE, [512])],\n    'unicodePwd': [(MODIFY_REPLACE, ['\"YourPassword123!\"'.encode('utf-16-le')])]\n})\n\n# Add to groups\ngroup_dns = ['CN=YourGroup,OU=Groups,DC=example,DC=com']\nadd_members_to_group(conn, [dn], group_dns, fix=True)\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-creation/#conclusion","title":"Conclusion","text":"<p>With Python and the <code>ldap3</code> library, you can fully automate the process of creating users and managing group memberships in Active Directory. This approach is scalable, secure, and adaptable to any ADDS environment. By modularizing each step and handling errors robustly, you can integrate this workflow into larger HR or IT automation pipelines.</p>"},{"location":"adds-user-creation/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"adds-user-update/","title":"Updating Active Directory User Attributes","text":""},{"location":"adds-user-update/#introduction","title":"Introduction","text":"<p>Active Directory Domain Services (ADDS) is the backbone of identity management in many organizations. While user creation and group assignment are common automation tasks, updating user attributes\u2014both standard (delivered) and custom\u2014is equally important for keeping directory data accurate and useful. This article explains, with practical Python code, how to update ADDS user attributes using the <code>ldap3</code> library, focusing on the function <code>update_existing_users_adds</code>.</p>"},{"location":"adds-user-update/#understanding-adds-attributes-delivered-vs-custom","title":"Understanding ADDS Attributes: Delivered vs. Custom","text":"<ul> <li>Delivered (Standard) Attributes:</li> <li>These are built-in attributes provided by Microsoft, such as <code>givenName</code>, <code>sn</code>, <code>title</code>, <code>department</code>, <code>telephoneNumber</code>, etc.</li> <li>They are part of the default AD schema and are widely supported by tools and scripts.</li> <li>Custom Attributes:</li> <li>Organizations can extend the AD schema to include custom attributes (e.g., <code>extensionAttribute1</code>, <code>departmentNumber</code>).</li> <li>These are used for business-specific data not covered by standard attributes.</li> </ul> <p>Both types can be updated using the same LDAP operations.</p>"},{"location":"adds-user-update/#the-python-approach-using-ldap3","title":"The Python Approach: Using ldap3","text":"<p>The <code>ldap3</code> library provides a high-level, Pythonic interface for interacting with ADDS. The function <code>update_existing_users_adds</code> demonstrates how to:</p> <ol> <li>Build a dictionary of user attributes to update (both standard and custom).</li> <li>Connect to ADDS securely.</li> <li>Use the <code>modify</code> method to update attributes for each user.</li> <li>Handle errors and notify administrators if updates fail.</li> </ol>"},{"location":"adds-user-update/#step-by-step-updating-user-attributes","title":"Step-by-Step: Updating User Attributes","text":""},{"location":"adds-user-update/#1-prepare-the-attribute-dictionary","title":"1. Prepare the Attribute Dictionary","text":"<p>For each user, a dictionary is built with the attributes to update. This can include both delivered and custom attributes:</p> <pre><code>item = {\n    'displayName': display_name,           # Standard\n    'givenName': first_name,               # Standard\n    'sn': last_name,                       # Standard\n    'title': title,                        # Standard\n    'department': department,              # Standard\n    'employeeType': employee_type,         # Standard\n    'extensionAttribute1': is_mgmt_position, # Custom\n    'manager': manager_dn,                 # Standard (DN of manager)\n    # ... add more as needed ...\n}\n</code></pre>"},{"location":"adds-user-update/#2-connect-to-adds","title":"2. Connect to ADDS","text":"<pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n</code></pre>"},{"location":"adds-user-update/#3-update-attributes-with-modify","title":"3. Update Attributes with <code>modify</code>","text":"<p>The <code>modify</code> method is used to update one or more attributes for a user. The changes dictionary maps attribute names to a tuple specifying the operation (e.g., <code>MODIFY_REPLACE</code>) and the new value(s):</p> <p><pre><code>changes = {key: (MODIFY_REPLACE, [value]) for key, value in item.items() if value}\nconn.modify(dn=distinguished_name, changes=changes)\n</code></pre> - <code>dn</code>: The distinguished name of the user to update. - <code>changes</code>: A dictionary of attribute updates.</p>"},{"location":"adds-user-update/#4-error-handling-and-notification","title":"4. Error Handling and Notification","text":"<p>After each modify operation, the result is checked. If the update fails, an email notification is sent to administrators:</p> <pre><code>if conn.result['result'] != 0:\n    send_email(\n        recipients=['admin@example.com'],\n        subject=f'Error while updating user {distinguished_name}',\n        plain_message=f\"An error occurred while modifying user: {conn.result}\",\n    )\n</code></pre>"},{"location":"adds-user-update/#example-updating-a-users-attributes","title":"Example: Updating a User's Attributes","text":"<p>Here is a simplified, generalized example:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nchanges = {\n    'title': (MODIFY_REPLACE, ['Senior Engineer']),\n    'department': (MODIFY_REPLACE, ['Engineering']),\n    'extensionAttribute1': (MODIFY_REPLACE, ['Project Lead'])\n}\nconn.modify(dn=dn, changes=changes)\n\nif conn.result['result'] != 0:\n    print(f\"Error updating user: {conn.result}\")\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-update/#best-practices","title":"Best Practices","text":"<ul> <li>Batch Updates: You can update multiple attributes in a single <code>modify</code> call for efficiency.</li> <li>Custom Attributes: Ensure custom attributes exist in your AD schema before attempting to update them.</li> <li>Error Handling: Always check the result of LDAP operations and log or notify on failure.</li> <li>Security: Never hardcode credentials; use secure storage.</li> </ul>"},{"location":"adds-user-update/#conclusion","title":"Conclusion","text":"<p>Updating user attributes in ADDS with Python and <code>ldap3</code> is straightforward and powerful. Whether you are updating standard or custom attributes, the process is the same. By following the approach in <code>update_existing_users_adds</code>, you can automate directory maintenance and ensure your AD data stays current and accurate.</p>"},{"location":"adds-user-update/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"azure-ad-certificate/","title":"Certificate Based Authentication","text":""},{"location":"azure-ad-certificate/#certificate-based-authentication-for-azure-ad-why-and-how","title":"Certificate-Based Authentication for Azure AD: Why and How","text":""},{"location":"azure-ad-certificate/#creating-a-certificate-for-azure-ad-authentication","title":"Creating a Certificate for Azure AD Authentication","text":"<p>To use certificate-based authentication with Azure Active Directory (Azure AD), you first need to generate a certificate. Certificates provide a secure, manageable, and standards-based way to authenticate applications. A <code>.pfx</code> certificate may be required because Azure AD expects a certificate in Personal Information Exchange (PFX) format when uploading via the portal or for certain SDKs. The <code>.pfx</code> file contains both the public and private keys, protected by a password, and is suitable for import/export scenarios.</p>"},{"location":"azure-ad-certificate/#steps-to-generate-a-certificate-using-openssl","title":"Steps to Generate a Certificate Using OpenSSL","text":"<ol> <li>Generate a Private Key: <pre><code>openssl genrsa -out my-app-auth.key 2048\n</code></pre></li> <li>Create a Certificate Signing Request (CSR): <pre><code>openssl req -new -key my-app-auth.key -out my-app-auth.csr\n</code></pre></li> <li>Generate a Self-Signed Certificate: <pre><code>openssl x509 -req -days 730 -in my-app-auth.csr -signkey my-app-auth.key -out my-app-auth.crt\n</code></pre></li> <li>Export to PFX (if needed for Azure): <pre><code>openssl pkcs12 -export -out my-app-auth.pfx -inkey my-app-auth.key -in my-app-auth.crt\n</code></pre> <p>Note: The <code>.pfx</code> format is required if you want to upload the certificate via the Azure Portal or use it with some SDKs/tools. The <code>.crt</code> file is the public certificate, and the <code>.key</code> file is your private key (keep it secure!).</p> </li> </ol>"},{"location":"azure-ad-certificate/#uploading-the-certificate-to-your-entra-azure-ad-application","title":"Uploading the Certificate to Your Entra (Azure AD) Application","text":"<ol> <li>Go to the Microsoft Entra admin center and select Azure Active Directory.</li> <li>Navigate to App registrations and select your application.</li> <li>In the left menu, click Certificates &amp; secrets.</li> <li>Under Certificates, click Upload certificate.</li> <li>Select your <code>.crt</code> or <code>.pfx</code> file and upload it.</li> <li>After uploading, Azure will display the certificate thumbprint. Save this value for use in your application code.</li> </ol>"},{"location":"azure-ad-certificate/#assigning-permissions-to-the-application","title":"Assigning Permissions to the Application","text":"<p>After uploading the certificate, you must assign the necessary API permissions to your application:</p> <ol> <li>In your application's App registration page, go to API permissions.</li> <li>Click Add a permission and select the required Microsoft APIs (e.g., Microsoft Graph, Azure Service Management, etc.).</li> <li>Choose the appropriate permission type (Application or Delegated) and select the required permissions.</li> <li>Click Add permissions.</li> <li>If required, click Grant admin consent to approve the permissions for your organization.</li> </ol> <p>Note: The application will only be able to access resources for which it has been granted permissions. Make sure to review and assign only the permissions your app needs.</p>"},{"location":"azure-ad-certificate/#why-use-a-certificate-instead-of-an-application-secret","title":"Why Use a Certificate Instead of an Application Secret?","text":""},{"location":"azure-ad-certificate/#1-security","title":"1. Security","text":"<ul> <li>Application secrets are essentially passwords. They are susceptible to accidental exposure (e.g., in code repositories, logs, or configuration files).</li> <li>Certificates use asymmetric cryptography. The private key never leaves your environment, and only the public key is uploaded to Azure AD. This makes certificates much harder to compromise.</li> </ul>"},{"location":"azure-ad-certificate/#2-lifecycle-management","title":"2. Lifecycle Management","text":"<ul> <li>Secrets typically expire every 6-12 months, requiring regular rotation and updates in all dependent systems.</li> <li>Certificates can have longer lifespans (e.g., 1-2 years), and their expiration is easier to track and automate.</li> </ul>"},{"location":"azure-ad-certificate/#3-compliance-and-best-practices","title":"3. Compliance and Best Practices","text":"<ul> <li>Microsoft and most security frameworks recommend certificates for service-to-service authentication.</li> <li>Certificates support better auditing and can be managed centrally (e.g., via Azure Key Vault).</li> </ul>"},{"location":"azure-ad-certificate/#why-use-the-msal-library-and-not-a-specific-azure-sdk","title":"Why Use the MSAL Library (and Not a Specific Azure SDK)?","text":"<p>The MSAL (Microsoft Authentication Library) for Python is a lightweight, flexible library for acquiring tokens from Azure AD. It supports a wide range of authentication scenarios, including certificate-based authentication for confidential clients.</p> <ul> <li>Why MSAL?</li> <li>MSAL is the official library for handling authentication and token acquisition with Azure AD.</li> <li>It is not tied to a specific Azure service, making it ideal for generic authentication scenarios.</li> <li> <p>It supports advanced scenarios like certificate-based authentication, multi-tenant apps, and more.</p> </li> <li> <p>Why Not Use a Specific Azure SDK?</p> </li> <li>Some Azure SDKs (e.g., for Storage, Key Vault, etc.) provide their own authentication mechanisms, but they may not support all advanced scenarios or may require additional dependencies.</li> <li>Using MSAL directly gives you full control over the authentication flow and token management, and is more transparent for troubleshooting and customization.</li> </ul>"},{"location":"azure-ad-certificate/#code-example-certificate-based-authentication-in-python","title":"Code Example: Certificate-Based Authentication in Python","text":"<p>Below is the function used in this project to acquire an Azure AD access token using a certificate:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token_from_azure(client_id, authority, tenant_id, resource_scopes):\n    \"\"\"\n    Retrieves an access token from Azure Active Directory using a confidential client application.\n    This function uses certificate-based authentication to acquire an access token for the specified resource.\n    \"\"\"\n    try:\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n\n        app = ConfidentialClientApplication(\n            client_id=client_id,\n            authority=f\"{authority}{tenant_id}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n\n        result = app.acquire_token_for_client(scopes=resource_scopes)\n        if \"access_token\" in result:\n            return result[\"access_token\"]\n\n    except Exception as exception:\n        handle_global_exception(sys._getframe().f_code.co_name, exception)\n    finally:\n        pass\n</code></pre>"},{"location":"azure-ad-certificate/#key-points","title":"Key Points:","text":"<ul> <li>The private key is read from a secure file (<code>certs/*.key</code>).</li> <li>The certificate thumbprint and private key are passed to MSAL's <code>ConfidentialClientApplication</code>.</li> <li>No secrets or passwords are stored in code or configuration.</li> </ul>"},{"location":"azure-ad-certificate/#conclusion","title":"Conclusion","text":"<p>Certificate-based authentication is the recommended and most secure way to authenticate service applications with Azure AD. It reduces risk, simplifies management, and aligns with industry best practices. Migrating from secrets to certificates is straightforward and well-supported by both Azure and the MSAL Python library.</p>"},{"location":"azure-ad-certificate/#references","title":"References","text":"<ul> <li>MSAL Python Certificate Auth Sample</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> <li>OpenSSL Documentation</li> </ul>"},{"location":"azure-billing/","title":"Download Azure Bill","text":""},{"location":"azure-billing/#programmatically-downloading-and-storing-azure-billing-data","title":"Programmatically Downloading and Storing Azure Billing Data:","text":""},{"location":"azure-billing/#introduction","title":"Introduction","text":"<p>Automating the retrieval and storage of Azure billing data is essential for organizations seeking cost transparency and operational efficiency. This guide details a robust, production-grade approach to programmatically obtaining Azure billing data using Python, authenticating securely with certificates, and efficiently storing the results in a SQL Server database. </p>"},{"location":"azure-billing/#1-secure-authentication-acquiring-an-azure-access-token-with-certificates","title":"1. Secure Authentication: Acquiring an Azure Access Token with Certificates","text":"<p>The first step is to authenticate with Azure Active Directory (Azure AD) using certificate-based authentication. This is more secure than using client secrets and is recommended for automation and service-to-service scenarios. For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p> <p>Python Example:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token(client_id, authority, tenant_id, resource_scopes, cert_thumbprint, cert_key_path):\n    \"\"\"\n    Acquire an Azure AD access token using certificate-based authentication.\n    \"\"\"\n    with open(cert_key_path, \"r\") as key_file:\n        private_key = key_file.read()\n    app = ConfidentialClientApplication(\n        client_id=client_id,\n        authority=f\"{authority}{tenant_id}\",\n        client_credential={\n            \"thumbprint\": cert_thumbprint,\n            \"private_key\": private_key,\n        },\n    )\n    result = app.acquire_token_for_client(scopes=resource_scopes)\n    if \"access_token\" not in result:\n        raise Exception(f\"Token acquisition failed: {result}\")\n    return result[\"access_token\"]\n</code></pre> <ul> <li>Why certificates? They are more secure, support longer lifecycles, and are recommended for automation.</li> <li>MSAL Library: The Microsoft Authentication Library (MSAL) is used for token acquisition, providing flexibility and support for advanced scenarios.</li> </ul>"},{"location":"azure-billing/#2-generating-the-azure-cost-report-via-rest-api","title":"2. Generating the Azure Cost Report via REST API","text":"<p>Once authenticated, you can use the Azure Cost Management API to request a cost details report for your subscription. This involves making a POST request to the appropriate endpoint and polling until the report is ready.</p> <p>Python Example:</p> <pre><code>import requests\nimport time\nimport json\n\ndef generate_azure_cost_report(subscription_id, access_token, start_date, end_date, api_version=\"2022-05-01\"):\n    url = f\"https://management.azure.com/subscriptions/{subscription_id}/providers/Microsoft.CostManagement/generateCostDetailsReport?api-version={api_version}\"\n    payload = json.dumps({\"metric\": \"ActualCost\", \"timePeriod\": {\"start\": start_date, \"end\": end_date}})\n    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}\n    response = requests.post(url, headers=headers, data=payload)\n    # Poll until the report is ready\n    while response.status_code == 202:\n        location_url = response.headers.get('Location')\n        retry_after = int(response.headers.get('Retry-After', 30))\n        time.sleep(retry_after)\n        response = requests.get(url=location_url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to generate cost report: {response.status_code} - {response.text}\")\n    return response.json()\n</code></pre> <ul> <li>Polling: The API may return a 202 status, indicating the report is being generated. Poll the <code>Location</code> header until a 200 response is received.</li> <li>Error Handling: Always check for non-200 responses and handle errors appropriately.</li> </ul>"},{"location":"azure-billing/#3-downloading-the-cost-report-data","title":"3. Downloading the Cost Report Data","text":"<p>The response from the cost report API includes a manifest with one or more blob URLs. Download these blobs to obtain the actual cost data, typically in CSV format.</p> <p>Python Example:</p> <pre><code>import urllib3\n\ndef download_cost_report_blobs(manifest, output_path):\n    http = urllib3.PoolManager()\n    for blob in manifest['blobs']:\n        blob_url = blob['blobLink']\n        with open(output_path, 'wb') as out_file:\n            blob_response = http.request('GET', blob_url, preload_content=False)\n            out_file.write(blob_response.data)\n</code></pre> <ul> <li>Blob Download: Use a robust HTTP client (e.g., <code>urllib3</code>) to download the report data.</li> <li>Output: Save the CSV file to a secure, accessible location for further processing.</li> </ul>"},{"location":"azure-billing/#4-loading-the-cost-data-into-sql-server-efficiently","title":"4. Loading the Cost Data into SQL Server Efficiently","text":"<p>After downloading the cost report, the next step is to load the data into a SQL Server table. For large datasets, use a fast, batch insert method to optimize performance.</p> <p>Python Example:</p> <pre><code>import pyodbc\nimport csv\n\ndef load_csv_to_sql_server(csv_path, connection_string, table_name):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    with open(csv_path, 'r', encoding='utf-8-sig') as csvfile:\n        reader = csv.reader(csvfile)\n        columns = next(reader)  # Header row\n        insert_query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['?' for _ in columns])})\"\n        data = list(reader)\n        cursor.fast_executemany = True\n        cursor.executemany(insert_query, data)\n        conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Fast Insert: The <code>fast_executemany</code> flag in <code>pyodbc</code> enables high-performance bulk inserts.</li> <li>Schema Alignment: Ensure the CSV columns match the SQL table schema.</li> </ul>"},{"location":"azure-billing/#5-orchestrating-the-end-to-end-process","title":"5. Orchestrating the End-to-End Process","text":"<p>A typical workflow to automate Azure billing data retrieval and storage:</p> <pre><code>def fetch_and_update_azure_billing_data():\n    # Step 1: Get access token\n    access_token = get_access_token(\n        client_id=..., authority=..., tenant_id=..., resource_scopes=..., cert_thumbprint=..., cert_key_path=...\n    )\n    # Step 2: Generate cost report\n    report = generate_azure_cost_report(\n        subscription_id=..., access_token=access_token, start_date=..., end_date=...\n    )\n    # Step 3: Download report blob(s)\n    download_cost_report_blobs(report['manifest'], output_path=\"azure_billing.csv\")\n    # Step 4: Load into SQL Server\n    load_csv_to_sql_server(\n        csv_path=\"azure_billing.csv\", connection_string=..., table_name=\"AzureBilling\"\n    )\n</code></pre>"},{"location":"azure-billing/#6-additional-considerations","title":"6. Additional Considerations","text":"<ul> <li>Permissions: The Azure AD application must have the required API permissions (e.g., Cost Management Reader) and access to the subscription.</li> <li>Certificate Security: Store private keys securely and never commit them to source control.</li> <li>Error Handling: Implement robust error handling and logging for production use.</li> <li>Scheduling: Use a scheduler (e.g., cron, Azure Automation) to run the process regularly.</li> </ul>"},{"location":"azure-billing/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can securely and efficiently automate the retrieval and storage of Azure billing data using Python. This enables advanced reporting, cost analysis, and integration with enterprise data platforms.</p>"},{"location":"azure-billing/#references","title":"References","text":"<ul> <li>Azure Cost Management REST API</li> <li>MSAL Python Library</li> <li>pyodbc Documentation</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> </ul>"},{"location":"azure-query-recovery-services-vault/","title":"Querying Azure Recovery Services Vault (RSV)","text":""},{"location":"azure-query-recovery-services-vault/#introduction","title":"Introduction","text":"<p>Azure Recovery Services Vault (RSV) is a core component for managing and monitoring backups of virtual machines and other resources in Azure. Automating the retrieval and analysis of backup data can help with compliance, reporting, and operational efficiency. This article demonstrates how to:</p> <ul> <li>Connect to Azure using the Python SDK and a service principal</li> <li>Query a Recovery Services Vault for backup details</li> <li>Process and store backup information for further analysis</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"azure-query-recovery-services-vault/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with Recovery Services Vault(s) and VM backups</li> <li>Service principal with appropriate permissions (Backup Reader, etc.)</li> <li>Python 3.8+ and the following packages (install with <code>pip install ...</code>):</li> <li><code>azure-identity</code> (for authentication)</li> <li><code>azure-mgmt-recoveryservicesbackup</code> (for querying backup data)</li> <li><code>azure-mgmt-resource</code> (for resource management, optional)</li> <li><code>requests</code> (for any direct REST API calls, optional)</li> <li><code>python-dateutil</code> (for date parsing, optional)</li> <li><code>csv</code> (standard library, for CSV export)</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-query-recovery-services-vault/#step-1-authenticate-to-azure-with-a-service-principal","title":"Step 1: Authenticate to Azure with a Service Principal","text":"<p>Use the Azure Identity library to authenticate securely:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_client_secret_credential(tenant_id, client_id, client_secret):\n    \"\"\"Obtain a ClientSecretCredential for Azure authentication.\"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n\n**Explanation:**\n- Use a service principal for secure, automated access.\n- Store secrets securely (e.g., Azure Key Vault).\n\n---\n\n## Step 2: Connect to the Recovery Services Backup Client\n\n\n```python\nfrom azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\n\ndef get_backup_client(credential, subscription_id):\n    \"\"\"Create a RecoveryServicesBackupClient for backup operations.\"\"\"\n    return RecoveryServicesBackupClient(credential, subscription_id)\n# Example usage:\n# backup_client = get_backup_client(credential, '&lt;your-subscription-id&gt;')\n</code></pre> <p>Explanation: - The <code>RecoveryServicesBackupClient</code> allows you to query backup items, jobs, and policies.</p>"},{"location":"azure-query-recovery-services-vault/#step-3-query-backup-items-in-a-recovery-services-vault","title":"Step 3: Query Backup Items in a Recovery Services Vault","text":"<pre><code>def list_vm_backups(backup_client, resource_group, vault_name):\n    \"\"\"List all backup items (e.g., Azure VMs) in the specified vault.\"\"\"\n    items = backup_client.backup_protected_items.list(\n        vault_name=vault_name,\n        resource_group_name=resource_group,\n        filter=\"backupManagementType eq 'AzureIaasVM'\"\n    )\n    backup_info = []\n    for item in items:\n        backup_info.append({\n            'vm_name': item.properties.friendly_name,\n            'protection_status': item.properties.protection_status,\n            'last_backup_time': item.properties.last_backup_time,\n            'health_status': item.properties.health_status,\n            'resource_id': item.id\n        })\n    return backup_info\n</code></pre> <p>Explanation: - Lists all VM backup items in the specified Recovery Services Vault. - Extracts key properties for reporting or further processing.</p>"},{"location":"azure-query-recovery-services-vault/#step-4-store-or-report-on-backup-data","title":"Step 4: Store or Report on Backup Data","text":"<p>You can save the backup information to a CSV file or database for further analysis:</p> <pre><code>def save_backup_info_to_csv(backup_info, filename):\n    \"\"\"Save backup information to a CSV file.\"\"\"\n    with open(filename, 'w', newline='') as csvfile:\n        fieldnames = ['vm_name', 'protection_status', 'last_backup_time', 'health_status', 'resource_id']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for row in backup_info:\n            writer.writerow(row)\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    # Retrieve credentials securely\n    tenant_id = '&lt;your-tenant-id&gt;'\n    client_id = '&lt;your-client-id&gt;'\n    client_secret = '&lt;your-client-secret&gt;'\n    subscription_id = '&lt;your-subscription-id&gt;'\n    resource_group = '&lt;your-resource-group&gt;'\n    vault_name = '&lt;your-vault-name&gt;'\n\n    credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n    backup_client = get_backup_client(credential, subscription_id)\n    backup_info = list_vm_backups(backup_client, resource_group, vault_name)\n    save_backup_info_to_csv(backup_info, 'azure_vm_backups.csv')\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#advanced-updating-backup-resource-details-programmatically","title":"Advanced: Updating Backup Resource Details Programmatically","text":"<p>In some enterprise scenarios, you may want to enrich or update your backup resource inventory with additional details from Azure Recovery Services Vault (RSV). The following function demonstrates how to programmatically update a list of backup resources with the latest backup status and metadata for each VM.</p> <pre><code>from azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\nfrom azure.mgmt.recoveryservicesbackup.activestamp.models import AzureIaaSComputeVMProtectedItem\nimport datetime\n\ndef update_backup_resources(resource_group, vault_name, backup_resources):\n    \"\"\"\n    Update backup resource details by fetching relevant VM backup information from Recovery Services Vault (RSV).\n\n    :param resource_group: Name of the Azure resource group\n    :param vault_name: Name of the Recovery Services Vault\n    :param backup_resources: List of backup resource dictionaries to update\n    \"\"\"\n    try:\n        # Obtain Azure credentials (replace with your secure credential retrieval)\n        credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n        backup_client = RecoveryServicesBackupClient(credential, subscription_id)\n\n        # Dictionary to store VM backups from the vault\n        azure_vm_backups = {}\n\n        # Fetch backup-protected items from the Recovery Services Vault\n        rsv_backup_items = backup_client.backup_protected_items.list(vault_name, resource_group)\n        azure_vm_backups.update({\n            item.properties.virtual_machine_id.lower(): item\n            for item in rsv_backup_items\n            if isinstance(item.properties, AzureIaaSComputeVMProtectedItem)\n        })\n\n        # Get today's date in YYYY-MM-DD format\n        today_date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n\n        def format_date(value):\n            return value.strftime(\"%Y-%m-%d\") if isinstance(value, datetime.datetime) else today_date\n\n        # Iterate through the list of backup resources and update details\n        for resource in backup_resources:\n            normalized_vm_id = resource['resource_id'].lower()\n            vm_backup = azure_vm_backups.get(normalized_vm_id)\n            resource.update({\n                \"friendly_name\": getattr(vm_backup.properties, 'friendly_name', '') if vm_backup else '',\n                \"policy_name\": getattr(vm_backup.properties, 'policy_name', '') if vm_backup else '',\n                \"last_backup_status\": getattr(vm_backup.properties, 'last_backup_status', '') if vm_backup else '',\n                \"last_backup_time\": format_date(getattr(vm_backup.properties, 'last_backup_time', None)) if vm_backup else today_date,\n                \"last_recovery_point\": format_date(getattr(vm_backup.properties, 'last_recovery_point', None)) if vm_backup else today_date,\n                \"protection_state\": getattr(vm_backup.properties, 'protection_state', '') if vm_backup else '',\n                \"protection_status\": getattr(vm_backup.properties, 'protection_status', '') if vm_backup else '',\n                \"container_name\": getattr(vm_backup.properties, 'container_name', '') if vm_backup else '',\n            })\n    except Exception as e:\n        print(f\"Error updating backup resources: {e}\")\n</code></pre> <p>Explanation: - This function takes a list of backup resources (e.g., VMs) and updates each with the latest backup metadata from Azure RSV. - It normalizes VM IDs for matching, fetches backup items from the vault, and updates each resource dictionary in-place. - Error handling is included for robustness; in production, use secure credential management and structured logging.</p>"},{"location":"azure-query-recovery-services-vault/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the retrieval and reporting of VM backup data from Azure Recovery Services Vaults. This enables better compliance, reporting, and operational insight into your backup posture.</p>"},{"location":"azure-resources/","title":"Download Azure Resource","text":""},{"location":"azure-resources/#programmatically-downloading-azure-resource-inventory-and-tag-management","title":"Programmatically Downloading Azure Resource Inventory and Tag Management","text":""},{"location":"azure-resources/#introduction","title":"Introduction","text":"<p>Maintaining an up-to-date inventory of Azure resources and their associated tags is critical for governance, cost management, and compliance. This article provides a detailed, production-grade approach to programmatically fetching Azure resource metadata and synchronizing it with a SQL Server database using Python. </p>"},{"location":"azure-resources/#1-authentication-secure-access-to-azure-apis","title":"1. Authentication: Secure Access to Azure APIs","text":"<p>Before accessing Azure resources, authenticate using a secure method. The function below demonstrates using the Azure Identity SDK's <code>ClientSecretCredential</code> for authentication. This is a common approach for automation scenarios, but for higher security, certificate-based authentication is recommended (see other article Certificate Based Authorization for Azure AD.)</p>"},{"location":"azure-resources/#deep-dive-get_azure_credential-function","title":"Deep Dive: <code>get_azure_credential</code> Function","text":"<p>The <code>get_azure_credential</code> function leverages the <code>azure-identity</code> Python SDK, which provides a unified way to authenticate to Azure services. Here, we use the <code>ClientSecretCredential</code> class, which is suitable for service principals (app registrations) with a client secret.</p> <p>Python Example:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_azure_credential(tenant_id, client_id, client_secret):\n    \"\"\"\n    Returns a credential object for authenticating with Azure SDKs.\n    Uses the azure-identity library's ClientSecretCredential.\n    \"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n</code></pre> <ul> <li>azure-identity SDK: This is the official Microsoft library for Azure authentication in Python. It supports multiple credential types, including secrets, certificates, managed identity, and interactive login.</li> <li>ClientSecretCredential: This class is used for service-to-service authentication using a client ID and secret. It is widely supported by Azure SDKs, including resource management, storage, and more.</li> <li>When to use: Use this for automation where a client secret is securely stored (e.g., in Azure Key Vault or environment variables). For higher security, use <code>CertificateCredential</code> instead.</li> </ul>"},{"location":"azure-resources/#2-fetching-azure-resource-inventory","title":"2. Fetching Azure Resource Inventory","text":"<p>Use the Azure SDK to enumerate all resources in a subscription. Extract key metadata such as resource ID, name, location, type, and tags.</p> <p>Python Example:</p> <pre><code>def fetch_azure_resources(credential, subscription_id):\n    client = ResourceManagementClient(credential, subscription_id)\n    resource_list = []\n    for item in client.resources.list():\n        type_parts = str(item.type).split('/')\n        type1, type2, type3, type4, type5 = (type_parts + [''] * 5)[:5]\n        resource_group_list = str(item.id).split('/')\n        resource_data = {\n            \"id\": str(item.id).replace(f'/subscriptions/{subscription_id}/', ''),\n            \"location\": item.location,\n            \"name\": item.name,\n            \"tags\": item.tags,\n            \"resourceGroup\": resource_group_list[4] if len(resource_group_list) &gt;= 4 else '',\n            \"type1\": type1,\n            \"type2\": type2,\n            \"type3\": type3,\n            \"type4\": type4,\n            \"type5\": type5,\n        }\n        resource_list.append(resource_data)\n    return resource_list\n</code></pre> <ul> <li>Resource Types: The code splits the resource type string to extract up to five type levels for flexible reporting.</li> <li>Tags: Tags are included for governance and cost allocation.</li> </ul>"},{"location":"azure-resources/#3-synchronizing-with-sql-server-fast-bulk-operations","title":"3. Synchronizing with SQL Server: Fast Bulk Operations","text":"<p>Efficiently update the SQL Server inventory table by marking all resources as inactive, then bulk updating existing resources and inserting new ones. This ensures the database reflects the current Azure state.</p> <p>Note: The <code>Dim_Resources</code> table is not a full load (truncate-and-reload) table. Instead, it is designed to retain records of resources that may have been deleted from Azure. By marking resources as inactive rather than removing them, you can track the lifecycle of resources, including those that have been deleted, for audit, compliance, and historical analysis purposes.</p> <p>Python Example:</p> <pre><code>import pyodbc\n\ndef sync_resources_to_sql(resource_list, connection_string):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    existing_resource_ids = {str(row[0]).lower() for row in cursor.execute(\"SELECT ResourceID FROM Dim_Resources\").fetchall()}\n    updateresources = [\n        [r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True, r['id']]\n        for r in resource_list if str(r['id']).lower() in existing_resource_ids\n    ]\n    newresources = [\n        [r['id'], r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True]\n        for r in resource_list if str(r['id']).lower() not in existing_resource_ids\n    ]\n    cursor.execute('UPDATE Dim_Resources SET Active = 0')\n    if updateresources:\n        query = '''UPDATE Dim_Resources SET Location=?, Name=?, ResourceGroup=?, Type1=?, Type2=?, Type3=?, Type4=?, Type5=?, Active=? WHERE ResourceId=?'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, updateresources)\n    if newresources:\n        query = '''INSERT INTO Dim_Resources (ResourceID, Location, Name, ResourceGroup, Type1, Type2, Type3, Type4, Type5, Active) VALUES (?,?,?,?,?,?,?,?,?,?)'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, newresources)\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Bulk Operations: Use <code>fast_executemany</code> for high-performance updates and inserts.</li> <li>Active Flag: Mark all resources as inactive before updating, then set active for current resources.</li> </ul>"},{"location":"azure-resources/#4-end-to-end-orchestration","title":"4. End-to-End Orchestration","text":"<p>A typical workflow for resource inventory management:</p> <pre><code>def fetch_and_store_resources():\n    credential = get_azure_credential(tenant_id=..., client_id=..., client_secret=...)\n    resource_list = fetch_azure_resources(credential, subscription_id=...)\n    sync_resources_to_sql(resource_list, connection_string=...)\n</code></pre>"},{"location":"azure-resources/#5-best-practices-and-considerations","title":"5. Best Practices and Considerations","text":"<ul> <li>Security: Use certificate-based authentication for automation when possible. Store credentials securely.</li> <li>Performance: Use bulk operations for large datasets.</li> <li>Data Quality: Regularly update the inventory to reflect the current Azure state.</li> <li>Scheduling: Automate the process with a scheduler (e.g., cron, Azure Automation).</li> <li>Auditing: Keep logs of changes and exceptions for compliance.</li> </ul>"},{"location":"azure-resources/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can automate the discovery and inventory of Azure resources, ensuring your SQL Server database remains a reliable source of truth for governance and reporting.</p>"},{"location":"azure-resources/#references","title":"References","text":"<ul> <li>Azure Resource Management Python SDK</li> <li>azure-identity Python SDK</li> <li>pyodbc Documentation</li> <li>Azure Tagging Best Practices</li> </ul>"},{"location":"meraki-nagios-device-sync/","title":"Automating Cisco Meraki Device Discovery and Nagios XI Monitoring Integration","text":""},{"location":"meraki-nagios-device-sync/#introduction","title":"Introduction","text":"<p>Keeping your network monitoring system in sync with your actual device inventory is critical for reliable operations. This article provides a deep dive into a robust Python workflow that:</p> <ul> <li>Discovers all current devices from the Cisco Meraki cloud API</li> <li>Uses SNMP OIDs to obtain Meraki hostnames</li> <li>Compares Meraki inventory to Nagios XI monitored hosts</li> <li>Adds missing devices to Nagios XI, including handling special device types</li> <li>Checks firmware status for compliance</li> </ul> <p>All code is provided and explained so you can adapt this solution for your own environment.</p>"},{"location":"meraki-nagios-device-sync/#required-python-libraries","title":"Required Python Libraries","text":"<p>This workflow uses the following Python libraries:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library. Used for all Meraki cloud API calls.</li> <li>requests: For making HTTP requests to the Nagios XI REST API.</li> <li>subprocess: To run SNMP commands (e.g., <code>snmpwalk</code>) from Python.</li> <li>re: For parsing SNMP command output with regular expressions.</li> </ul> <p>Install any missing libraries with pip:</p> <pre><code>pip install meraki requests\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1-authenticating-to-cisco-meraki-and-nagios-xi-apis","title":"1. Authenticating to Cisco Meraki and Nagios XI APIs","text":""},{"location":"meraki-nagios-device-sync/#cisco-meraki-api-authentication","title":"Cisco Meraki API Authentication","text":"<p>To connect to the Meraki Dashboard API, you need an API key. This key can be generated in your Meraki dashboard under Organization &gt; Settings &gt; Dashboard API access.</p> <pre><code>import meraki\n\nMERAKI_API_KEY = 'YOUR_MERAKI_API_KEY'  # Replace with your Meraki API key\nMERAKI_BASE_URL = 'https://api.meraki.com/api/v1/'\n\ndashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    base_url=MERAKI_BASE_URL,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre>"},{"location":"meraki-nagios-device-sync/#nagios-xi-api-authentication","title":"Nagios XI API Authentication","text":"<p>Nagios XI provides a REST API. You need an API key, which can be generated in the Nagios XI web interface under My Account &gt; API Keys.</p> <pre><code>import requests\n\nNAGIOS_XI_API_URL = 'https://your-nagios-server.example.com/nagiosxi/api/v1/'  # Replace with your Nagios XI URL\nNAGIOS_XI_API_KEY = 'YOUR_NAGIOS_API_KEY'  # Replace with your Nagios XI API key\n\ndef call_nagios_api(endpoint, method='GET', data=None):\n    url = f\"{NAGIOS_XI_API_URL}{endpoint}\"\n    headers = {'Authorization': f'Bearer {NAGIOS_XI_API_KEY}'}\n    if method == 'GET':\n        response = requests.get(url, headers=headers)\n    elif method == 'POST':\n        response = requests.post(url, headers=headers, json=data)\n    elif method == 'PUT':\n        response = requests.put(url, headers=headers, json=data)\n    elif method == 'DELETE':\n        response = requests.delete(url, headers=headers)\n    else:\n        raise ValueError('Unsupported HTTP method')\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1a-understanding-and-setting-up-meraki_dashboard_snmp_community_string","title":"1a. Understanding and Setting Up <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>","text":""},{"location":"meraki-nagios-device-sync/#what-is-meraki_dashboard_snmp_community_string","title":"What is <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>?","text":"<p>The <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code> is a shared secret (like a password) used for authenticating SNMP v2c queries to the Meraki cloud SNMP endpoint. It is required to retrieve device information via SNMP, such as hostnames and other device attributes.</p>"},{"location":"meraki-nagios-device-sync/#how-to-set-up-the-snmp-community-string-in-meraki-dashboard","title":"How to Set Up the SNMP Community String in Meraki Dashboard","text":"<ol> <li>Log in to your Meraki Dashboard</li> <li>Navigate to Organization &gt; Settings</li> <li>Scroll to the SNMP section</li> <li>Enable Cloud Monitoring (SNMP v2c)</li> <li>Set your desired SNMP Community String (e.g., <code>mysnmpcommunity</code>)</li> <li>Save your changes</li> <li>Whitelist your public IP address in the SNMP section to allow SNMP queries from your monitoring server</li> </ol> <p>Note: The SNMP community string acts as a password for SNMP v2c. Keep it secure and do not share it publicly.</p>"},{"location":"meraki-nagios-device-sync/#plugging-the-community-string-into-your-code","title":"Plugging the Community String into Your Code","text":"<p>In your Python code, set the value as follows:</p> <pre><code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING = 'mysnmpcommunity'  # Replace with your actual SNMP community string\n</code></pre> <p>This value is then used in SNMP queries, for example:</p> <pre><code>import subprocess\nimport re\n\ndef get_snmp_data(snmp_server, port, oid, community):\n    command = [\n        \"snmpwalk\",\n        \"-v\", \"2c\",\n        \"-c\", community,\n        f\"{snmp_server}:{port}\",\n        oid\n    ]\n    try:\n        result = subprocess.run(command, capture_output=True, text=True, check=True)\n        output = result.stdout\n        snmp_dict = {}\n        pattern = re.compile(r'(\\S+)\\s+=\\s+STRING:\\s+\"([^\"]+)\"')\n        for match in pattern.finditer(output):\n            oid = match.group(1)\n            string_value = match.group(2)\n            snmp_dict[string_value] = oid\n        return snmp_dict\n    except Exception as e:\n        print(f\"SNMP error: {e}\")\n        return None\n\nMERAKI_DASHBOARD_SNMP_HOST_NAME = 'snmp.meraki.com'\nMERAKI_DASHBOARD_SNMP_PORT = '16100'\n\nmerakihostnames = get_snmp_data(\n    MERAKI_DASHBOARD_SNMP_HOST_NAME,\n    MERAKI_DASHBOARD_SNMP_PORT,\n    '1.3.6.1.4.1.29671.1.1.4.1.2',\n    MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING\n)\n</code></pre> <p>If you change the community string in the Meraki dashboard, update it in your code as well.</p>"},{"location":"meraki-nagios-device-sync/#2-obtaining-all-current-devices-from-meraki","title":"2. Obtaining All Current Devices from Meraki","text":"<p>We use the official Meraki Dashboard API to fetch all organizations, devices, and networks:</p> <p><pre><code>gdepOrganizations = dashboard.organizations.getOrganizations()\norganizationid = gdepOrganizations[0]['id']\ngdepdevices = dashboard.organizations.getOrganizationDevices(organizationid, -1)\ngdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid, -1)\n</code></pre> - <code>getOrganizations()</code> returns all organizations your API key can access. - <code>getOrganizationDevices()</code> fetches all devices (appliances, switches, cameras, wireless, etc.). - <code>getOrganizationNetworks()</code> fetches all networks (logical groupings of devices).</p>"},{"location":"meraki-nagios-device-sync/#3-obtaining-meraki-hostnames-via-snmp-oid","title":"3. Obtaining Meraki Hostnames via SNMP OID","text":"<p>To get hostnames as seen by Meraki's SNMP dashboard, we use the SNMP OID <code>1.3.6.1.4.1.29671.1.1.4.1.2</code> (see code above).</p> <ul> <li>This function runs an <code>snmpwalk</code> command and parses the output into a dictionary of hostnames and OIDs.</li> <li>SNMP access must be enabled and your IP whitelisted in the Meraki dashboard.</li> </ul>"},{"location":"meraki-nagios-device-sync/#4-checking-firmware-status-for-each-network","title":"4. Checking Firmware Status for Each Network","text":"<p>For each network, we check the current firmware status of all products using the Meraki Dashboard API's <code>getNetworkFirmwareUpgrades</code> method.</p>"},{"location":"meraki-nagios-device-sync/#what-is-getnetworkfirmwareupgrades","title":"What is <code>getNetworkFirmwareUpgrades</code>?","text":"<p>This method retrieves the current and available firmware versions for all devices in a given Meraki network. It helps you: - Audit firmware compliance - Identify devices that need upgrades - Track which products are running which firmware</p>"},{"location":"meraki-nagios-device-sync/#example-usage","title":"Example Usage","text":"<pre><code># For each network, get firmware upgrade status\nfor network in gdepnetworks:\n    network_id = network['id']\n    networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network_id)\n    print(f\"Firmware info for network {network['name']}:\\n\", networkupgrades)\n    if 'products' in networkupgrades:\n        products = networkupgrades['products']\n        for product_type, firmware_info in products.items():\n            print(f\"Product: {product_type}\")\n            print(f\"Current Version: {firmware_info.get('currentVersion', {}).get('name', 'N/A')}\")\n            print(f\"Available Version: {firmware_info.get('availableVersion', {}).get('name', 'N/A')}\")\n            print(f\"Status: {firmware_info.get('status', 'N/A')}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#sample-output-structure","title":"Sample Output Structure","text":"<p>The returned dictionary typically looks like:</p> <pre><code>{\n  \"products\": {\n    \"appliance\": {\n      \"currentVersion\": {\"name\": \"MX 18.107.2\"},\n      \"availableVersion\": {\"name\": \"MX 18.107.4\"},\n      \"status\": \"Up to date\"\n    },\n    \"switch\": {\n      \"currentVersion\": {\"name\": \"MS 15.21\"},\n      \"availableVersion\": {\"name\": \"MS 15.22\"},\n      \"status\": \"Upgrade available\"\n    }\n  }\n}\n</code></pre> <ul> <li><code>currentVersion</code>: The firmware currently running on the product type.</li> <li><code>availableVersion</code>: The latest available firmware for that product type.</li> <li><code>status</code>: Whether the device is up to date or needs an upgrade.</li> </ul> <p>This information can be used to automate firmware compliance checks and trigger upgrades as needed.</p>"},{"location":"meraki-nagios-device-sync/#5-comparing-meraki-devices-to-nagios-xi-hosts","title":"5. Comparing Meraki Devices to Nagios XI Hosts","text":"<p>We fetch all hosts from Nagios XI and compare them to the Meraki inventory:</p> <p><pre><code>nagioshost = call_nagios_api('objects/host')\nfor device in gdepdevices:\n    nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n    if len(nagioshostitems) == 0:\n        # Device is missing from Nagios XI\n        # ...add to missing list and prepare for addition...\n</code></pre> - Devices not found in Nagios XI are flagged for addition. - Special handling for device types (appliance, switch, camera, wireless, etc.).</p>"},{"location":"meraki-nagios-device-sync/#6-adding-missing-devices-to-nagios-xi","title":"6. Adding Missing Devices to Nagios XI","text":"<p>For each missing device, we call helper functions to create/update hosts and services in Nagios XI:</p> <p><pre><code>if len(str(device['name']).strip()) != 0:\n    if (str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS):\n        if (str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS):\n            if (device['productType'] == 'appliance' and 'VMX' not in device['model']):\n                applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                if len(vlan999) == 0:\n                    device['lanIp'] = '0.0.0.0'\n                else:\n                    device['lanIp'] = vlan999[0]['applianceIp']\n            if device['lanIp'] is None:\n                device['lanIp'] = '0.0.0.0'\n            create_update_meraki_host(device, nagioshostitems, gdepnetworks, False)\n            nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n            create_update_meraki_host_services(device, nagiosserviceitems, False, merakihostnames)\n</code></pre> - <code>create_update_meraki_host()</code> and <code>create_update_meraki_host_services()</code> are responsible for adding/updating hosts and their services in Nagios XI. - VLAN and IP logic ensures correct addressing for appliances.</p>"},{"location":"meraki-nagios-device-sync/#7-applying-configuration","title":"7. Applying Configuration","text":"<p>After all additions/updates, we apply the Nagios XI configuration:</p> <pre><code>data = {'alias': 'Nagios XI', 'applyconfig': '1'}\ncall_nagios_api('config/host/localhost', method='PUT', data=data)\nnagioshost = call_nagios_api('objects/host')\n</code></pre>"},{"location":"meraki-nagios-device-sync/#8-full-function-code-add_missing_network_device_to_nagios","title":"8. Full Function Code: <code>add_missing_network_device_to_nagios</code>","text":"<p>Below is the complete function, ready to adapt for your own environment:</p> <pre><code>def add_missing_network_device_to_nagios():\n    try:\n        # OID to obtain all host names from Meraki SNMP Dashboard\n        merakihostnames = get_snmp_data(\n            MERAKI_DASHBOARD_SNMP_HOST_NAME, \n            MERAKI_DASHBOARD_SNMP_PORT,\n            '1.3.6.1.4.1.29671.1.1.4.1.2',\n            MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING)\n        gdepOrganizations = dashboard.organizations.getOrganizations()\n        organizationid = gdepOrganizations[0]['id']\n        gdepdevices = dashboard.organizations.getOrganizationDevices(organizationid,-1)\n        gdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid,-1)\n\n        for network in gdepnetworks:\n            networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network['id'])\n            if ('products' in networkupgrades):\n                products = networkupgrades['products']\n                # ...process firmware info as needed...\n\n        nagioshost = call_nagios_api('objects/host')\n        nagioshostservices = call_nagios_api('objects/service')\n        nagioshostconfig = call_nagios_api('config/host')\n        nagioshostgroupmembers = call_nagios_api('objects/hostgroupmembers')\n        nagioshostservicesconfig = call_nagios_api('config/service')\n\n        SKIP_MERAKI_HOSTS = ['tst','tes']\n\n        for device in gdepdevices:\n            if (device['productType'] == 'appliance'):\n                # ...handle appliance types...\n                pass\n            elif (device['productType'] == 'camera'):\n                pass\n            elif (device['productType'] == 'switch'):\n                pass\n            elif (device['productType'] == 'wireless'):\n                pass\n            # ...other device handling as needed...\n            nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n            if (len(nagioshostitems) == 0):\n                if (len(str(device['name']).strip()) != 0):\n                    if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                            # ...add to Nagios XI...\n                            pass\n            if (len(str(device['name']).strip()) != 0):\n                if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                    if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((device['productType'] == 'appliance') and ('VMX' not in device['model'])):\n                            applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                            vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                            if (len(vlan999) == 0):\n                                device['lanIp'] = '0.0.0.0'\n                            else:\n                                device['lanIp'] = vlan999[0]['applianceIp']\n                        if (device['lanIp'] is None):\n                            device['lanIp'] = '0.0.0.0'\n                        create_update_meraki_host(device,nagioshostitems,gdepnetworks,False)\n                        nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n                        create_update_meraki_host_services(device,nagiosserviceitems,False,merakihostnames)\n        data = {'alias': 'Nagios XI', 'applyconfig': '1'}\n        call_nagios_api('config/host/localhost', method='PUT', data=data)\n        nagioshost = call_nagios_api('objects/host')\n    except Exception as exception_obj:\n        print(f\"Error: {exception_obj}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#9-conclusion","title":"9. Conclusion","text":"<p>This workflow ensures your Nagios XI monitoring system is always in sync with your actual Meraki device inventory, with full visibility into firmware status and device types. By automating device discovery, comparison, and configuration, you can maintain a reliable, up-to-date monitoring environment with minimal manual effort.</p>"},{"location":"meraki-nagios-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Python meraki library</li> </ul>"},{"location":"proofpoint-user-management/","title":"Automating User Management in Proofpoint Essentials","text":""},{"location":"proofpoint-user-management/#introduction","title":"Introduction","text":"<p>Proofpoint Essentials provides robust APIs for managing users and optimizing licensing. Marking certain users as \"functional accounts\" (such as service, shared, or terminated accounts) can help reduce licensing costs and improve compliance. This article demonstrates how to:</p> <ul> <li>Connect to the Proofpoint Essentials API using an API user and key</li> <li>Retrieve active users from Proofpoint</li> <li>Compare users to your HR system (e.g., ADP or any HRIS)</li> <li>Mark users as functional accounts via API</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"proofpoint-user-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Proofpoint Essentials administrator access</li> <li>An API user and API key (see below)</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Access to your HR system data (e.g., via SQL, API, or CSV)</li> </ul>"},{"location":"proofpoint-user-management/#how-to-create-an-api-key-in-proofpoint-essentials","title":"How to Create an API Key in Proofpoint Essentials","text":"<ol> <li>Log in to the Proofpoint Essentials admin portal.</li> <li>Navigate to Account Management &gt; API Keys.</li> <li>Click Create API Key.</li> <li>Assign the key to a dedicated API user with appropriate permissions.</li> <li>Save the API key securely (e.g., in Azure Key Vault or a secrets manager).</li> </ol> <p>For more details, refer to the official Proofpoint Essentials API documentation. </p>"},{"location":"proofpoint-user-management/#step-1-connect-to-the-proofpoint-essentials-api","title":"Step 1: Connect to the Proofpoint Essentials API","text":"<pre><code>import requests\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\nPROOFPOINT_BASE_API = 'https://&lt;your-region&gt;.proofpointessentials.com/api/v1/'\nPROOFPOINT_API_USER = get_azure_kv_sceret('pp-api-user')\nPROOFPOINT_API_PASSWORD = get_azure_kv_sceret('pp-api-key')\n\n# Example: Get all users in your organization\nurl = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\nresponse = requests.get(\n    url=url,\n    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n)\nresponse.raise_for_status()\nusers = response.json().get('users', [])\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The API call retrieves all users for your organization.</p>"},{"location":"proofpoint-user-management/#step-2-retrieve-user-data-from-your-hr-system","title":"Step 2: Retrieve User Data from Your HR System","text":"<p>Assume you have a function to get user data from your HR system (e.g., via SQL):</p> <pre><code>def get_latest_hr_data():\n    sql_statement = \"\"\"\n        SELECT status, email, company_code, worker_category_code, location_code\n        FROM hr_employees WHERE email IS NOT NULL\n    \"\"\"\n    return execute_sql_fetch_dicts(sql_statement)\n</code></pre>"},{"location":"proofpoint-user-management/#step-3-compare-and-mark-users-as-functional-accounts","title":"Step 3: Compare and Mark Users as Functional Accounts","text":"<p>The following function compares Proofpoint users to your HR data and marks users as functional accounts via the API:</p> <pre><code>def mark_users_as_functional(hr_users):\n    try:\n        # Get active users from Proofpoint\n        url_to_invoke = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\n        response = requests.get(\n            url=url_to_invoke,\n            headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n        )\n        response.raise_for_status()\n        active_users_from_pp = response.json().get('users', [])\n\n        hr_emails = {user['email'].lower(): user for user in hr_users}\n        users_to_mark_functional = []\n\n        for user in active_users_from_pp:\n            if user.get('type') == 'end_user':\n                email = user['primary_email'].lower()\n                hr_user = hr_emails.get(email)\n                # Example logic: mark as functional if not in HR or if terminated\n                if not hr_user or hr_user['status'].lower() == 'terminated':\n                    users_to_mark_functional.append(user)\n\n        for user in users_to_mark_functional:\n            email_to_lookup = user['primary_email']\n            url_update = PROOFPOINT_BASE_API + f'orgs/&lt;your-domain&gt;/users/{email_to_lookup}'\n            json_update_values = {\n                \"uid\": user['uid'],\n                \"primary_email\": email_to_lookup,\n                \"is_active\": True,\n                \"type\": \"functional_account\"\n            }\n            try:\n                requests.put(\n                    url_update,\n                    json=json_update_values,\n                    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD}\n                ).raise_for_status()\n            except requests.RequestException:\n                pass\n        return True\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n</code></pre> <p>Explanation: - Retrieves all active users from Proofpoint. - Compares each user to the HR system. - Marks users as functional accounts if they are not in HR or are terminated. - Updates are made via the Proofpoint API.</p>"},{"location":"proofpoint-user-management/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    hr_data = get_latest_hr_data()\n    mark_users_as_functional(hr_data)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"proofpoint-user-management/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the process of marking functional accounts in Proofpoint Essentials, optimizing your licensing and compliance posture. The approach is secure, repeatable, and adaptable to any enterprise environment.</p> <p>For more details, consult the Proofpoint Essentials API documentation.</p>"},{"location":"sap-concur-expense-reports-aggregation/","title":"Automating SAP Concur Expense Report Aggregation and Adaptive Card Notifications","text":""},{"location":"sap-concur-expense-reports-aggregation/#introduction","title":"Introduction","text":"<p>This article provides a comprehensive, company-agnostic walkthrough for automating SAP Concur expense report aggregation and delivering actionable, interactive notifications to managers using Adaptive Cards. We\u2019ll cover:</p> <ul> <li>Securely connecting to SAP Concur with OAuth2</li> <li>Fetching and processing users and expense reports</li> <li>Aggregating by employee and by full management chain (organization-wide rollup)</li> <li>Creating and sending Adaptive Card emails with summary/detail toggles</li> <li>All supporting functions, with code and explanations</li> </ul> <p>By the end, you\u2019ll be able to connect to your own SAP Concur instance and deliver organization-wide expense insights to managers in a modern, interactive format.</p>"},{"location":"sap-concur-expense-reports-aggregation/#1-connecting-to-sap-concur-api","title":"1. Connecting to SAP Concur API","text":"<p>To fetch expense reports, you need to: - Obtain an OAuth2 access token using your SAP Concur client credentials and refresh token. - Use the access token to call the Concur API endpoints for users and expense reports.</p>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_scope","title":"Supporting Function: <code>get_scope()</code>","text":"<p>SAP Concur APIs require a specific OAuth2 scope string. This function returns the required scope for all expense and user operations:</p> <pre><code>def get_scope():\n    return (\n        \"openid USER user.read user.write LIST spend.list.read spend.listitem.read CONFIG EXPRPT FISVC \"\n        \"creditcardaccount.read IMAGE expense.exchangerate.writeonly profile.user.generaluser.read \"\n        \"profile.user.generalemployee.read expense.report.read expense.report.readwrite spend.list.write \"\n        \"spend.listitem.write identity.user.ids.read identity.user.core.read identity.user.coresensitive.read \"\n        \"identity.user.enterprise.read identity.user.event.read\"\n    )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_access_token","title":"Supporting Function: <code>get_access_token()</code>","text":"<p>This function retrieves an OAuth2 access token using your client ID, secret, and refresh token:</p> <pre><code>def get_access_token():\n    try:\n        return get_authentication_token(\n            client_id=SAP_CONCUR_CLIENT_APP_ID,\n            client_secret=SAP_CONCUR_CLIENT_SECRET,\n            refresh_token=SAP_CONCUR_REFRESH_TOKEN,\n            scope=get_scope(),\n        )\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_authentication_token","title":"Supporting Function: <code>get_authentication_token()</code>","text":"<p>Handles the actual OAuth2 token request:</p> <pre><code>def get_authentication_token(client_id, client_secret, refresh_token, scope):\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n        \"scope\": scope,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n    response = requests.post(SAP_CONCUR_OAUTH_END_POINT, headers=headers, data=data)\n    response.raise_for_status()\n    return response.json().get(\"access_token\")\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_cached_access_token","title":"Supporting Function: <code>get_cached_access_token()</code>","text":"<p>Caches the access token to avoid unnecessary requests:</p> <pre><code>access_token_cache = {\"token\": None, \"expires_at\": None}\n\ndef get_cached_access_token():\n    if access_token_cache[\"token\"] and access_token_cache[\"expires_at\"] &gt; datetime.now(timezone.utc):\n        return access_token_cache[\"token\"]\n    new_token = get_access_token()\n    if new_token:\n        access_token_cache[\"token\"] = new_token\n        access_token_cache[\"expires_at\"] = datetime.now(timezone.utc) + timedelta(hours=1)\n    return new_token\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#2-fetching-users-and-expense-reports","title":"2. Fetching Users and Expense Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_all_sap_concur_users","title":"Supporting Function: <code>get_all_sap_concur_users()</code>","text":"<p>Fetches all users from SAP Concur (with pagination):</p> <pre><code>def get_all_sap_concur_users():\n    try:\n        access_token = get_cached_access_token()\n        base_url = \"https://us.api.concursolutions.com/profile/identity/v4.1/Users\"\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\"\n        }\n        all_users = []\n        next_cursor = None\n        while True:\n            url = base_url\n            if next_cursor:\n                url += f\"?cursor={next_cursor}\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n            all_users.extend(data.get(\"items\", []))\n            next_cursor = data.get(\"nextCursor\")\n            if not next_cursor:\n                break\n        return all_users\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_expense_reports","title":"Supporting Function: <code>fetch_expense_reports()</code>","text":"<p>Fetches all expense reports for a given user:</p> <pre><code>def fetch_expense_reports(user_name, query_parameters):\n    access_token = get_cached_access_token()\n    base_url = f\"https://us.api.concursolutions.com/api/v3.0/expense/reports\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\", \"Accept\": \"application/json\"}\n    reports = []\n    next_page = f\"{base_url}?user={user_name}{query_parameters}\"\n    while next_page:\n        response = requests.get(next_page, headers=headers)\n        response.raise_for_status()\n        data = response.json()\n        reports.extend(data.get(\"Items\", []))\n        next_page = data.get(\"NextPage\")\n    return reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_all_expense_reports","title":"Supporting Function: <code>fetch_all_expense_reports()</code>","text":"<p>Fetches all reports for all users:</p> <pre><code>def fetch_all_expense_reports(user_mappings, query_parameters):\n    all_reports = []\n    for user in user_mappings:\n        reports = fetch_expense_reports(user, query_parameters)\n        for report in reports:\n            report[\"UserId\"] = user\n        all_reports.extend(reports)\n    return all_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#3-processing-and-aggregating-reports","title":"3. Processing and Aggregating Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-process_reports","title":"Supporting Function: <code>process_reports()</code>","text":"<p>Normalizes report data for aggregation:</p> <pre><code>def process_reports(all_reports):\n    return [\n        {\n            \"UserId\": report.get(\"UserId\"),\n            \"Name\": report.get(\"Name\"),\n            \"Total\": report.get(\"Total\"),\n            \"CurrencyCode\": report.get(\"CurrencyCode\"),\n            \"SubmitDate\": report.get(\"SubmitDate\"),\n            \"OwnerLoginID\": report.get(\"OwnerLoginID\"),\n            \"OwnerName\": report.get(\"OwnerName\"),\n            \"ApproverLoginID\": report.get(\"ApproverLoginID\"),\n            \"ApproverName\": report.get(\"ApproverName\"),\n            \"ApprovalStatusName\": report.get(\"ApprovalStatusName\"),\n            \"ApprovalStatusCode\": report.get(\"ApprovalStatusCode\"),\n            \"PaymentStatusName\": report.get(\"PaymentStatusName\"),\n            \"PaymentStatusCode\": report.get(\"PaymentStatusCode\"),\n            \"LastModifiedDate\": report.get(\"LastModifiedDate\"),\n            \"AmountDueEmployee\": report.get(\"AmountDueEmployee\"),\n            \"AmountDueCompanyCard\": report.get(\"AmountDueCompanyCard\"),\n            \"TotalClaimedAmount\": report.get(\"TotalClaimedAmount\"),\n            \"TotalApprovedAmount\": report.get(\"TotalApprovedAmount\"),\n            \"LedgerName\": report.get(\"LedgerName\"),\n            \"PolicyID\": report.get(\"PolicyID\"),\n            \"EverSentBack\": report.get(\"EverSentBack\"),\n            \"HasException\": report.get(\"HasException\"),\n            \"URI\": report.get(\"URI\"),\n        }\n        for report in all_reports\n    ]\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-employee-aggregate_expense_reports_by_employee","title":"Aggregating by Employee: <code>aggregate_expense_reports_by_employee()</code>","text":"<p>Groups and sums expense reports for each employee, optionally by approval status or by individual report.</p> <pre><code>def aggregate_expense_reports_by_employee(processed_reports, summary):\n    employee_reports = {}\n    for report in processed_reports:\n        user_name = str(report.get(\"OwnerLoginID\", \"\") or \"\").lower()\n        report_name = report.get(\"Name\", \"\")\n        report_id = report.get(\"ReportID\", \"\")\n        approval_status_code = str(report.get(\"ApprovalStatusCode\", \"\") or \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        key = (\n            f\"{approval_status_code}-({approval_status_name})\"\n            if summary\n            else f\"{report_name}-({report_id})-{approval_status_code}-({approval_status_name})\"\n        )\n        total = report.get(\"Total\", 0)\n        employee_reports.setdefault(user_name, {}).setdefault(key, 0)\n        employee_reports[user_name][key] += total\n    return employee_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-organization-aggregate_expense_reports_by_full_oraganization","title":"Aggregating by Organization: <code>aggregate_expense_reports_by_full_oraganization()</code>","text":"<p>Rolls up expense totals for each manager, including all direct and indirect reports, using a recursive helper.</p> <pre><code>def aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, summary):\n    object_organization_reports = {}\n    # Build a reverse mapping of manager to their direct reports\n    manager_to_reports = {}\n    for employee, details in management_upns.items():\n        manager = details.get(\"manager\")\n        if manager:\n            manager_to_reports.setdefault(manager.lower(), []).append(employee.lower())\n    def aggregate_totals_upwards(manager, visited):\n        if manager in visited:\n            return\n        visited.add(manager)\n        if manager not in object_organization_reports:\n            object_organization_reports[manager] = {}\n        for employee in manager_to_reports.get(manager, []):\n            aggregate_totals_upwards(employee, visited)\n            for status, total in object_organization_reports.get(employee, {}).items():\n                if status not in object_organization_reports[manager]:\n                    object_organization_reports[manager][status] = 0\n                object_organization_reports[manager][status] += total\n    # Populate initial totals for each employee based on processed reports\n    for report in processed_reports:\n        if report.get(\"UserManager\"):\n            user_manager = report.get(\"UserManager\", \"\").lower()\n        else:\n            continue\n        approval_status_code = report.get(\"ApprovalStatusCode\", \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        user_name = report.get(\"OwnerLoginID\", \"\")\n        key = f\"{approval_status_code}-({approval_status_name})\" if summary else f\"{user_name}-{approval_status_code}-({approval_status_name})\"\n        total = report.get(\"Total\", 0)\n        if user_manager not in object_organization_reports:\n            object_organization_reports[user_manager] = {}\n        if key not in object_organization_reports[user_manager]:\n            object_organization_reports[user_manager][key] = 0\n        object_organization_reports[user_manager][key] += total\n    # Aggregate totals upwards starting from all unique managers\n    visited = set()\n    for manager in manager_to_reports.keys():\n        aggregate_totals_upwards(manager, visited)\n    return object_organization_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#4-creating-adaptive-card-emails-summary-vs-detail-toggle","title":"4. Creating Adaptive Card Emails (Summary vs. Detail Toggle)","text":"<p>Adaptive Cards are JSON payloads that Outlook and Teams can render as interactive UI. Here\u2019s how to create a card with a summary and a toggle for details:</p> <pre><code>def create_adaptive_info_card_for_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    try:\n        summary_total = summary_by_organization.get(manager_email, 0)\n        detail_items = [\n            {\n                \"type\": \"TextBlock\",\n                \"text\": f\"{user}: {summary_by_employee.get(user, 0):,.2f}\",\n                \"wrap\": True\n            }\n            for user in detail_by_organization.get(manager_email, [])\n        ]\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.4\",\n            \"body\": [\n                {\"type\": \"TextBlock\", \"text\": \"Expense Report Summary\", \"weight\": \"Bolder\", \"size\": \"Large\"},\n                {\"type\": \"TextBlock\", \"text\": f\"Total for your organization: {summary_total:,.2f}\", \"wrap\": True},\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Click below to view details.\",\n                    \"wrap\": True,\n                    \"spacing\": \"Medium\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"id\": \"detailsContainer\",\n                    \"isVisible\": False,\n                    \"items\": detail_items\n                }\n            ],\n            \"actions\": [\n                {\n                    \"type\": \"Action.ToggleVisibility\",\n                    \"title\": \"Show/Hide Details\",\n                    \"targetElements\": [\"detailsContainer\"]\n                }\n            ]\n        }\n        return adaptive_card\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#sending-the-adaptive-card-email","title":"Sending the Adaptive Card Email","text":"<pre><code>def send_adaptive_info_email_to_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    adaptive_card = create_adaptive_info_card_for_manager(\n        manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports\n    )\n    email_payload = {\n        \"message\": {\n            \"subject\": \"Expense Report Summary\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                    f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                    f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                    f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n        }\n    }\n    send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#5-end-to-end-workflow-example","title":"5. End-to-End Workflow Example","text":"<p>Here\u2019s a high-level workflow you can adapt:</p> <pre><code>def main():\n    # 1. Fetch management hierarchy from your HR system\n    management_upns = fetch_management_upns()  # {employee: {\"manager\": manager_email, ...}}\n    # 2. Fetch all SAP Concur users\n    sap_concur_users = get_all_sap_concur_users()\n    # 3. Fetch all expense reports for all users\n    all_reports = fetch_all_expense_reports(sap_concur_users, \"&amp;submitDateAfter=2025-01-01\")\n    # 4. Normalize and process reports\n    processed_reports = process_reports(all_reports)\n    # 5. Aggregate by employee and organization\n    summary_by_employee = aggregate_expense_reports_by_employee(processed_reports, True)\n    summary_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, True)\n    detail_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, False)\n    # 6. Send Adaptive Card emails to each manager\n    for manager_email in summary_by_organization:\n        send_adaptive_info_email_to_manager(\n            manager_email, summary_by_employee, summary_by_organization, detail_by_organization, processed_reports\n        )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#references","title":"References","text":"<ul> <li>SAP Concur API Reference</li> <li>Microsoft Adaptive Cards</li> <li>Microsoft Graph API for Sending Mail</li> </ul>"},{"location":"sap-concur-expense-reports-aggregation/#conclusion","title":"Conclusion","text":"<p>With these patterns and supporting functions, you can connect to your own SAP Concur instance, fetch and aggregate expense reports by employee and by full reporting chain, and deliver actionable, interactive notifications to managers using Adaptive Cards. This enables powerful, organization-wide financial insights and automated reporting for managers at every level.</p>"},{"location":"sap-rfc-python-container/","title":"Installing the <code>PyRFC</code> Module for SAP Integration: A Step-by-Step Guide","text":"<p>Integrating Python with SAP systems using the <code>PyRFC</code> module can unlock powerful automation and data access capabilities. This article provides a clear, professional walkthrough for setting up the SAP NetWeaver RFC SDK and building the <code>PyRFC</code> Python package from scratch.</p>"},{"location":"sap-rfc-python-container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the SAP NetWeaver RFC SDK (download from the official SAP website)</li> <li>Basic familiarity with Linux command line</li> <li>Python 3.x and administrative privileges on your system</li> </ul>"},{"location":"sap-rfc-python-container/#1-download-the-netweaver-rfc-sdk","title":"1. Download the NetWeaver RFC SDK","text":"<ul> <li>Download the latest NetWeaver RFC SDK from the SAP website.</li> <li>Place the downloaded file (<code>nwrfc750P_14-70002752.zip</code>) in your repository's <code>assets</code> folder for easy access.</li> </ul>"},{"location":"sap-rfc-python-container/#2-prepare-the-sap-sdk-directory","title":"2. Prepare the SAP SDK Directory","text":"<p>Create the target directory for the SAP SDK:</p> <pre><code>sudo mkdir -p /usr/local/sap/\n</code></pre>"},{"location":"sap-rfc-python-container/#3-extract-and-copy-the-sdk","title":"3. Extract and Copy the SDK","text":"<ul> <li>Extract the <code>nwrfcsdk</code> folder from the ZIP file.</li> <li>Copy the extracted <code>nwrfcsdk</code> folder to <code>/usr/local/sap/</code>.</li> </ul>"},{"location":"sap-rfc-python-container/#4-configure-the-library-path","title":"4. Configure the Library Path","text":"<p>Create a configuration file for the dynamic linker and add the SDK library path:</p> <pre><code>sudo nano /etc/ld.so.conf.d/nwrfcsdk.conf\n\n# Add the following line to the file:\n/usr/local/sap/nwrfcsdk/lib\n</code></pre>"},{"location":"sap-rfc-python-container/#5-update-the-library-cache-and-set-environment-variable","title":"5. Update the Library Cache and Set Environment Variable","text":"<p>Update the system's library cache and set the required environment variable:</p> <pre><code>sudo ldconfig\n# Verify the path configuration should not have any error(s)\nldconfig -p | grep sap\n# Set Environment Variable\nexport SAPNWRFC_HOME=/usr/local/sap/nwrfcsdk\n</code></pre>"},{"location":"sap-rfc-python-container/#6-install-cython-and-build-essentials","title":"6. Install Cython and Build Essentials","text":"<p>Install the necessary build tools and Python dependencies:</p> <pre><code>pip install Cython\nsudo apt-get update\nsudo apt-get install -y build-essential python3-dev\n</code></pre>"},{"location":"sap-rfc-python-container/#7-build-and-install-pyrfc","title":"7. Build and Install <code>pyrfc</code>","text":"<p>Clone the PyRFC repository and build the package:</p> <pre><code>git clone https://github.com/SAP/PyRFC.git\ncd PyRFC\npython -m pip install --upgrade build\nPYRFC_BUILD_CYTHON=yes python -m build --wheel --sdist --outdir dist\npip install --upgrade --no-index --find-links=dist pyrfc\n</code></pre> <p>Pro Tip: Double-check all paths and environment variables before building. For troubleshooting, consult the PyRFC documentation or reach out to the SAP community forums.</p> <p>By following these steps, you\u2019ll have a working Python-to-SAP integration environment using the <code>pyrfc</code> module. Happy coding!</p>"},{"location":"sap-rfc-python/","title":"Calling SAP RFC Function Modules from Python Using PyRFC: A Step-by-Step Guide","text":"<p>Note: For details on installing and configuring the <code>PyRFC</code> module inside a container, see the companion article: Installing the PyRFC Module for SAP Integration</p>"},{"location":"sap-rfc-python/#introduction","title":"Introduction","text":"<p>SAP ECC systems expose powerful RFC (Remote Function Call) interfaces that allow external programs to interact with SAP data and business logic. Python, with the help of the PyRFC library, makes it possible to call these RFC function modules directly and process the results in a modern, flexible way.</p> <p>This article demonstrates how to: - Connect to an SAP ECC 6.0 (EHP 8) system from Python - Call a custom RFC function module  - Pass parameters to the RFC - Retrieve tabular data - Save the results to a CSV file</p> <p>We will use a modular, production-ready approach inspired by real-world enterprise integration scripts.</p>"},{"location":"sap-rfc-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an SAP ECC system with a custom RFC function module you can call</li> <li>SAP user credentials with RFC permissions</li> <li>The PyRFC library installed (see Installing the PyRFC Module for SAP Integration for setup)</li> <li>Python 3.7+</li> </ul>"},{"location":"sap-rfc-python/#example-extracting-data-from-sap-via-rfc","title":"Example: Extracting Data from SAP via RFC","text":"<p>Suppose you want to extract financial data from SAP using a custom RFC function module. The following example shows how to do this in a robust, reusable way.</p>"},{"location":"sap-rfc-python/#1-define-your-rfc-connection-and-extract-configuration","title":"1. Define Your RFC Connection and Extract Configuration","text":"<pre><code>from pyrfc import Connection, LogonError, ABAPApplicationError, ABAPRuntimeError\nimport csv\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"sap_rfc_extract\")\n\n# --- RFC Connection Parameters (replace with your SAP system details) ---\nSAP_CONN_PARAMS = {\n    'ashost': 'SAP_APP_SERVER_HOST',   # SAP application server\n    'sysnr': '00',                     # System number\n    'client': '100',                   # Client number\n    'user': 'SAP_USERNAME',            # SAP user\n    'passwd': 'SAP_PASSWORD',          # SAP password\n    'lang': 'EN',                      # Language\n}\n\n# --- RFC Extract Configuration ---\nEXTRACT_CONFIG = {\n    'example_extract': {\n        'function_module': 'ZMY_CUSTOM_RFC_MODULE',  # Replace with your RFC FM name\n        'table_name': 'IT_RESULT_TAB',              # The table returned by the RFC\n        'params': ['IM_CC', 'IM_YEAR', 'IM_PERIOD'],\n        'default_params': {'IM_CC': '1000', 'IM_YEAR': '2025', 'IM_PERIOD': '05'},\n        'filename_fmt': 'sap_extract_{cc}_{year}_{period}.csv',\n    },\n}\n</code></pre>"},{"location":"sap-rfc-python/#2-utility-functions-for-rfc-calls-and-csv-export","title":"2. Utility Functions for RFC Calls and CSV Export","text":"<pre><code>def call_rfc(conn_params, function_module, params):\n    try:\n        conn = Connection(**conn_params)\n        logger.info(f\"Calling RFC: {function_module} with params: {params}\")\n        return conn.call(function_module, **params)\n    except LogonError as e:\n        logger.error(f\"Logon Error: {e}\")\n    except ABAPApplicationError as e:\n        logger.error(f\"ABAP Application Error: {e}\")\n    except ABAPRuntimeError as e:\n        logger.error(f\"ABAP Runtime Error: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n    return None\n\ndef export_result_to_csv(table_data, filename):\n    if not table_data:\n        logger.warning(\"No data to export.\")\n        return\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=table_data[0].keys())\n        writer.writeheader()\n        writer.writerows(table_data)\n    logger.info(f\"Exported data to {filename}\")\n</code></pre>"},{"location":"sap-rfc-python/#3-main-script-running-the-extract","title":"3. Main Script: Running the Extract","text":"<pre><code>import argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run SAP RFC extract via PyRFC.\")\n    parser.add_argument('--im_cc', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_CC'], help='Company code')\n    parser.add_argument('--im_year', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_YEAR'], help='Fiscal year')\n    parser.add_argument('--im_period', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_PERIOD'], help='Fiscal period')\n    args = parser.parse_args()\n\n    # Prepare parameters for RFC call\n    params = {\n        'IM_CC': args.im_cc,\n        'IM_YEAR': args.im_year,\n        'IM_PERIOD': args.im_period,\n    }\n\n    config = EXTRACT_CONFIG['example_extract']\n    result = call_rfc(SAP_CONN_PARAMS, config['function_module'], params)\n    if result and config['table_name'] in result:\n        # Build filename\n        filename = config['filename_fmt'].format(\n            cc=args.im_cc, year=args.im_year, period=args.im_period\n        )\n        export_result_to_csv(result[config['table_name']], filename)\n    else:\n        logger.error(\"No data returned from RFC or table not found in result.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sap-rfc-python/#4-running-the-script","title":"4. Running the Script","text":"<p>You can run the script from the command line, specifying parameters as needed:</p> <pre><code>python sap_rfc_extract.py --im_cc=1000 --im_year=2025 --im_period=05\n</code></pre> <ul> <li>The script will connect to SAP, call the RFC, and save the results to a CSV file (e.g., <code>sap_extract_1000_2025_05.csv</code>).</li> <li>You can override any parameter using the command line.</li> </ul>"},{"location":"sap-rfc-python/#5-step-by-step-explanation","title":"5. Step-by-Step Explanation","text":"<ol> <li>Configuration:</li> <li>All SAP connection details and extract metadata are defined at the top for easy maintenance.</li> <li> <p>The RFC function module name and table name are generic placeholders\u2014replace them with your actual SAP details.</p> </li> <li> <p>Calling the RFC:</p> </li> <li>The <code>call_rfc</code> function establishes a connection and calls the RFC, handling common SAP errors.</li> <li> <p>Parameters are passed as a dictionary, matching the RFC signature.</p> </li> <li> <p>Exporting Data:</p> </li> <li> <p>The <code>export_result_to_csv</code> function writes the returned table to a CSV file, using the first row's keys as headers.</p> </li> <li> <p>Command-Line Interface:</p> </li> <li> <p>The script uses <code>argparse</code> to allow easy parameter overrides from the command line.</p> </li> <li> <p>Error Handling:</p> </li> <li>All errors are logged, and the script will not crash on SAP or network errors.</li> </ol>"},{"location":"sap-rfc-python/#conclusion","title":"Conclusion","text":"<p>With this approach, you can easily: - Call any SAP RFC function module from Python - Parameterize your extracts - Save results to CSV for downstream processing - Integrate SAP data into modern Python workflows</p> <p>For more advanced scenarios (multi-company code loops, dynamic extract configuration, etc.), see the full project code or reach out for further examples.</p>"},{"location":"sap-rfc-python/#further-reading","title":"Further Reading","text":"<ul> <li>PyRFC Documentation</li> <li>SAP RFC SDK</li> <li>Installing the PyRFC Module for SAP Integration \u2014 How to install and configure PyRFC in a container</li> </ul>"},{"location":"sharepoint-site-library-enumeration/","title":"SharePoint Files and Folders Inventory with Python and Microsoft Graph API","text":""},{"location":"sharepoint-site-library-enumeration/#introduction","title":"Introduction","text":"<p>This article provides a detailed, company-agnostic guide to inventorying all files and folders across all SharePoint sites and document libraries in a Microsoft 365 tenant using Python and the Microsoft Graph API. It focuses on the <code>get_all_files_from_sp</code> function and its supporting functions, with best practices for handling large environments, including recommendations for running the code in an Azure container.</p>"},{"location":"sharepoint-site-library-enumeration/#microsoft-graph-api-endpoint-constant","title":"Microsoft Graph API Endpoint Constant","text":"<p>The code uses the following constant for all Microsoft Graph API v1.0 calls:</p> <p><pre><code>AZURE_GRAPH_V1 = 'https://graph.microsoft.com/v1.0/'\n</code></pre> This ensures all API requests are made to the correct Microsoft Graph endpoint.</p>"},{"location":"sharepoint-site-library-enumeration/#key-functions-and-code-walkthrough","title":"Key Functions and Code Walkthrough","text":""},{"location":"sharepoint-site-library-enumeration/#1-get_all_files_from_sp","title":"1. <code>get_all_files_from_sp</code>","text":"<p>This is the main orchestration function for SharePoint inventory. It: - Retrieves all root SharePoint sites using <code>get_all_sp_sites</code>. - Expands the list to include all subsites with <code>fetch_all_sites_including_subsites</code>. - Iterates through every site and its document libraries, calling <code>process_document_library</code> for each. - Sends notification emails on progress and completion.</p> <p>Full Function Code: <pre><code>def get_all_files_from_sp():\n    try:\n        gdep_sharepoint_root_sites = get_all_sp_sites()\n        gdep_all_sites = fetch_all_sites_including_subsites(gdep_sharepoint_root_sites)\n\n        for site in gdep_all_sites:\n            site_id = site[\"id\"]\n            site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives\"\n\n            document_libraries = execute_odata_query_get(site_url)\n            for library in document_libraries:\n                process_document_library(site_id, library[\"id\"], library[\"name\"], gdep_all_sites)\n                send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n                    subject=f'Completed Doc Lib --&gt;{library[\"name\"]} - on site {site[\"webUrl\"]}',\n                    plain_message=f'Update on SP Library{library[\"name\"]} - for site {site_url}')\n\n        send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n            subject=f'Finished all Sites',\n            plain_message=f'Finished all Sites')\n\n    except Exception as e:\n        handle_global_exception(inspect.currentframe().f_code.co_name, e)\n    finally:\n        pass\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#explanation","title":"Explanation","text":"<ul> <li>Site Discovery: Uses <code>get_all_sp_sites()</code> to get all root sites, then <code>fetch_all_sites_including_subsites()</code> to get all subsites.</li> <li>Document Library Enumeration: For each site, queries all document libraries (drives) and processes them.</li> <li>Progress Notification: Sends emails after each library and when all sites are complete.</li> <li>Error Handling: All exceptions are logged and reported.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#2-get_all_sp_sites","title":"2. <code>get_all_sp_sites</code>","text":"<p>Fetches all root-level SharePoint sites in the tenant using the Microsoft Graph API: <pre><code>def get_all_sp_sites():\n    url = f\"{AZURE_GRAPH_V1}sites?search=*\"\n    return execute_odata_query_get(url)\n</code></pre> - Purpose: Returns a list of all root SharePoint sites. - API Used: List SharePoint Sites</p>"},{"location":"sharepoint-site-library-enumeration/#3-fetch_all_sites_including_subsites","title":"3. <code>fetch_all_sites_including_subsites</code>","text":"<p>Recursively discovers all subsites for each root site: <pre><code>def fetch_all_sites_including_subsites(sharepoint_root_sites):\n    all_sites = []\n    for site in sharepoint_root_sites:\n        logger.info(f\"Started site {site['webUrl']}\")\n        all_sites.append({\"id\": site[\"id\"], \"webUrl\": site[\"webUrl\"]})\n        sharepoint_subsites = get_sp_subsites(site[\"id\"])\n        if len(sharepoint_subsites) &gt; 0:\n            for subsite in sharepoint_subsites:\n                all_sites.append({\"id\": subsite[\"id\"], \"webUrl\": subsite[\"webUrl\"]})\n    return all_sites\n</code></pre> - Purpose: Ensures every site and subsite is included in the inventory. - API Used: List Subsites</p>"},{"location":"sharepoint-site-library-enumeration/#4-process_document_library","title":"4. <code>process_document_library</code>","text":"<p>Processes each document library (drive) for a site: <pre><code>def process_document_library(site_id, drive_id, drive_name, all_sites):\n    data = []\n    logger.info(f\"Started Document Library -- {drive_name}\")\n    start_time = time.perf_counter()\n    site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives/{drive_id}/root/delta{DOCUMENT_LIB_SELECT_QUERY}\"\n    search_results = execute_odata_query_get(site_url)\n    for item in search_results:\n        entry = {\n            \"site_id\": site_id,\n            \"webUrl\": next(site[\"webUrl\"] for site in all_sites if site[\"id\"] == site_id),\n            \"drive_id\": drive_id,\n            \"document_id\": item[\"id\"],\n            \"name\": item[\"name\"],\n            \"lastModifiedDateTime\": parse_iso_date(item.get(\"lastModifiedDateTime\")),\n            \"size\": item.get(\"size\") if \"file\" in item else \"\"\n        }\n        data.append(entry)\n    write_data_to_csv(data, SP_WITHOUT_VERSION_CSV_FILE_PATH)\n    elapsed_time = time.perf_counter() - start_time\n    logger.info(f\"Document Library '{drive_name}' took {elapsed_time:.2f} seconds to process.\")\n</code></pre> - Purpose:   - Queries all files in the document library using the Graph API delta endpoint.   - Collects metadata for each file.   - Writes results to a CSV for further processing or database import.   - Logs processing time for performance monitoring.</p>"},{"location":"sharepoint-site-library-enumeration/#supporting-utilities-full-implementations","title":"Supporting Utilities (Full Implementations)","text":""},{"location":"sharepoint-site-library-enumeration/#execute_odata_query_geturl","title":"<code>execute_odata_query_get(url)</code>","text":"<p>Handles authenticated GET requests to the Microsoft Graph API, including error handling and token refresh. <pre><code>def execute_odata_query_get(url):\n    try:\n        token = get_access_token_API_Access_AAD()\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json().get(\"value\", [])\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#parse_iso_datedate_str","title":"<code>parse_iso_date(date_str)</code>","text":"<p>Converts ISO 8601 date strings to Python datetime objects for easier manipulation and formatting. <pre><code>def parse_iso_date(date_str: str):\n    if not date_str:\n        return None\n    date_str = date_str.rstrip('Z')\n    formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\"]\n    for fmt in formats:\n        try:\n            return datetime.strptime(date_str, fmt).date()\n        except ValueError:\n            continue\n    return None\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#write_data_to_csvdata-file_path","title":"<code>write_data_to_csv(data, file_path)</code>","text":"<p>Appends data to a CSV file, writing headers if the file does not exist. <pre><code>def write_data_to_csv(data, file_path):\n    file_exists = os.path.isfile(file_path)\n    with open(file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n        fieldnames = [\"site_id\", \"webUrl\", \"drive_id\", \"document_id\", \"name\", \"lastModifiedDateTime\", \"size\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerows(data)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handle_global_exceptionfunctionname-exceptionobject","title":"<code>handle_global_exception(functionName, exceptionObject)</code>","text":"<p>Logs and emails details of any exception that occurs. <pre><code>def handle_global_exception(functionName, exceptionObject):\n    emailBody = f\"Function Name: {functionName}; Exception Description: {exceptionObject}\"\n    send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n               subject='Exception occured in code', \n               plain_message=emailBody)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#get_access_token_api_access_aadscopesnone","title":"<code>get_access_token_API_Access_AAD(scopes=None)</code>","text":"<p>Obtains an access token for Microsoft Graph API using MSAL or Azure Identity. (Example implementation:) <pre><code>def get_access_token_API_Access_AAD(scopes=None):\n    if scopes is None:\n        scopes = ['https://graph.microsoft.com/.default']\n    app = ConfidentialClientApplication(\n        client_id=AZURE_CONFIDENTIAL_APP_ID,\n        authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n        client_credential=AZURE_CONFIDENTIAL_SECRET\n    )\n    result = app.acquire_token_for_client(scopes=scopes)\n    return result[\"access_token\"]\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handling-large-sharepoint-environments","title":"Handling Large SharePoint Environments","text":"<p>Important: Large tenants with many sites, subsites, and document libraries can have tens or hundreds of thousands of files. Processing all of them can take significant time and resources.</p>"},{"location":"sharepoint-site-library-enumeration/#best-practices-for-large-document-libraries","title":"Best Practices for Large Document Libraries","text":"<ul> <li>Run in Azure: For large environments, it is highly recommended to run this inventory code in an Azure Container Instance or Azure VM. This ensures:</li> <li>Sufficient compute and memory resources.</li> <li>Proximity to Microsoft 365 services for faster API calls.</li> <li>Ability to scale or schedule the job as needed.</li> <li>Batch Processing: The code is designed to process and write data in batches, minimizing memory usage and allowing for partial progress in case of interruptions.</li> <li>Progress Notifications: The function sends email notifications after each document library and when all sites are complete, so you can monitor long-running jobs.</li> <li>Error Handling: All exceptions are logged and reported, ensuring that issues with individual sites or libraries do not halt the entire process.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#example-end-to-end-inventory-flow","title":"Example: End-to-End Inventory Flow","text":"<ol> <li>Discover Sites:</li> <li><code>get_all_sp_sites()</code> \u2192 returns all root sites.</li> <li>Expand to Subsites:</li> <li><code>fetch_all_sites_including_subsites()</code> \u2192 returns all sites and subsites.</li> <li>Process Each Library:</li> <li>For each site, enumerate all document libraries and call <code>process_document_library()</code>.</li> <li>Write Results:</li> <li>Metadata for each file is written to a CSV file for further analysis or database import.</li> </ol>"},{"location":"sharepoint-site-library-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API: List SharePoint Sites</li> <li>Microsoft Graph API: List Drive Items</li> <li>Azure Container Instances Documentation</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#conclusion","title":"Conclusion","text":"<p>The <code>get_all_files_from_sp</code> function and its supporting helpers provide a robust, scalable way to inventory all files and folders across a Microsoft 365 tenant's SharePoint environment. For large tenants, running this code in an Azure container or VM is strongly recommended to ensure reliability and performance.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/","title":"Copying Files from SharePoint to Azure File Share at Scale","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#overview","title":"Overview","text":"<p>This article provides a comprehensive, company-agnostic guide for copying large volumes of files from SharePoint Online document libraries into Azure File Shares. The solution is designed for high-throughput, scalable execution (e.g., as an Azure Container Instance), and is suitable for enterprise-scale migrations, backups, or data archiving. The approach leverages multi-threading for performance and handles large files (30\u201350 GB+) efficiently by streaming and chunked uploads.</p> <p>Key Features: - Secure, certificate-based authentication to Microsoft Graph and Azure - Multi-threaded file copy for high throughput - Chunked upload for large files - Robust error handling and progress tracking - All secrets managed via Azure Key Vault</p> <p>Note: This article assumes you have already extracted the list of files and their metadata from SharePoint. For details on how to enumerate SharePoint files and extract metadata, see Extracting SharePoint Document Library Metadata.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Azure File Share and connection string</li> <li>Azure AD App Registration with certificate-based authentication</li> <li>Azure Key Vault for secret management</li> <li>Extracted metadata for all SharePoint files to be copied (see stub above)</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#solution-architecture","title":"Solution Architecture","text":"<ol> <li>Metadata Extraction: Retrieve all file metadata from SharePoint (site ID, drive ID, item ID, file path, size, timestamps, etc.) and store in a database or CSV. (See stub above)</li> <li>File Copy Process: For each file, download from SharePoint using Microsoft Graph and upload to Azure File Share, preserving directory structure and metadata.</li> <li>Multi-threading: Use a thread pool to process multiple files in parallel for maximum throughput.</li> <li>Chunked Upload: For large files, stream and upload in chunks to avoid memory issues and support files up to 100s of GB.</li> <li>Progress Tracking: Log and track progress for monitoring and troubleshooting.</li> </ol>"},{"location":"sharepoint-site-library-to-azure-fileshare/#full-python-code-example","title":"Full Python Code Example","text":"<p>Below is a complete, production-ready script for the file copy process. All company-specific values have been removed. Replace stub values and secret names as appropriate for your environment.</p> <pre><code>\"\"\"\nThis script copies files from SharePoint Online to Azure File Share using multi-threading and chunked uploads.\n- Designed for high-volume, large-file scenarios (30\u201350 GB+)\n- All secrets are retrieved from Azure Key Vault\n- Can be run as an Azure Container Instance (ACI) or VM\n\"\"\"\nimport threading\nimport requests\nfrom queue import Queue\nfrom urllib.parse import unquote\nimport os\nfrom azure.storage.fileshare import ShareFileClient, ShareDirectoryClient\nfrom datetime import datetime, timedelta, timezone, time\nfrom msal import ConfidentialClientApplication\n\nfrom gdepcommon.logger import setup_logger\nfrom gdepcommon.utils import (\n    get_azure_kv_sceret,\n    sql_dbconnection,\n    PFX_CERTIFICATE_NAME,\n    PFX_CERTIFICATE_NAME_TP\n)\nfrom gdepazure.common import (\n    AZURE_CONFIDENTIAL_APP_ID,\n    AZURE_TENANT_ID,\n    AZURE_AUTHORITY_BASE_URL,\n    AZURE_GRAPH_DEFAULT_RESOURCE\n)\n\n# Thread and chunk parameters\nTHREAD_COUNT = 10  # Tune based on environment\nCHUNK_SIZE = 4 * 1024 * 1024  # 4 MB\n\n# Shared progress state\ntotal_files = 0\nprocessed_files = 0\nlock = threading.Lock()\n\n# Token management\naccess_token = None\ntoken_expiry_time = None\n\ndef refresh_access_token(logger):\n    \"\"\"Refreshes the Microsoft Graph access token using certificate-based auth.\"\"\"\n    global access_token, token_expiry_time\n    try:\n        logger.info(\"Refreshing access token...\")\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n        app = ConfidentialClientApplication(\n            client_id=AZURE_CONFIDENTIAL_APP_ID,\n            authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n        result = app.acquire_token_for_client(scopes=AZURE_GRAPH_DEFAULT_RESOURCE)\n        access_token = result[\"access_token\"]\n        expires_in = result[\"expires_in\"]\n        token_expiry_time = datetime.now(tz=timezone.utc) + timedelta(seconds=expires_in - 60)\n        logger.info(\"Access token refreshed successfully.\")\n    except Exception as e:\n        logger.error(f\"Failed to refresh access token: {e}\")\n        raise Exception(\"Access token refresh failed.\")\n\ndef get_access_token(logger):\n    \"\"\"Returns a valid access token, refreshing if expired.\"\"\"\n    global access_token, token_expiry_time\n    if not access_token or datetime.now(tz=timezone.utc) &gt;= token_expiry_time:\n        for attempt in range(3):\n            try:\n                refresh_access_token(logger)\n                break\n            except Exception as e:\n                if attempt &lt; 2:\n                    logger.warning(f\"Retrying token refresh (attempt {attempt + 1}/3)...\")\n                else:\n                    raise e\n    return access_token\n\ndef ensure_directory_path_exists(azure_conn_str, share_name, directory_path, cache=None):\n    \"\"\"Ensures the full directory path exists in Azure File Share.\"\"\"\n    if cache is None:\n        cache = set()\n    parts = directory_path.strip('/').split('/')\n    current_path = ''\n    for part in parts:\n        current_path = f\"{current_path}/{part}\" if current_path else part\n        if current_path in cache:\n            continue\n        dir_client = ShareDirectoryClient.from_connection_string(\n            conn_str=azure_conn_str,\n            share_name=share_name,\n            directory_path=current_path\n        )\n        try:\n            dir_client.create_directory()\n            cache.add(current_path)\n        except Exception as ex:\n            if \"ResourceAlreadyExists\" in str(ex):\n                cache.add(current_path)\n            else:\n                raise\n\ndef copy_file_from_sp_to_azure(site_id, drive_id, item_id, azure_file_client, total_file_size_in_bytes, created_date=None, modified_date=None, logger=None):\n    \"\"\"Streams a file from SharePoint and uploads to Azure File Share in chunks.\"\"\"\n    access_token = get_access_token(logger)\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/items/{item_id}/content\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    retries = 3\n    for attempt in range(retries):\n        try:\n            logger.info(f\"Starting file copy for item_id: {item_id}\")\n            with requests.get(url, headers=headers, stream=True) as response:\n                response.raise_for_status()\n                azure_file_client.create_file(size=total_file_size_in_bytes)\n                offset = 0\n                for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n                    azure_file_client.upload_range(data=chunk, offset=offset, length=len(chunk))\n                    offset += len(chunk)\n                    # Log progress for large files (&gt;5GB) every 10%\n                    if total_file_size_in_bytes &gt; 5 * 1024 * 1024 * 1024:\n                        percent_complete = (offset / total_file_size_in_bytes) * 100\n                        if int(offset / CHUNK_SIZE) % int((total_file_size_in_bytes / CHUNK_SIZE) * 0.05) == 0:\n                            logger.info(f\"File {item_id}: {percent_complete:.2f}% complete\")\n            # Set file properties for created/modified dates\n            if created_date or modified_date:\n                file_properties = {}\n                if created_date:\n                    created_datetime = datetime.combine(created_date, time())\n                    file_properties['file_creation_time'] = created_datetime\n                if modified_date:\n                    modified_datetime = datetime.combine(modified_date, time())\n                    file_properties['file_last_write_time'] = modified_datetime\n                from azure.storage.fileshare import ContentSettings\n                content_settings = ContentSettings(content_type=\"application/octet-stream\")\n                azure_file_client.set_http_headers(file_attributes=\"none\", content_settings=content_settings, **file_properties)\n            logger.info(f\"Successfully copied file for item_id: {item_id}\")\n            return\n        except requests.exceptions.RequestException as e:\n            if attempt &lt; retries - 1:\n                logger.warning(f\"Retrying file copy for item_id {item_id} (attempt {attempt + 1}/{retries})...\")\n            else:\n                logger.error(f\"Failed to copy file for item_id: {item_id}. Error: {e}\")\n                raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during file copy for item_id {item_id}: {e}\")\n\ndef worker(queue, azure_conn_str, share_name, created_dirs, logger):\n    \"\"\"Thread worker function: processes files from the queue.\"\"\"\n    global processed_files\n    while not queue.empty():\n        try:\n            file_record = queue.get()\n            site_id = file_record['site_id']\n            drive_id = file_record['drive_id']\n            item_id = file_record['item_id']\n            total_file_size_in_bytes = file_record['length']\n            created_date = file_record['created_date']\n            modified_date = file_record['modified_date']\n            decoded_path = file_record['decoded_path']\n            # Ensure directory exists\n            azure_directory_path = os.path.dirname(decoded_path)\n            if azure_directory_path:\n                ensure_directory_path_exists(azure_conn_str, share_name, azure_directory_path, created_dirs)\n            file_client = ShareFileClient.from_connection_string(\n                conn_str=azure_conn_str,\n                share_name=share_name,\n                file_path=decoded_path\n            )\n            copy_file_from_sp_to_azure(site_id, drive_id, item_id, file_client, total_file_size_in_bytes, created_date, modified_date, logger)\n            # Mark as copied in DB\n            with sql_dbconnection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"UPDATE [dbo].[Fact_Document_Library_Details] SET [copied] = 1 WHERE [unique_id] = ?\", file_record['unique_id'])\n                conn.commit()\n            with lock:\n                global processed_files\n                processed_files += 1\n                overall_progress = (processed_files / total_files) * 100\n                logger.info(f\"Overall Progress: {overall_progress:.2f}% ({processed_files}/{total_files} files complete)\")\n            queue.task_done()\n        except Exception as e:\n            logger.error(f\"Error processing file: {e}\")\n\ndef main(logger):\n    \"\"\"\n    Main entry point: loads file metadata, initializes threads, and starts the copy process.\n    - Loads Azure File Share connection string from Key Vault\n    - Loads file metadata (site_id, drive_id, item_id, file path, size, timestamps, etc.)\n    - Spawns worker threads to process the file queue\n    - Tracks and logs progress\n    \"\"\"\n    global total_files\n    try:\n        azure_conn_str = get_azure_kv_sceret('your-azure-file-connection-string-secret')\n        share_name = \"your-azure-file-share-name\"\n        site_id = \"your-sharepoint-site-id\"\n        drive_id = \"your-sharepoint-drive-id\"\n        created_dirs = set()\n        # Fetch files to copy (replace with your DB or CSV logic)\n        with sql_dbconnection() as sqlConnection:\n            cursor = sqlConnection.cursor()\n            cursor.execute(\"SELECT * FROM [dbo].[Fact_Document_Library_Details] WHERE [type] = 'file' AND [copied] = 0\")\n            results = cursor.fetchall()\n        total_files = len(results)\n        if total_files == 0:\n            logger.info(\"No files to process.\")\n            return\n        queue = Queue()\n        for row in results:\n            sp_relative_url = row.server_relative_url\n            decoded_path = unquote(sp_relative_url[len(\"/sites/YourSite/YourLibrary\"):].lstrip('/'))\n            queue.put({\n                'site_id': site_id,\n                'drive_id': drive_id,\n                'item_id': row.unique_id,\n                'unique_id': row.unique_id,\n                'length': row.length,\n                'created_date': row.time_created,\n                'modified_date': row.time_last_modified,\n                'decoded_path': decoded_path\n            })\n        threads = []\n        for _ in range(THREAD_COUNT):\n            thread = threading.Thread(target=worker, args=(queue, azure_conn_str, share_name, created_dirs, logger))\n            thread.start()\n            threads.append(thread)\n        for thread in threads:\n            thread.join()\n        logger.info(\"All files have been processed successfully.\")\n    except Exception as e:\n        logger.error(f\"Error in main function: {e}\")\n\nif __name__ == \"__main__\":\n    logger = setup_logger(\"sp2azfileshare\", \"/mnt/azure/logs/sp2azfileshare.log\")\n    main(logger)\n</code></pre>"},{"location":"sharepoint-site-library-to-azure-fileshare/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#1-authentication-and-secret-management","title":"1. Authentication and Secret Management","text":"<ul> <li>All secrets (Azure connection string, certificate thumbprint, etc.) are retrieved from Azure Key Vault using a utility function (<code>get_azure_kv_sceret</code>).</li> <li>Microsoft Graph authentication uses certificate-based credentials for security and automation.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#2-multi-threaded-file-copy","title":"2. Multi-Threaded File Copy","text":"<ul> <li>The script uses a thread pool (<code>THREAD_COUNT</code>) and a <code>Queue</code> to distribute file copy tasks across multiple threads.</li> <li>Each thread processes files independently, ensuring high throughput and efficient use of resources.</li> <li>Thread-safe progress tracking is implemented using a <code>Lock</code>.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#3-chunked-upload-for-large-files","title":"3. Chunked Upload for Large Files","text":"<ul> <li>Files are streamed from SharePoint and uploaded to Azure File Share in 4 MB chunks.</li> <li>This approach supports very large files (30\u201350 GB+) without excessive memory usage.</li> <li>Progress for large files is logged every 10% (configurable).</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#4-directory-structure-and-metadata-preservation","title":"4. Directory Structure and Metadata Preservation","text":"<ul> <li>The script ensures that the full directory path exists in Azure File Share before uploading each file.</li> <li>File creation and modification timestamps are preserved if available.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#5-database-integration-and-idempotency","title":"5. Database Integration and Idempotency","text":"<ul> <li>The script marks each file as copied in the database after successful upload, ensuring idempotency and resumability.</li> <li>You can adapt this logic to use a CSV or other metadata store as needed.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#scaling-and-running-in-azure","title":"Scaling and Running in Azure","text":"<ul> <li>This script is designed to run as an Azure Container Instance (ACI), but can also be run on VMs or Kubernetes.</li> <li>Tune <code>THREAD_COUNT</code> based on available CPU and network bandwidth.</li> <li>For very large migrations, consider splitting the workload across multiple containers or jobs.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#related-articles","title":"Related Articles","text":"<ul> <li>Extracting SharePoint Document Library Metadata for Automation</li> <li>Automating Secure Secret Management with Azure Key Vault</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#conclusion","title":"Conclusion","text":"<p>By following this guide and using the provided code, you can efficiently and securely copy massive volumes of files from SharePoint Online to Azure File Share, with full support for large files, multi-threaded performance, and robust error handling. All sensitive information is managed via Azure Key Vault, ensuring compliance and security for enterprise automation scenarios.</p>"},{"location":"sharepoint-sites-enumeration/","title":"How to Retrieve All SharePoint Sites in Your Microsoft 365 Tenant","text":""},{"location":"sharepoint-sites-enumeration/#introduction","title":"Introduction","text":"<p>Retrieving a complete list of SharePoint sites in your Microsoft 365 (M365) tenant is essential for IT automation, reporting, and governance. This article provides a detailed, company-agnostic, step-by-step guide to programmatically enumerate all SharePoint sites using Python and the Microsoft Graph API. All code samples are generic and ready to use in any tenant.</p>"},{"location":"sharepoint-sites-enumeration/#prerequisites","title":"Prerequisites","text":""},{"location":"sharepoint-sites-enumeration/#1-azure-entra-application-registration","title":"1. Azure Entra Application Registration","text":"<ul> <li>Register an application in Azure Entra (Azure AD).</li> <li>Assign the following Microsoft Graph API permissions:</li> <li><code>Sites.Read.All</code> (Application permission)</li> <li><code>Sites.ReadWrite.All</code> (if you need to write/update)</li> <li>Grant admin consent for these permissions.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#2-certificate-based-authentication","title":"2. Certificate-Based Authentication","text":"<ul> <li>Upload a certificate to your Azure Entra application.</li> <li>Use the certificate thumbprint and private key for authentication.</li> <li>For a detailed guide and code on certificate-based authentication, see: Certificate Auth for Microsoft Graph API</li> </ul>"},{"location":"sharepoint-sites-enumeration/#3-python-environment","title":"3. Python Environment","text":"<ul> <li>Install the required packages:   <pre><code>pip install requests msal\n</code></pre></li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-1-authenticate-and-get-an-access-token","title":"Step 1: Authenticate and Get an Access Token","text":"<p>You need to authenticate as your Azure Entra application and obtain an access token for Microsoft Graph. This is best done using certificate-based authentication for security.</p> <p>Below is a full, reusable function for certificate-based authentication. (Replace the placeholders with your actual values.)</p> <pre><code>import msal\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    # Replace these with your app's values\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n</code></pre> <p>See this blog post for a full explanation and troubleshooting tips for certificate-based authentication.</p>"},{"location":"sharepoint-sites-enumeration/#step-2-query-the-microsoft-graph-api-for-sharepoint-sites","title":"Step 2: Query the Microsoft Graph API for SharePoint Sites","text":"<p>The Microsoft Graph API endpoint to list all sites is:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites?search=*\n</code></pre> <p>This returns a paginated list of root SharePoint sites in your tenant.</p>"},{"location":"sharepoint-sites-enumeration/#helper-function-execute-odata-query","title":"Helper Function: Execute OData Query","text":"<pre><code>import requests\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#retrieve-all-sites-with-pagination","title":"Retrieve All Sites (with Pagination)","text":"<pre><code>def get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation","title":"Explanation:","text":"<ul> <li><code>get_all_sp_sites</code> starts with the root search URL.</li> <li>It uses the access token for authentication.</li> <li>It loops through all pages using the <code>@odata.nextLink</code> property for pagination.</li> <li>All sites are collected in the <code>sites</code> list.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-3-retrieve-subsites-for-each-site","title":"Step 3: Retrieve Subsites for Each Site","text":"<p>To enumerate subsites for a given site, use:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{site-id}/sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#function-to-get-subsites","title":"Function to Get Subsites","text":"<pre><code>def get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation_1","title":"Explanation:","text":"<ul> <li>For each site, call <code>get_sp_subsites(site_id)</code> to get its direct subsites.</li> <li>You can recursively call this function to build a full site tree.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-4-full-example-enumerate-all-sites-and-subsites","title":"Step 4: Full Example - Enumerate All Sites and Subsites","text":"<p>Here is a complete script you can copy, edit, and run in your own environment:</p> <pre><code>import msal\nimport requests\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n\ndef get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n\ndef enumerate_all_sites_and_subsites():\n    all_sites = get_all_sp_sites()\n    all_sites_with_subsites = []\n    for site in all_sites:\n        site_id = site['id']\n        subsites = get_sp_subsites(site_id)\n        site['subsites'] = subsites\n        all_sites_with_subsites.append(site)\n    return all_sites_with_subsites\n\nif __name__ == \"__main__\":\n    all_sites = enumerate_all_sites_and_subsites()\n    print(json.dumps(all_sites, indent=2))\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#step-by-step-code-walkthrough","title":"Step-by-Step Code Walkthrough","text":"<ol> <li>get_access_token_API_Access_AAD: Authenticates using your Azure Entra app and certificate, returning a valid access token for Microsoft Graph.</li> <li>execute_odata_query_get: Sends a GET request to the specified Microsoft Graph endpoint using the access token, returning the parsed JSON response.</li> <li>get_all_sp_sites: Uses the <code>/sites?search=*</code> endpoint to retrieve all root SharePoint sites, handling pagination.</li> <li>get_sp_subsites: For each site, retrieves its direct subsites.</li> <li>enumerate_all_sites_and_subsites: Combines the above to build a list of all sites and their subsites.</li> <li>Main block: Runs the enumeration and prints the result as formatted JSON.</li> </ol>"},{"location":"sharepoint-sites-enumeration/#required-permissions-recap","title":"Required Permissions Recap","text":"<ul> <li><code>Sites.Read.All</code> (Application permission, admin consent required)</li> <li>The Azure Entra app must be granted consent by a tenant admin</li> <li>The app must authenticate using a certificate or secret (certificate recommended)</li> </ul>"},{"location":"sharepoint-sites-enumeration/#troubleshooting-and-tips","title":"Troubleshooting and Tips","text":"<ul> <li>If you get a 403 error, check that your app registration has admin consent for <code>Sites.Read.All</code>.</li> <li>If you get a 401 error, check your certificate and app credentials.</li> <li>The <code>search=*</code> parameter is required to enumerate all sites, not just the root site.</li> <li>For large tenants, always handle pagination using <code>@odata.nextLink</code>.</li> <li>You can extend the code to recursively enumerate subsites to any depth.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API - List sites</li> <li>Microsoft Graph API - List subsites</li> <li>Microsoft Graph permissions reference</li> <li>Register an application with the Microsoft identity platform</li> <li>Certificate credentials for application authentication</li> <li>MSAL for Python documentation</li> <li>Microsoft Graph Explorer</li> </ul>"},{"location":"sharepoint-sites-enumeration/#summary","title":"Summary","text":"<ul> <li>Register an Azure Entra application and grant it <code>Sites.Read.All</code> permission</li> <li>Authenticate using a certificate (see this blog post)</li> <li>Use the Microsoft Graph API <code>/sites?search=*</code> endpoint to enumerate all SharePoint sites</li> <li>Use <code>/sites/{site-id}/sites</code> to enumerate subsites</li> <li>Handle pagination using <code>@odata.nextLink</code></li> </ul> <p>This approach is secure, scalable, and works in any Microsoft 365 tenant. You can now automate SharePoint site inventory, reporting, or governance tasks in your own environment.</p>"},{"location":"ukg-api-employee-verification/","title":"Validating Employee Data Consistency Between UKG and Your HR System","text":""},{"location":"ukg-api-employee-verification/#introduction","title":"Introduction","text":"<p>When integrating HR data between systems such as ADP (or any HRIS) and UKG Dimensions, it's critical to ensure that the data loaded into UKG matches the source-of-truth in your HR system. This is especially important when using middleware (like Dell Boomi) for automated data loads. This article provides a step-by-step, company-agnostic guide to:</p> <ul> <li>Connect securely to the UKG API</li> <li>Retrieve employee data from UKG</li> <li>Retrieve employee data from your HR system (e.g., ADP)</li> <li>Compare the two datasets for validation</li> <li>Report discrepancies for remediation</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment.</p>"},{"location":"ukg-api-employee-verification/#why-validate-data-after-integration","title":"Why Validate Data After Integration?","text":"<p>Automated integrations (e.g., via Dell Boomi) can occasionally result in mismatches due to mapping errors, transformation issues, or upstream data changes. Validating data post-load ensures:</p> <ul> <li>Data integrity between systems</li> <li>Early detection of integration or mapping issues</li> <li>Compliance with audit requirements</li> <li>Improved trust in downstream business processes</li> </ul>"},{"location":"ukg-api-employee-verification/#solution-overview","title":"Solution Overview","text":"<ol> <li>Connect to UKG API: Use secure credentials (ideally from Azure Key Vault or similar) to authenticate and retrieve employee data from UKG.</li> <li>Connect to HR System: Query your HR system (e.g., ADP) for the same set of employees.</li> <li>Compare Data: Match employees by a unique identifier (e.g., email or employee ID) and compare key fields.</li> <li>Report Results: Output a report of discrepancies for review and correction.</li> </ol>"},{"location":"ukg-api-employee-verification/#step-1-securely-connect-to-the-ukg-api","title":"Step 1: Securely Connect to the UKG API","text":"<p>UKG Dimensions provides a REST API for programmatic access. Authentication typically uses OAuth2 with client credentials. Credentials should be stored securely (e.g., Azure Key Vault).</p> <pre><code>import requests\nimport urllib.parse\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\ndef get_ukg_environment_credentials(environment):\n    if environment == 'PROD':\n        return (\n            get_azure_kv_sceret('ukg-base-uri'),\n            get_azure_kv_sceret('ukg-api-username'),\n            get_azure_kv_sceret('ukg-api-password'),\n            get_azure_kv_sceret('ukg-api-key'),\n            get_azure_kv_sceret('ukg-api-client-id'),\n            get_azure_kv_sceret('ukg-api-client-secret')\n        )\n    else:\n        # Use your non-production secrets\n        ...\n\ndef get_token_apikey_and_uri(environment):\n    base_uri, username, password, api_key, client_id, client_secret = get_ukg_environment_credentials(environment)\n    access_token_uri = base_uri + 'api/authentication/access_token'\n    payload = (\n        f\"username={urllib.parse.quote(username)}&amp;\"\n        f\"password={urllib.parse.quote(password)}&amp;\"\n        f\"client_id={urllib.parse.quote(client_id)}&amp;\"\n        f\"client_secret={urllib.parse.quote(client_secret)}&amp;\"\n        \"grant_type=password&amp;auth_chain=OAuthLdapService\"\n    )\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'appkey': api_key\n    }\n    response = requests.post(access_token_uri, headers=headers, data=payload)\n    response.raise_for_status()\n    response_json = response.json()\n    return response_json[\"access_token\"], base_uri, api_key\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The function requests an OAuth2 access token from UKG. - The token is used for all subsequent API calls.</p>"},{"location":"ukg-api-employee-verification/#step-2-retrieve-employee-data-from-ukg","title":"Step 2: Retrieve Employee Data from UKG","text":"<p>Once authenticated, you can call the UKG API to retrieve employee details. The following function demonstrates how to fetch all employees and their details:</p> <pre><code>import requests\n\ndef get_all_employees(environment):\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    employees = []\n    # Example endpoint for listing employees (adjust as needed for your UKG tenant)\n    url = base_uri + 'api/v1/commons/persons'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    employees = response.json().get('persons', [])\n    return employees\n</code></pre> <p>Explanation: - Calls the UKG API endpoint to list all employees. - Returns a list of employee records as dictionaries.</p>"},{"location":"ukg-api-employee-verification/#step-3-retrieve-employee-data-from-your-hr-system-eg-adp","title":"Step 3: Retrieve Employee Data from Your HR System (e.g., ADP)","text":"<p>Assuming you have a database or API access to your HR system, you can retrieve employee data for comparison. Here is a sample function for fetching from a SQL database:</p> <pre><code>def fetch_users_from_hr():\n    sql_query = \"\"\"\n        SELECT email, position_id, status, associate_id, last_name, first_name, location, pay_rate_code, reports_to\n        FROM hr_employees\n    \"\"\"\n    # Replace with your actual DB query logic\n    results = execute_Select_SQL_statement(sql_query)[0]\n    return {\n        row[0].lower(): {\n            \"position_id\": row[1],\n            \"status\": row[2],\n            \"associate_id\": row[3],\n            \"last_name\": row[4],\n            \"first_name\": row[5],\n            \"location\": row[6],\n            \"pay_rate_code\": row[7],\n            \"reports_to\": row[8]\n        } for row in results\n    }\n</code></pre> <p>Explanation: - Queries the HR system for employee data. - Returns a dictionary keyed by email for easy lookup.</p>"},{"location":"ukg-api-employee-verification/#step-4-compare-and-validate-employee-data","title":"Step 4: Compare and Validate Employee Data","text":"<p>Now, compare the two datasets and report any discrepancies. Here is a function that does this and saves the results to a CSV file:</p> <pre><code>def compare_ukg_and_hr_employees(ukg_employees, hr_employees):\n    processed_details = []\n    for employee in ukg_employees:\n        username = employee.get('user', {}).get('userAccount', {}).get('userName', None)\n        if username and username.lower() in hr_employees:\n            hr_user = hr_employees[username.lower()]\n            # Compare fields as needed\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': hr_user.get('status'),\n                # Add more fields as needed\n            })\n        else:\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': 'Not Found',\n            })\n    # Save to CSV for review\n    save_list_to_csv(processed_details, 'ukg_hr_comparison.csv')\n</code></pre> <p>Explanation: - For each UKG employee, attempts to find a match in the HR system by email/username. - Compares relevant fields and records the results. - Outputs a CSV file for review.</p>"},{"location":"ukg-api-employee-verification/#step-5-full-example-putting-it-all-together","title":"Step 5: Full Example \u2013 Putting It All Together","text":"<p>Here is a complete example that ties all the steps together:</p> <pre><code>def main(environment='PROD'):\n    ukg_employees = get_all_employees(environment)\n    hr_employees = fetch_users_from_hr()\n    compare_ukg_and_hr_employees(ukg_employees, hr_employees)\n    print(\"Comparison complete. Results saved to ukg_hr_comparison.csv.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-api-employee-verification/#benefits-of-post-load-validation","title":"Benefits of Post-Load Validation","text":"<ul> <li>Data Quality: Ensures that the data loaded into UKG matches your HR system.</li> <li>Early Issue Detection: Quickly identifies mapping or integration errors.</li> <li>Audit Readiness: Provides evidence of data integrity for compliance.</li> <li>Continuous Improvement: Enables ongoing monitoring and process improvement.</li> </ul>"},{"location":"ukg-api-employee-verification/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the validation of employee data between UKG and your HR system, regardless of your integration platform. This approach is scalable, secure, and adaptable to any enterprise environment.</p> <p>For further enhancements, consider automating the process to run after each integration cycle and integrating with your alerting or ticketing system for proactive remediation.</p>"},{"location":"ukg-api-integration-and-dataview/","title":"Automating UKG Dimensions Integrations and DataView Exports with Python","text":""},{"location":"ukg-api-integration-and-dataview/#introduction","title":"Introduction","text":"<p>UKG Dimensions (formerly Kronos) provides powerful APIs for automating data extraction and integration tasks. This article demonstrates how to:</p> <ul> <li>Programmatically execute a predefined integration (such as a payroll export) in UKG Dimensions and retrieve the output file.</li> <li>Programmatically extract data from an existing DataView using a Hyperfind query.</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment. This guide is company-agnostic and can be adapted to your own UKG tenant.</p>"},{"location":"ukg-api-integration-and-dataview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to UKG Dimensions APIs (with appropriate permissions)</li> <li>A predefined integration (e.g., Payroll Export) already set up in your UKG tenant</li> <li>An existing DataView in UKG Dimensions</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Secure storage for API credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#1-executing-a-predefined-integration-in-ukg-dimensions","title":"1. Executing a Predefined Integration in UKG Dimensions","text":"<p>UKG Dimensions allows you to define integrations (such as payroll exports) via the UI. These integrations can be triggered and monitored via API, and the resulting files can be downloaded programmatically.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_payroll_hours","title":"Python Function: <code>fetch_and_store_payroll_hours</code>","text":"<pre><code>def fetch_and_store_payroll_hours(environment, week_start, week_end, week_start_datetime, week_end_datetime):\n    import uuid, json, time, csv\n    from io import StringIO\n    # ... import your secret and DB utilities ...\n    unique_id = 'Automation-' + str(uuid.uuid4())\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    # 1. Trigger the integration\n    dimensions_api_uri = base_uri + 'api/v1/platform/integrations/4/execute'\n    payload = json.dumps({\n        \"integrationParameters\": [\n            {\"name\": \"Symbolic Period\", \"value\": {'symbolicPeriod': {'id': '0'}, 'startDate': week_start + '.000Z', 'endDate': week_end + '.000Z'}},\n            {\"name\": \"Summary File Name\", \"value\": \"AutomationPayrollSummaryExport.csv\"},\n            {\"name\": \"Hyperfind ID\", \"value\": {'hyperfind': {'id': '1304'}}},\n            {\"name\": \"Ignore Sign Off\", \"value\": False},\n            {\"name\": \"File Name\", \"value\": \"automationpayrollexport.csv\"}\n        ],\n        \"name\": unique_id\n    })\n    response = requests.post(dimensions_api_uri, headers=headers, data=payload).json()\n    # 2. Poll for completion\n    execution_id = response['id']\n    status_url = base_uri + f'api/v1/platform/integration_executions/{execution_id}'\n    while True:\n        status_response = requests.get(status_url, headers=headers).json()\n        if status_response['status'] == 'Completed':\n            break\n        time.sleep(60)  # Wait before polling again\n    # 3. Download the output file\n    file_url = status_url + '/file'\n    params = {'file_name': 'automationpayrollexport.csv'}\n    file_response = requests.get(file_url, headers=headers, params=params)\n    data_file = StringIO(file_response.text)\n    csv_reader = csv.DictReader(data_file)\n    data_list = list(csv_reader)\n    # ... process and store data as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation","title":"Explanation","text":"<ul> <li>Trigger Integration: The function sends a POST request to the integration execution endpoint, passing required parameters (dates, file names, hyperfind, etc.).</li> <li>Poll for Completion: The function polls the execution status endpoint until the integration is complete.</li> <li>Download Output: Once complete, the output file is downloaded and parsed as CSV.</li> <li>Processing: The data can then be processed or loaded into a database as needed.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 Integrations</p>"},{"location":"ukg-api-integration-and-dataview/#2-extracting-data-from-a-dataview-using-a-hyperfind-query","title":"2. Extracting Data from a DataView Using a Hyperfind Query","text":"<p>DataViews in UKG Dimensions allow you to define custom reports. You can extract data from a DataView programmatically using the API and a Hyperfind query to filter employees.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_hours_using_dataview","title":"Python Function: <code>fetch_and_store_hours_using_dataview</code>","text":"<pre><code>def fetch_and_store_hours_using_dataview(environment, week_start_datetime, week_end_datetime):\n    import json, time, csv, os\n    # ... import your secret and DB utilities ...\n    pay_code_translation = { 'Regular': 'REG', 'Overtime 1.5': 'OT', 'Doubletime': 'DBL', ... }\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    export_url = base_uri + 'api/v1/commons/exports/async'\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    payload = json.dumps({\n        \"name\": \"UKG DV Export Pay Code\",\n        \"payLoad\": {\n            \"from\": {\n                \"view\": 0,\n                \"employeeSet\": {\n                    \"hyperfind\": {\"id\": \"1304\"},\n                    \"dateRange\": {\n                        \"startDate\": week_start_datetime.strftime(\"%Y-%m-%d\"),\n                        \"endDate\": week_end_datetime.strftime(\"%Y-%m-%d\"),\n                    }\n                },\n                \"viewPresentation\": \"People\"\n            },\n            \"select\": [\n                {\"key\": \"PEOPLE_PERSON_NUMBER\", \"alias\": \"Employee ID\", ...},\n                {\"key\": \"CORE_PAYCODE\", \"alias\": \"Pay Code Name\", ...},\n                {\"key\": \"TIMECARD_TRANS_ACTUAL_HOURS\", \"alias\": \"Actual Hours\", ...},\n                # ... more fields ...\n            ],\n            \"groupBy\": [],\n            \"where\": [],\n        },\n        \"type\": \"DATA\"\n    })\n    # 1. Trigger DataView export\n    response = requests.post(export_url, headers=headers, data=payload)\n    execution_key = response.json()['executionKey']\n    # 2. Wait for export to complete\n    time.sleep(60)\n    # 3. Download the CSV file\n    csv_export_url = base_uri + f'api/v1/commons/exports/{execution_key}/file'\n    response = requests.get(csv_export_url, headers=headers)\n    temp_file_path = '/tmp/paycodeextracttempdetail.csv'\n    with open(temp_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(response.text)\n    # ... process CSV as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_1","title":"Explanation","text":"<ul> <li>Trigger DataView Export: Sends a POST request to the DataView export endpoint with the required payload (including Hyperfind and date range).</li> <li>Wait for Completion: Waits for the export to complete (can be improved with polling).</li> <li>Download CSV: Downloads the resulting CSV file for further processing.</li> <li>Processing: The CSV can be parsed and loaded into a database or used for reporting.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 DataViews</p>"},{"location":"ukg-api-integration-and-dataview/#3-orchestrating-the-process-process_and_validate_payroll_hours","title":"3. Orchestrating the Process: <code>process_and_validate_payroll_hours</code>","text":"<p>This function coordinates the two previous steps, automating the extraction of both payroll export and DataView data for a given period. It uses the <code>fetch_period</code> function to retrieve the start and end dates for both the prior and current pay periods, and then passes these as parameters to the extraction functions.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-process_and_validate_payroll_hours-with-prior_period-and-current_period-parameters","title":"Python Function: <code>process_and_validate_payroll_hours</code> (with <code>prior_period</code> and <code>current_period</code> parameters)","text":"<pre><code>def process_and_validate_payroll_hours(environment='PROD'):\n    \"\"\"\n    Main function to process and validate payroll hours.\n    Steps:\n    1. Fetches the prior period data and stores payroll hours.\n    2. Fetches and stores hours using dataview for the prior period.\n    3. Fetches the current period data and stores hours using dataview.\n    \"\"\"\n    # Get prior period (returns tuple: start_date, end_date, start_datetime, end_datetime)\n    prior_period = fetch_period('Previous', environment)\n    # Extract and store payroll hours for prior period\n    fetch_and_store_payroll_hours(environment, *prior_period)\n    # Extract and store DataView hours for prior period\n    fetch_and_store_hours_using_dataview(environment, prior_period[2], prior_period[3])\n\n    # Get current period\n    current_period = fetch_period('Current', environment)\n    # Extract and store DataView hours for current period\n    fetch_and_store_hours_using_dataview(environment, current_period[2], current_period[3])\n\n    # Optionally, add validation or reporting here\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#about-prior_period-and-current_period","title":"About <code>prior_period</code> and <code>current_period</code>","text":"<p>The <code>fetch_period</code> function returns a tuple for each period:</p> <ul> <li><code>start_date</code> (str): Start date of the pay period (e.g., '2025-06-01')</li> <li><code>end_date</code> (str): End date of the pay period (e.g., '2025-06-15')</li> <li><code>start_datetime</code> (datetime): Start date as a Python <code>datetime</code> object</li> <li><code>end_datetime</code> (datetime): End date as a Python <code>datetime</code> object</li> </ul> <p>These are used as parameters for the extraction functions:</p> <ul> <li><code>fetch_and_store_payroll_hours(environment, start_date, end_date, start_datetime, end_datetime)</code></li> <li><code>fetch_and_store_hours_using_dataview(environment, start_datetime, end_datetime)</code></li> </ul> <p>This approach ensures that all data extraction is aligned to the correct pay periods, and makes it easy to extend the process for additional periods or custom ranges.</p>"},{"location":"ukg-api-integration-and-dataview/#example-usage","title":"Example Usage","text":"<pre><code>if __name__ == \"__main__\":\n    process_and_validate_payroll_hours(environment=\"PROD\")\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_2","title":"Explanation","text":"<ul> <li>Fetch Periods: Retrieves the date ranges for the previous and current pay periods using <code>fetch_period</code>.</li> <li>Extract Data: Calls the two extraction functions for each period, passing the correct parameters.</li> <li>Validation: (Optional) You can add logic to compare and validate the extracted data.</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#conclusion","title":"Conclusion","text":"<p>By leveraging the UKG Dimensions API, you can automate the execution of predefined integrations and the extraction of DataView data. This enables robust, repeatable, and auditable data flows for payroll, compliance, and analytics.</p> <p>For more details, consult the official UKG Dimensions API Documentation or your UKG support representative.</p>"},{"location":"ukg-sftp-file-transfer/","title":"Secure File Transfer with UKG Dimensions SFTP","text":""},{"location":"ukg-sftp-file-transfer/#introduction","title":"Introduction","text":"<p>UKG Dimensions (Kronos) provides SFTP endpoints for secure file exchange. For additional security, files are often encrypted (e.g., with PGP/GPG) before upload and must be decrypted after download. This article demonstrates how to:</p> <ul> <li>Connect to a UKG SFTP server using Python</li> <li>Download and decrypt files from UKG</li> <li>Encrypt and upload files to UKG</li> <li>Use Azure Storage (or any local mount) as your working directory</li> <li>Securely manage all credentials and keys using Azure Key Vault</li> <li>Import and manage GPG keys for file encryption/decryption</li> </ul> <p>We use Python libraries such as <code>pysftp</code>, <code>paramiko</code>, <code>gnupg</code>, and Azure SDKs to accomplish these tasks.</p>"},{"location":"ukg-sftp-file-transfer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>The following Python packages:</li> <li><code>pysftp</code> (SFTP client)</li> <li><code>paramiko</code> (SSH key handling)</li> <li><code>python-gnupg</code> (PGP encryption/decryption)</li> <li><code>azure-identity</code>, <code>azure-keyvault-secrets</code> (Azure Key Vault access)</li> <li>Access to your UKG SFTP credentials and keys (public/private, passphrase)</li> <li>GPG/PGP keys for file encryption/decryption</li> <li>A local directory (e.g., Azure Storage mount) for file staging</li> <li>Azure Key Vault for Secrets: Store all sensitive credentials (SFTP username, private key, passphrase, GPG passphrase, etc.) in Azure Key Vault and retrieve them securely at runtime. This avoids hardcoding secrets in your code or environment variables.</li> </ul> <p>Install dependencies: <pre><code>pip install pysftp paramiko python-gnupg azure-identity azure-keyvault-secrets\n</code></pre></p>"},{"location":"ukg-sftp-file-transfer/#key-constants-and-their-secure-retrieval","title":"Key Constants and Their Secure Retrieval","text":"<p>All SFTP credentials and keys are securely retrieved from Azure Key Vault using the <code>get_azure_kv_sceret</code> function. Here are the main constants and how they are constructed:</p> <ul> <li>SFTP_UKG_DATA_HOST_NAME: The SFTP server hostname (e.g., <code>'your-ukg-sftp-host.com'</code>).</li> <li>SFTP_UKG_DATA_USER_NAME: The SFTP username.</li> <li>SFTP_UKG_PUBLIC_KEY: The SFTP server's public key, retrieved and decoded as bytes:   <pre><code>SFTP_UKG_PUBLIC_KEY = bytes(get_azure_kv_sceret('ukg-sftp-host-public-key'), encoding='utf-8')\n</code></pre></li> <li>SFTP_UKG_PRIVATE_KEY: The private key for SFTP authentication, retrieved as a base64-encoded string from Key Vault, then decoded to a PEM string:   <pre><code>SFTP_UKG_PRIVATE_KEY = base64.b64decode(get_azure_kv_sceret('ukg-sftp-host-private-key')).decode('utf-8')\n</code></pre> Why base64.b64decode? <p>When storing sensitive files like private keys in Azure Key Vault, it is common to first encode them using Base64. This ensures the key is stored as a plain string (since Key Vault secrets are always strings) and avoids issues with special characters or line breaks. When retrieving the key, you must decode it back to its original binary (or PEM) format using <code>base64.b64decode</code>. This allows you to safely store and retrieve binary data (like private keys) in a text-only secret store.</p> </li> <li>SFTP_UKG_PRIVATE_KEY_PASSPHRASE: The passphrase for the private key, also retrieved from Key Vault.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#how-get_azure_kv_sceret-works","title":"How <code>get_azure_kv_sceret</code> Works","text":"<p>This function retrieves secrets from Azure Key Vault using the Azure SDK. It authenticates using environment variables for client ID, client secret, and tenant ID, then fetches the secret value by name:</p> <pre><code>def get_azure_kv_sceret(name):\n    secret = None\n    try:\n        vault_url = \"https://&lt;your-key-vault-name&gt;.vault.azure.net/\"\n        client_id = os.environ.get(\"application_interface_clientid\")\n        client_secret = os.environ.get(\"application_interface_clientsecret\")\n        tenant_id = os.environ.get(\"application_interface_tenantid\")\n        credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)\n        secret_client = SecretClient(vault_url=vault_url, credential=credential)\n        retrieved_secret = secret_client.get_secret(name)\n        secret = retrieved_secret.value\n    except Exception as e:\n        print(\"Error:\", str(e))\n    finally:\n        return secret\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#importing-and-managing-gpg-keys-for-ukg-file-encryptiondecryption","title":"Importing and Managing GPG Keys for UKG File Encryption/Decryption","text":"<p>When your container is first provisioned, you should import the GPG keys required for file encryption and decryption. The following function, typically run at container startup, retrieves the GPG public and private keys from Azure Key Vault, decodes them, saves them to disk, and imports them into the GPG keyring:</p> <pre><code>def download_and_import_gpg_keys():\n    try:\n        ukg_sftp_public_encrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-public-encrypt')).decode('utf-8')\n        ukg_sftp_private_decrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-private-decrypt')).decode('utf-8')\n        # Save public key\n        public_key_file = \"certs/public_ukg_encrypt.asc\"\n        with open(public_key_file, \"w\") as file:\n            file.write(ukg_sftp_public_encrypt)\n        # Save private key\n        private_key_file = \"certs/private_ukg_decrypt.asc\"\n        with open(private_key_file, \"w\") as file:\n            file.write(ukg_sftp_private_decrypt)\n        # Import the public key\n        subprocess.run([\"gpg\", \"--batch\", \"--yes\", \"--import\", public_key_file], check=True)\n        # Import the private key with passphrase\n        subprocess.run([\n            \"gpg\", \"--batch\", \"--yes\", \"--pinentry-mode=loopback\",\n            \"--passphrase\", os.environ[\"ukg_encrypt_passphrase\"],\n            \"--import\", private_key_file\n        ], check=True)\n        os.remove(public_key_file)\n        os.remove(private_key_file)\n    except Exception as e:\n        print(f\"Failed to import GPG keys: {e}\")\n</code></pre> <p>When to run this: - Run this function once at container startup (or as part of your provisioning script) to ensure the GPG keys are available for all encryption/decryption operations in your UKG SFTP workflows. - This ensures that all subsequent file transfers (upload/download) can use GPG seamlessly at the OS level.</p>"},{"location":"ukg-sftp-file-transfer/#sftp-connection-function-with-explanation","title":"SFTP Connection Function (with Explanation)","text":"<p>The following function establishes a secure SFTP connection to the UKG server using all the above constants. It supports both production and non-production environments:</p> <pre><code>import pysftp\nimport paramiko\nimport base64\nimport io\nimport warnings\n\ndef get_sftp_connection(environment: str) -&gt; Optional[pysftp.Connection]:\n    \"\"\"\n    Establish an SFTP connection to the UKG server.\n    Returns a pysftp.Connection object if successful, otherwise None.\n    \"\"\"\n    localConnection: Optional[pysftp.Connection] = None\n    try:\n        hostname = (SFTP_UKG_DATA_HOST_NAME if environment == 'PROD' else SFTP_UKG_DATA_HOST_NAME_NON_PROD)\n        username = (SFTP_UKG_DATA_USER_NAME if environment == 'PROD' else SFTP_UKG_DATA_USER_NAME_NON_PROD)\n        hostkey = (SFTP_UKG_PUBLIC_KEY if environment == 'PROD' else SFTP_UKG_PUBLIC_KEY_NON_PROD)\n        warnings.filterwarnings('ignore', '.*Failed to load HostKeys.*')\n        hostKey = paramiko.RSAKey(data=base64.decodebytes(hostkey))\n        cnopts = pysftp.CnOpts()\n        cnopts.hostkeys.add(hostname, 'ssh-rsa', hostKey)\n        # Convert private key string to file-like object\n        with io.StringIO(SFTP_UKG_PRIVATE_KEY) as private_key_file:\n            private_key = paramiko.RSAKey.from_private_key(private_key_file, password=SFTP_UKG_PRIVATE_KEY_PASSPHRASE)\n            localConnection = pysftp.Connection(host=hostname, username=username, private_key=private_key, cnopts=cnopts)\n    except Exception as e:\n        print(f\"SFTP connection failed: {e}\")\n    return localConnection\n</code></pre> <p>This function: - Retrieves all connection parameters and keys from Azure Key Vault. - Decodes and loads the SFTP server's public key for host verification. - Loads the private key and passphrase for authentication. - Returns a live SFTP connection object for use in upload/download operations.</p>"},{"location":"ukg-sftp-file-transfer/#downloading-and-decrypting-files-from-ukg","title":"Downloading and Decrypting Files from UKG","text":"<p>UKG may deliver files encrypted with PGP/GPG. Use <code>python-gnupg</code> to decrypt after download.</p> <pre><code>import gnupg\n\ngpg = gnupg.GPG()\nLOCAL_DOWNLOAD_DIR = '/mnt/azure/UKG/Download/'  # Example: Azure Storage mount\nREMOTE_UKG_FOLDER = './Outbound/'\n\ndef download_and_decrypt_files():\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(REMOTE_UKG_FOLDER)\n        for filename in sftp.listdir():\n            if filename.endswith('.gpg'):\n                local_path = os.path.join(LOCAL_DOWNLOAD_DIR, filename)\n                sftp.get(filename, local_path)\n                print(f\"Downloaded: {filename}\")\n                # Decrypt the file\n                with open(local_path, 'rb') as f:\n                    decrypted_data = gpg.decrypt_file(f, passphrase=os.environ[\"ukg_encrypt_passphrase\"])\n                if decrypted_data.ok:\n                    decrypted_path = local_path.replace('.gpg', '')\n                    with open(decrypted_path, 'w', encoding='utf-8') as out:\n                        out.write(str(decrypted_data))\n                    print(f\"Decrypted: {decrypted_path}\")\n                else:\n                    print(f\"Decryption failed for {filename}: {decrypted_data.status}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#encrypting-and-uploading-files-to-ukg","title":"Encrypting and Uploading Files to UKG","text":"<p>Before uploading, files must be encrypted with UKG's public key.</p> <pre><code>def encrypt_and_upload_file(local_file, remote_folder='./Inbound/'):\n    with open(local_file, 'rb') as f:\n        encrypted_data = gpg.encrypt_file(\n            f,\n            recipients=['UKG_PUBLIC_KEY_NAME'],  # Replace with UKG's GPG key name\n            always_trust=True\n        )\n    if not encrypted_data.ok:\n        print(f\"Encryption failed: {encrypted_data.status}\")\n        return\n    encrypted_file = local_file + '.gpg'\n    with open(encrypted_file, 'wb') as ef:\n        ef.write(encrypted_data.data)\n    print(f\"Encrypted: {encrypted_file}\")\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(remote_folder)\n        sftp.put(encrypted_file)\n        print(f\"Uploaded: {encrypted_file} to {remote_folder}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#putting-it-all-together","title":"Putting It All Together","text":"<p>You can automate the full workflow:</p> <pre><code>def main():\n    download_and_import_gpg_keys()  # Ensure GPG keys are imported at container startup\n    download_and_decrypt_files()\n    file_to_upload = '/mnt/azure/UKG/Upload/myfile.csv'\n    encrypt_and_upload_file(file_to_upload)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#key-points-and-best-practices","title":"Key Points and Best Practices","text":"<ul> <li>Key Management: Never hardcode sensitive keys or passphrases in your code. Use environment variables or a secure vault.</li> <li>File Cleanup: Remove decrypted/encrypted files after processing if not needed.</li> <li>Error Handling: Add robust error handling for production use.</li> <li>Azure Storage: If using Azure Files, ensure your container mounts the share with correct permissions.</li> <li>Security: Only trust files from known sources and validate signatures if possible.</li> <li>GPG Key Import: Always import GPG keys at the OS level before running any file encryption/decryption operations.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#references","title":"References","text":"<ul> <li>pysftp Documentation</li> <li>python-gnupg Documentation</li> <li>UKG Dimensions Integration Guides</li> <li>Azure Key Vault Documentation</li> </ul>"}]}