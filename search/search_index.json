{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog","text":""},{"location":"#azure","title":"Azure","text":"<ul> <li>Certificate Based Authentication</li> <li>Download Subscription Bill</li> <li>Download Azure Resource</li> </ul>"},{"location":"#cisco-meraki-nagios-xi","title":"Cisco Meraki &amp; Nagios XI","text":"<ul> <li>Cisco Meraki Devices Discovery and Nagios Integration</li> </ul>"},{"location":"#active-directory-domain-services","title":"Active Directory Domain Services","text":"<ul> <li>Create User and assign groups</li> <li>Update User</li> </ul>"},{"location":"#microsoft-365","title":"Microsoft 365","text":"<ul> <li>Outlook Actionable Adaptive Card - Part 1</li> <li>Outlook Actionable Adaptive Card - Part 2</li> <li>Sharepoint Sites Enumeration</li> <li>Sharepoint Document Library Enumeration</li> </ul>"},{"location":"#sap","title":"SAP","text":"<ul> <li>Setup PyRFC in your Container</li> <li>Concur Expense Report Aggregation</li> <li>Calling SAP RFC Function Modules from Python</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/","title":"Reviewing Consultants via Adaptive Card (Actionable Outlook Messages) \u2013 Part 1","text":"<p>In this article, we\u2019ll walk through a real-world Python implementation for reviewing consultants using Adaptive Cards in Outlook. This solution enables managers to receive an actionable email, review their consultants, and submit decisions directly from their inbox. We'll cover the end-to-end process, focusing on how to build and send an actionable Adaptive Card email using Python and Microsoft Graph.</p> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part1/#overview","title":"Overview","text":"<p>The workflow consists of the following steps:</p> <ol> <li>Fetch consultants grouped by manager using <code>fetch_manager_consultants</code>.</li> <li>Build an Adaptive Card for each manager using <code>create_adaptive_card_outlook</code>.</li> <li>Send the Adaptive Card email using <code>send_adaptive_card_email</code>.</li> </ol> <p>Let\u2019s dive into each step and the code behind it.</p>"},{"location":"adaptive-card-consultant-review-part1/#1-fetching-consultants-grouped-by-manager","title":"1. Fetching Consultants Grouped by Manager","text":"<p>The function <code>fetch_manager_consultants(frequency)</code> retrieves consultants from the database and groups them by their manager\u2019s email.</p> <pre><code>def fetch_manager_consultants(frequency):\n    \"\"\"Fetch consultants grouped by their manager's email.\"\"\"\n    get_consultants_sql_statement = get_consultants_sql(frequency)\n    consultants_data = execute_Select_SQL_statement(get_consultants_sql_statement)[0]\n    manager_to_consultants = {}\n\n    try:\n        for row in consultants_data:\n            manager_email = row[5]\n            consultant_info = {\n                \"in_adp\": row[0],\n                \"name\": row[1],\n                \"last_logon\": row[2],\n                \"email\": row[3],\n                \"last_password_change\": row[4],\n                \"hire_date\": row[6],\n            }\n            manager_to_consultants.setdefault(manager_email, []).append(consultant_info)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, Exception)\n\n    return manager_to_consultants\n</code></pre> <ul> <li>Key Points:</li> <li>The function queries the database for consultant data.</li> <li>It organizes consultants by their manager\u2019s email, returning a dictionary mapping each manager to their consultants.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#2-building-the-adaptive-card","title":"2. Building the Adaptive Card","text":"<p>The function <code>create_adaptive_card_outlook(manager_email, consultants)</code> constructs an Adaptive Card JSON payload for Outlook. This card allows managers to review each consultant and select an action (keep active or deactivate).</p> <pre><code>def create_adaptive_card_outlook(manager_email, consultants):\n    \"\"\"Create an Adaptive Card with consultant details and actions.\"\"\"\n    try:\n        manager_name = manager_email.split('@')[0].split('.')[0]  # Extract manager's first name\n        inputs = []\n        action_data = {}\n\n        for consultant in consultants:\n            consultant_id = consultant[\"email\"].replace(\"@\", \"_\").replace(\".\", \"_\")\n            last_logon = consultant['last_logon'] or 'N/A'\n            hire_date = consultant['hire_date'] or 'N/A'\n\n            inputs.extend([\n                {\n                    \"type\": \"TextBlock\",\n                    \"wrap\": True,\n                    \"weight\": \"Bolder\",\n                    \"color\": \"Warning\",\n                    \"spacing\": \"Medium\",\n                    \"text\": \"****\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"padding\": \"None\",\n                    \"spacing\": \"None\",\n                    \"items\": [\n                        {\n                            \"type\": \"TextBlock\",\n                            \"text\": f\"**{consultant['name']}** ({consultant['email']}), with Last Logon: {last_logon} and Hire Date: {hire_date}\",\n                            \"weight\": \"Bolder\",\n                            \"wrap\": True\n                        },\n                        {\n                            \"type\": \"Input.ChoiceSet\",\n                            \"id\": f\"decision_{consultant_id}\",\n                            \"isMultiSelect\": False,\n                            \"value\": \"keep\",\n                            \"choices\": [\n                                {\"title\": \"Keep Active\", \"value\": \"keep\"},\n                                {\"title\": \"Deactivate\", \"value\": \"deactivate\"}\n                            ],\n                            \"style\": \"expanded\",\n                            \"spacing\": \"None\",\n                        }\n                    ]\n                }\n            ])\n            action_data[consultant_id] = {\n                \"decision\": f\"{{{{decision_{consultant_id}.value}}}}\",\n                \"email\": consultant[\"email\"],\n                \"manageremail\": manager_email,\n                \"managername\": manager_name,\n            }\n\n        inputs.append({\n            \"type\": \"TextBlock\",\n            \"wrap\": True,\n            \"weight\": \"Bolder\",\n            \"color\": \"Warning\",\n            \"spacing\": \"Medium\",\n            \"text\": \"****\"\n        })\n\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.0\",\n            \"originator\": ORGINATOR_ID,\n            \"body\": [\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Consultant Review\",\n                    \"weight\": \"bolder\",\n                    \"size\": \"extraLarge\",\n                    \"color\": \"attention\",\n                    \"separator\": True,\n                    \"horizontalAlignment\": \"center\",\n                    \"spacing\": \"small\",\n                    \"wrap\": True\n                },\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": (\n                        f\"Hello {manager_name}, please review the details of your consultants and select the appropriate action. \"\n                        \"Some consultants may not be in the HR system as they were set up directly as Guest accounts, so their hire date will show as N/A. \"\n                        \"Please review all consultants and provide feedback so that appropriate action can be taken if they no longer require network access.\"\n                    ),\n                    \"wrap\": True,\n                    \"color\": \"Default\",\n                    \"spacing\": \"Medium\",\n                    \"weight\": \"Bolder\"\n                }\n            ] + inputs,\n            \"actions\": [\n                {\n                    \"type\": \"Action.Http\",\n                    \"title\": \"Submit Consultant Actions\",\n                    \"headers\": [\n                        {\"name\": \"Content-Type\", \"value\": \"application/json\"},\n                        {\"name\": \"Authorization\", \"value\": \"\"}\n                    ],\n                    \"method\": \"POST\",\n                    \"url\": \"https://api.example.com/consultant-review-confirmation\",\n                    \"body\": \"\"\n                }\n            ],\n            \"style\": \"default\"\n        }\n\n        # Prepare the action data for the body\n        action_data_str = json.dumps(action_data)\n        adaptive_card['actions'][0]['body'] = action_data_str\n\n        email_payload = {\n            \"message\": {\n            \"subject\": \"Consultant Review - Action Required\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n            \"bccRecipients\": [{\"emailAddress\": {\"address\": \"audit@example.com\"}}]\n            }\n        }\n\n        return email_payload\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        return None\n</code></pre> <ul> <li>Key Points:</li> <li>The card is dynamically built for each manager and their consultants.</li> <li>Each consultant has a choice set for the manager to select \"Keep Active\" or \"Deactivate\".</li> <li>The card is embedded in the email as a <code>&lt;script type='application/adaptivecard+json'&gt;...&lt;/script&gt;</code> block, which is required for actionable messages in Outlook.</li> <li>The card uses an <code>Action.Http</code> action to POST the manager\u2019s decisions to a specified endpoint.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#3-sending-the-adaptive-card-email","title":"3. Sending the Adaptive Card Email","text":"<p>The function <code>send_adaptive_card_email(email_payload)</code> sends the constructed Adaptive Card email using the Microsoft Graph API.</p> <pre><code>def send_adaptive_card_email(email_payload):\n    \"\"\"Send an email with an embedded Adaptive Card using Microsoft Graph API.\"\"\"\n    try:\n        user_id = \"your-user-guid\"\n        graph_api_url = f\"https://graph.microsoft.com/v1.0/users/{user_id}/sendMail\"\n        access_token = get_access_token_API_Access_AAD()\n\n        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n\n        response = requests.post(graph_api_url, json=email_payload, headers=headers)\n\n        if response.status_code == 202:\n            print(\"Email sent successfully!\")\n        else:\n            print(f\"Failed to send email: {response.status_code}, {response.text}\")\n\n    except requests.exceptions.RequestException as req_error:\n        handle_global_exception(sys._getframe().f_code.co_name, req_error)\n        print(f\"Request error occurred: {req_error}\")\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        print(f\"An unexpected error occurred: {error}\")\n</code></pre> <ul> <li>Key Points:</li> <li>The function authenticates using an Azure AD access token.</li> <li>It sends the email via the Microsoft Graph <code>/sendMail</code> endpoint.</li> <li>The Adaptive Card is delivered as an actionable message in Outlook.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#end-to-end-example","title":"End-to-End Example","text":"<p>Here\u2019s how you might orchestrate the process:</p> <pre><code>def process_consultants(frequency):\n    manager_to_consultants = fetch_manager_consultants(frequency)\n    for manager_email, consultants in manager_to_consultants.items():\n        email_payload = create_adaptive_card_outlook(manager_email, consultants)\n        if email_payload:\n            send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part1/#conclusion","title":"Conclusion","text":"<p>This article (Part 1) demonstrated how to:</p> <ul> <li>Fetch consultants grouped by manager.</li> <li>Build an Adaptive Card for actionable review in Outlook.</li> <li>Send the Adaptive Card email using Microsoft Graph.</li> </ul> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part2/","title":"Company-Agnostic Adaptive Card Consultant Review Blog (Part 2, Deep Dive)","text":""},{"location":"adaptive-card-consultant-review-part2/#introduction","title":"Introduction","text":"<p>In Part 1, we covered how to send actionable Adaptive Card emails for consultant review. In this Part 2, we focus on the backend: how to securely receive, verify, and process the manager's response when the Adaptive Card is submitted. This article recursively examines each function involved in the request processing chain, providing a complete, end-to-end understanding of how an incoming Adaptive Card request is handled\u2014with all relevant Python code included and all company-specific references replaced with generic placeholders (e.g., <code>mycompany.com</code>).</p>"},{"location":"adaptive-card-consultant-review-part2/#1-endpoint-consultant-review-confirmation","title":"1. Endpoint: <code>/consultant-review-confirmation</code>","text":"<p>When a manager submits the Adaptive Card, the card's action posts the data to the <code>/consultant-review-confirmation</code> endpoint:</p> <pre><code>@app.route('/consultant-review-confirmation', methods=['POST'])\ndef consultant_review_confirmation():\n    try:\n        payload, client_ip, error_response, status_code = process_request_headers_and_payload(request)\n        if error_response:\n            return error_response, status_code\n        process_adaptive_card_payload(payload, client_ip)\n        return jsonify({\"status\": \"success\", \"message\": \"Actions processed successfully\"}), 200\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>This route does two things: 1. Verifies the request and extracts the payload using <code>process_request_headers_and_payload</code>. 2. Processes the submitted data using <code>process_adaptive_card_payload</code>.</p>"},{"location":"adaptive-card-consultant-review-part2/#2-deep-dive-process_request_headers_and_payload","title":"2. Deep Dive: <code>process_request_headers_and_payload</code>","text":"<p>This function is responsible for: - Extracting and logging request headers. - Validating the JWT Bearer token in the <code>Action-Authorization</code> header. - Decoding the token and verifying its authenticity. - Extracting the JSON payload from the request.</p> <pre><code>def process_request_headers_and_payload(request):\n    headers = dict(request.headers)\n    logger.info(f\"Request headers: {headers}\")\n    action_auth_header = headers.get(\"Action-Authorization\", \"\")\n    client_ip = headers.get(\"X-Forwarded-For\", \"\")\n    logger.info(f\"Incoming request from IP: {client_ip}\")\n    logger.info(f\"Action Authorization: {action_auth_header}\")\n    if not action_auth_header.startswith(\"Bearer \"):\n        logger.error(f\"Missing or invalid Bearer token in Action-Authorization header from {client_ip}\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Missing Bearer token\"}), 401\n    token = action_auth_header.split(\" \", 1)[1]\n    log_jwt_payload(token)\n    public_key = fetch_public_key(token)\n    if not public_key:\n        logger.error(\"Public key not found!\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    if not validate_token(token, public_key):\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    payload = request.get_json()\n    logger.info(f\"Payload: {payload}\")\n    return payload, client_ip, None, None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#21-log_jwt_payloadtoken","title":"2.1. <code>log_jwt_payload(token)</code>","text":"<p>Logs the decoded JWT payload (without verifying the signature) for debugging and traceability.</p> <pre><code>def log_jwt_payload(token):\n    \"\"\"Logs the decoded JWT payload without verification.\"\"\"\n    payload = jwt.decode(token, options={\"verify_signature\": False})\n    for key, value in payload.items():\n        logger.info(f\"{key}: {value}\")\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#22-fetch_public_keytoken","title":"2.2. <code>fetch_public_key(token)</code>","text":"<p>Extracts the key ID (<code>kid</code>) from the JWT header, fetches the public keys from the identity provider's JWKS endpoint, and finds the matching key for signature verification.</p> <pre><code>def fetch_public_key(token):\n    \"\"\"Fetches the public key for the given token.\"\"\"\n    try:\n        header = jwt.get_unverified_header(token)\n        key_id = header.get(\"kid\")\n        jwks_url = 'https://substrate.office.com/sts/common/discovery/keys'  # Replace with your IdP's JWKS endpoint if needed\n        jwks = requests.get(jwks_url).json()\n        for key in jwks[\"keys\"]:\n            if key[\"kid\"] == key_id:\n                return RSAAlgorithm.from_jwk(json.dumps(key))\n    except Exception as e:\n        raise Exception(f\"Error fetching public key: {e}\")\n    return None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#23-validate_tokentoken-public_key","title":"2.3. <code>validate_token(token, public_key)</code>","text":"<p>Decodes and verifies the JWT signature using the public key, checks the token's issuer and audience, and raises an error if the token is expired or invalid.</p> <pre><code>def validate_token(token, public_key):\n    \"\"\"Validates the JWT token using the public key.\"\"\"\n    try:\n        decoded_token = jwt.decode(\n            token, public_key, algorithms=[\"RS256\"], audience=\"https://api.mycompany.com\"\n        )\n        if decoded_token.get(\"iss\") != \"https://substrate.office.com/sts/\":  # Replace with your IdP's issuer if needed\n            raise Exception(\"Invalid issuer!\")\n        return True\n    except jwt.ExpiredSignatureError:\n        raise Exception(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        raise Exception(\"Invalid token!\")\n</code></pre> <p>Summary: Only requests with a valid JWT token (issued by your identity provider) are accepted. The payload is only processed if authentication passes. All actions are logged for traceability.</p>"},{"location":"adaptive-card-consultant-review-part2/#3-deep-dive-process_adaptive_card_payload","title":"3. Deep Dive: <code>process_adaptive_card_payload</code>","text":"<p>This function is responsible for: - Iterating through the submitted consultant actions. - Extracting manager and consultant details from the payload. - Taking the appropriate action (e.g., sending confirmation emails, saving to disk, triggering downstream automation).</p> <pre><code>def process_adaptive_card_payload(payload, client_ip):\n    for consultant_id, values in payload.items():\n        manager_email = values.get(\"manageremail\")\n        manager_name = values.get(\"managername\")\n        # ... process each consultant's action ...\n    send_email_to_manager(payload, manager_email, manager_name)\n    save_payload_to_disk(payload, manager_email)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#31-send_email_to_managerpayload-manager_email-manager_name","title":"3.1. <code>send_email_to_manager(payload, manager_email, manager_name)</code>","text":"<p>Builds an HTML summary of the manager's actions for all consultants and sends a confirmation email to the manager with a table of decisions (keep/deactivate).</p> <pre><code>def send_email_to_manager(payload, manager_email, manager_name):\n    \"\"\"Sends an HTML formatted email to the manager.\"\"\"\n    try:\n        subject = \"Consultant Review Actions Summary\"\n        body = f\"\"\"\n        &lt;html&gt;\n        &lt;body style=\\\"font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;p&gt;Dear {manager_name},&lt;/p&gt;\n            &lt;p&gt;Thank you for reviewing the consultants. Below is a summary of your actions:&lt;/p&gt;\n            &lt;table border=\\\"1\\\" style=\\\"border-collapse: collapse; width: 100%; font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Consultant Email&lt;/th&gt;\n                &lt;th&gt;Decision&lt;/th&gt;\n            &lt;/tr&gt;\n        \"\"\"\n        for consultant_id, values in payload.items():\n            body += f\"&lt;tr&gt;&lt;td&gt;{values.get('email')}&lt;/td&gt;&lt;td&gt;{values.get('decision')}&lt;/td&gt;&lt;/tr&gt;\"\n        body += \"\"\"\n            &lt;/table&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n        send_email(recipients=[manager_email], subject=subject, html_message=body)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#32-save_payload_to_diskpayload-manager_email","title":"3.2. <code>save_payload_to_disk(payload, manager_email)</code>","text":"<p>Serializes the entire payload to a JSON file and saves it to a mounted share or persistent storage for auditing and further processing.</p> <pre><code>def save_payload_to_disk(payload, manager_email):\n    \"\"\"Saves the entire payload to the mounted share as a single JSON file.\"\"\"\n    try:\n        import os, json, datetime\n        filename = f\"{manager_email}_consultant_review_{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}.json\"\n        path = os.path.join(UNPROCESSED_PATH, filename)\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(payload, f, ensure_ascii=False, indent=2)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#4-downstream-automation-process_deactived_consultants","title":"4. Downstream Automation: <code>process_deactived_consultants</code>","text":"<p>This function is typically run on a schedule to process all submitted consultant reviews: - Loads all unprocessed review files from disk. - For each consultant marked for deactivation, adds them to a deactivation list. - Sends a summary email to HR and IT for further action. - Moves processed files to an archive location.</p> <pre><code>def process_deactived_consultants():\n    deactivate_list = []\n    manager_consultants_files = fetch_and_ignore_unprocessed_review_files()\n    for file in manager_consultants_files:\n        file_path, file_name = file.rsplit('/', 1)\n        file_time_utc = os.path.getmtime(file)\n        file_time = datetime.fromtimestamp(file_time_utc, pytz.utc).astimezone(pytz.timezone('America/Chicago'))\n        with open(file, 'r') as f:\n            file_content = f.read()\n        consultants_data = json.loads(file_content)\n        for consultant, details in consultants_data.items():\n            if details.get('decision') == 'deactivate':\n                deactivate_list.append({\n                    'manager_email': details.get('manageremail'),\n                    'consultant_email': details.get('email'),\n                    'approval_time': file_time.strftime('%Y-%m-%d %H:%M:%S')\n                })\n    if deactivate_list:\n        send_email_to_hr_and_it(deactivate_list)\n    for file in manager_consultants_files:\n        file_name = os.path.basename(file)\n        processed_file_path = os.path.join(PROCESSED_PATH, file_name)\n        os.rename(file, processed_file_path)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#5-error-handling-handle_global_exception","title":"5. Error Handling: <code>handle_global_exception</code>","text":"<p>All major functions use <code>handle_global_exception</code> to log and report errors, ensuring that issues are traceable and do not silently fail.</p> <pre><code>def handle_global_exception(function_name, exception_obj):\n    logger.error(f\"Exception in {function_name}: {exception_obj}\")\n    # Optionally, send an alert email or take other action\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#6-recap-full-request-processing-chain","title":"6. Recap: Full Request Processing Chain","text":"<ol> <li>Adaptive Card submission posts to <code>/consultant-review-confirmation</code>.</li> <li><code>process_request_headers_and_payload</code> authenticates and extracts the payload.<ul> <li>Calls <code>log_jwt_payload</code>, <code>fetch_public_key</code>, <code>validate_token</code>.</li> </ul> </li> <li><code>process_adaptive_card_payload</code> processes the payload.<ul> <li>Calls <code>send_email_to_manager</code>, <code>save_payload_to_disk</code>.</li> </ul> </li> <li><code>process_deactived_consultants</code> (scheduled) processes all reviews and notifies HR/IT.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#7-example-end-to-end-flow","title":"7. Example: End-to-End Flow","text":"<ol> <li>Manager receives Adaptive Card, reviews consultants, and submits actions.</li> <li>Submission is POSTed to <code>/consultant-review-confirmation</code> with a JWT Bearer token.</li> <li>The backend verifies the token, extracts the payload, and logs all actions.</li> <li>The manager receives a confirmation email summarizing their decisions.</li> <li>The payload is saved for auditing and further automation (e.g., account deactivation).</li> <li>HR/IT are notified of deactivation approvals as needed.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#conclusion","title":"Conclusion","text":"<p>By recursively examining each function and providing the full code, you can see how the system securely and reliably processes Adaptive Card submissions. This approach is company-agnostic and can be adapted to any workflow requiring secure, actionable messaging in Outlook.</p> <ul> <li>Always validate and log incoming requests.</li> <li>Process and audit all actions.</li> <li>Automate downstream actions as needed.</li> </ul> <p>This completes the deep dive into the end-to-end workflow for actionable consultant review using Adaptive Cards and Python.</p>"},{"location":"adds-user-creation/","title":"Automating Active Directory User Creation and Group Assignment","text":""},{"location":"adds-user-creation/#introduction","title":"Introduction","text":"<p>Automating user provisioning in Active Directory Domain Services (ADDS) is a common requirement for IT teams managing large organizations. Python, with its rich ecosystem of libraries, makes it possible to programmatically create users and assign them to groups in ADDS. This article provides a detailed walkthrough of a working Python implementation for creating new ADDS users and adding them to groups, using the <code>ldap3</code> library and related tools.</p>"},{"location":"adds-user-creation/#overview-of-the-workflow","title":"Overview of the Workflow","text":"<p>The core function, <code>create_new_users_adds</code>, orchestrates the process of:</p> <ol> <li>Establishing a secure connection to the ADDS server.</li> <li>Creating a new user account with the required attributes.</li> <li>Setting the user's password and enabling the account.</li> <li>Adding the user to one or more ADDS groups.</li> </ol> <p>This workflow is modular, with each step handled by a dedicated function or library call, making it easy to adapt for different environments.</p>"},{"location":"adds-user-creation/#step-1-establishing-a-connection-to-adds","title":"Step 1: Establishing a Connection to ADDS","text":"<p>The function <code>get_adds_Connection</code> uses the <code>ldap3</code> library to connect to the ADDS server over SSL. Credentials are securely retrieved (in this codebase, from Azure Key Vault, but you can use environment variables or other secure stores):</p> <pre><code>server = ldap3.Server(dc_ip, use_ssl=True)\nconn = ldap3.Connection(server, user=LDAP_USER_ID, password=LDAP_USER_PASSWORD)\nif not conn.bind():\n    print('Error in bind', conn.result)\n</code></pre> <p>This returns a connection object used for all subsequent LDAP operations.</p>"},{"location":"adds-user-creation/#step-2-creating-a-new-user-in-adds","title":"Step 2: Creating a New User in ADDS","text":"<p>The function <code>create_adds_user</code> (called within <code>create_new_users_adds</code>) performs the following:</p> <ul> <li> <p>Adds the user object: <pre><code>conn.add(\n    distinguished_name,\n    ['top', 'person', 'organizationalPerson', 'user'],\n    {\n        'givenName': first_name,\n        'sn': last_name,\n        'sAMAccountName': sam_account_name,\n        'userPrincipalName': upn_name,\n        'mail': upn_name\n    }\n)\n</code></pre>   The <code>distinguished_name</code> (DN) specifies the user's location in the directory tree (OU). For generalization, replace any organization-specific OUs with your own structure.</p> </li> <li> <p>Enables the account and sets the password: <pre><code>conn.modify(\n    distinguished_name,\n    {\n        'userAccountControl': [(ldap3.MODIFY_REPLACE, [512])],  # Enable the account\n        'unicodePwd': [(ldap3.MODIFY_REPLACE, [f'\"{default_password}\"'.encode('utf-16-le')])]\n    }\n)\n</code></pre>   The password must be encoded in UTF-16-LE and quoted. The <code>userAccountControl</code> value of 512 enables the account.</p> </li> </ul>"},{"location":"adds-user-creation/#step-3-adding-the-user-to-adds-groups","title":"Step 3: Adding the User to ADDS Groups","text":"<p>After the user is created, the code assigns them to one or more groups using the <code>add_members_to_group</code> function from <code>ldap3.extend.microsoft.addMembersToGroups</code>:</p> <p><pre><code>add_members_to_group(conn, [distinguished_name], group_dns, fix=True)\n</code></pre> - <code>conn</code>: The active LDAP connection. - <code>[distinguished_name]</code>: A list of user DNs to add. - <code>group_dns</code>: A list of group DNs (distinguished names) to which the user should be added. - <code>fix=True</code>: Ensures the function will attempt to fix any inconsistencies in group membership.</p> <p>This function performs the necessary LDAP modifications to add the user as a member of each specified group. It is robust and handles group membership updates according to Microsoft's AD schema.</p>"},{"location":"adds-user-creation/#error-handling-and-best-practices","title":"Error Handling and Best Practices","text":"<ul> <li>Error Handling: Each step is wrapped in try/except blocks, and errors are logged or emailed to administrators. This is critical for production automation.</li> <li>Security: Credentials are not hardcoded. Use secure storage for service accounts and passwords.</li> <li>Generalization: Replace any organization-specific OUs or group names with your own. The logic is portable to any ADDS environment.</li> </ul>"},{"location":"adds-user-creation/#example-creating-and-assigning-a-user","title":"Example: Creating and Assigning a User","text":"<p>Here is a simplified, generalized version of the workflow:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\nfrom ldap3.extend.microsoft.addMembersToGroups import ad_add_members_to_groups as add_members_to_group\n\n# Connect to ADDS\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\n# Create user\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nconn.add(dn, ['top', 'person', 'organizationalPerson', 'user'], {\n    'givenName': 'John',\n    'sn': 'Doe',\n    'sAMAccountName': 'jdoe',\n    'userPrincipalName': 'jdoe@example.com',\n    'mail': 'jdoe@example.com'\n})\n\n# Enable account and set password\nconn.modify(dn, {\n    'userAccountControl': [(MODIFY_REPLACE, [512])],\n    'unicodePwd': [(MODIFY_REPLACE, ['\"YourPassword123!\"'.encode('utf-16-le')])]\n})\n\n# Add to groups\ngroup_dns = ['CN=YourGroup,OU=Groups,DC=example,DC=com']\nadd_members_to_group(conn, [dn], group_dns, fix=True)\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-creation/#conclusion","title":"Conclusion","text":"<p>With Python and the <code>ldap3</code> library, you can fully automate the process of creating users and managing group memberships in Active Directory. This approach is scalable, secure, and adaptable to any ADDS environment. By modularizing each step and handling errors robustly, you can integrate this workflow into larger HR or IT automation pipelines.</p>"},{"location":"adds-user-creation/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"adds-user-update/","title":"Updating Active Directory User Attributes","text":""},{"location":"adds-user-update/#introduction","title":"Introduction","text":"<p>Active Directory Domain Services (ADDS) is the backbone of identity management in many organizations. While user creation and group assignment are common automation tasks, updating user attributes\u2014both standard (delivered) and custom\u2014is equally important for keeping directory data accurate and useful. This article explains, with practical Python code, how to update ADDS user attributes using the <code>ldap3</code> library, focusing on the function <code>update_existing_users_adds</code>.</p>"},{"location":"adds-user-update/#understanding-adds-attributes-delivered-vs-custom","title":"Understanding ADDS Attributes: Delivered vs. Custom","text":"<ul> <li>Delivered (Standard) Attributes:</li> <li>These are built-in attributes provided by Microsoft, such as <code>givenName</code>, <code>sn</code>, <code>title</code>, <code>department</code>, <code>telephoneNumber</code>, etc.</li> <li>They are part of the default AD schema and are widely supported by tools and scripts.</li> <li>Custom Attributes:</li> <li>Organizations can extend the AD schema to include custom attributes (e.g., <code>extensionAttribute1</code>, <code>departmentNumber</code>).</li> <li>These are used for business-specific data not covered by standard attributes.</li> </ul> <p>Both types can be updated using the same LDAP operations.</p>"},{"location":"adds-user-update/#the-python-approach-using-ldap3","title":"The Python Approach: Using ldap3","text":"<p>The <code>ldap3</code> library provides a high-level, Pythonic interface for interacting with ADDS. The function <code>update_existing_users_adds</code> demonstrates how to:</p> <ol> <li>Build a dictionary of user attributes to update (both standard and custom).</li> <li>Connect to ADDS securely.</li> <li>Use the <code>modify</code> method to update attributes for each user.</li> <li>Handle errors and notify administrators if updates fail.</li> </ol>"},{"location":"adds-user-update/#step-by-step-updating-user-attributes","title":"Step-by-Step: Updating User Attributes","text":""},{"location":"adds-user-update/#1-prepare-the-attribute-dictionary","title":"1. Prepare the Attribute Dictionary","text":"<p>For each user, a dictionary is built with the attributes to update. This can include both delivered and custom attributes:</p> <pre><code>item = {\n    'displayName': display_name,           # Standard\n    'givenName': first_name,               # Standard\n    'sn': last_name,                       # Standard\n    'title': title,                        # Standard\n    'department': department,              # Standard\n    'employeeType': employee_type,         # Standard\n    'extensionAttribute1': is_mgmt_position, # Custom\n    'manager': manager_dn,                 # Standard (DN of manager)\n    # ... add more as needed ...\n}\n</code></pre>"},{"location":"adds-user-update/#2-connect-to-adds","title":"2. Connect to ADDS","text":"<pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n</code></pre>"},{"location":"adds-user-update/#3-update-attributes-with-modify","title":"3. Update Attributes with <code>modify</code>","text":"<p>The <code>modify</code> method is used to update one or more attributes for a user. The changes dictionary maps attribute names to a tuple specifying the operation (e.g., <code>MODIFY_REPLACE</code>) and the new value(s):</p> <p><pre><code>changes = {key: (MODIFY_REPLACE, [value]) for key, value in item.items() if value}\nconn.modify(dn=distinguished_name, changes=changes)\n</code></pre> - <code>dn</code>: The distinguished name of the user to update. - <code>changes</code>: A dictionary of attribute updates.</p>"},{"location":"adds-user-update/#4-error-handling-and-notification","title":"4. Error Handling and Notification","text":"<p>After each modify operation, the result is checked. If the update fails, an email notification is sent to administrators:</p> <pre><code>if conn.result['result'] != 0:\n    send_email(\n        recipients=['admin@example.com'],\n        subject=f'Error while updating user {distinguished_name}',\n        plain_message=f\"An error occurred while modifying user: {conn.result}\",\n    )\n</code></pre>"},{"location":"adds-user-update/#example-updating-a-users-attributes","title":"Example: Updating a User's Attributes","text":"<p>Here is a simplified, generalized example:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nchanges = {\n    'title': (MODIFY_REPLACE, ['Senior Engineer']),\n    'department': (MODIFY_REPLACE, ['Engineering']),\n    'extensionAttribute1': (MODIFY_REPLACE, ['Project Lead'])\n}\nconn.modify(dn=dn, changes=changes)\n\nif conn.result['result'] != 0:\n    print(f\"Error updating user: {conn.result}\")\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-update/#best-practices","title":"Best Practices","text":"<ul> <li>Batch Updates: You can update multiple attributes in a single <code>modify</code> call for efficiency.</li> <li>Custom Attributes: Ensure custom attributes exist in your AD schema before attempting to update them.</li> <li>Error Handling: Always check the result of LDAP operations and log or notify on failure.</li> <li>Security: Never hardcode credentials; use secure storage.</li> </ul>"},{"location":"adds-user-update/#conclusion","title":"Conclusion","text":"<p>Updating user attributes in ADDS with Python and <code>ldap3</code> is straightforward and powerful. Whether you are updating standard or custom attributes, the process is the same. By following the approach in <code>update_existing_users_adds</code>, you can automate directory maintenance and ensure your AD data stays current and accurate.</p>"},{"location":"adds-user-update/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"azure-ad-certificate/","title":"Certificate Based Authentication","text":""},{"location":"azure-ad-certificate/#certificate-based-authentication-for-azure-ad-why-and-how","title":"Certificate-Based Authentication for Azure AD: Why and How","text":""},{"location":"azure-ad-certificate/#creating-a-certificate-for-azure-ad-authentication","title":"Creating a Certificate for Azure AD Authentication","text":"<p>To use certificate-based authentication with Azure Active Directory (Azure AD), you first need to generate a certificate. Certificates provide a secure, manageable, and standards-based way to authenticate applications. A <code>.pfx</code> certificate may be required because Azure AD expects a certificate in Personal Information Exchange (PFX) format when uploading via the portal or for certain SDKs. The <code>.pfx</code> file contains both the public and private keys, protected by a password, and is suitable for import/export scenarios.</p>"},{"location":"azure-ad-certificate/#steps-to-generate-a-certificate-using-openssl","title":"Steps to Generate a Certificate Using OpenSSL","text":"<ol> <li>Generate a Private Key: <pre><code>openssl genrsa -out my-app-auth.key 2048\n</code></pre></li> <li>Create a Certificate Signing Request (CSR): <pre><code>openssl req -new -key my-app-auth.key -out my-app-auth.csr\n</code></pre></li> <li>Generate a Self-Signed Certificate: <pre><code>openssl x509 -req -days 730 -in my-app-auth.csr -signkey my-app-auth.key -out my-app-auth.crt\n</code></pre></li> <li>Export to PFX (if needed for Azure): <pre><code>openssl pkcs12 -export -out my-app-auth.pfx -inkey my-app-auth.key -in my-app-auth.crt\n</code></pre> <p>Note: The <code>.pfx</code> format is required if you want to upload the certificate via the Azure Portal or use it with some SDKs/tools. The <code>.crt</code> file is the public certificate, and the <code>.key</code> file is your private key (keep it secure!).</p> </li> </ol>"},{"location":"azure-ad-certificate/#uploading-the-certificate-to-your-entra-azure-ad-application","title":"Uploading the Certificate to Your Entra (Azure AD) Application","text":"<ol> <li>Go to the Microsoft Entra admin center and select Azure Active Directory.</li> <li>Navigate to App registrations and select your application.</li> <li>In the left menu, click Certificates &amp; secrets.</li> <li>Under Certificates, click Upload certificate.</li> <li>Select your <code>.crt</code> or <code>.pfx</code> file and upload it.</li> <li>After uploading, Azure will display the certificate thumbprint. Save this value for use in your application code.</li> </ol>"},{"location":"azure-ad-certificate/#assigning-permissions-to-the-application","title":"Assigning Permissions to the Application","text":"<p>After uploading the certificate, you must assign the necessary API permissions to your application:</p> <ol> <li>In your application's App registration page, go to API permissions.</li> <li>Click Add a permission and select the required Microsoft APIs (e.g., Microsoft Graph, Azure Service Management, etc.).</li> <li>Choose the appropriate permission type (Application or Delegated) and select the required permissions.</li> <li>Click Add permissions.</li> <li>If required, click Grant admin consent to approve the permissions for your organization.</li> </ol> <p>Note: The application will only be able to access resources for which it has been granted permissions. Make sure to review and assign only the permissions your app needs.</p>"},{"location":"azure-ad-certificate/#why-use-a-certificate-instead-of-an-application-secret","title":"Why Use a Certificate Instead of an Application Secret?","text":""},{"location":"azure-ad-certificate/#1-security","title":"1. Security","text":"<ul> <li>Application secrets are essentially passwords. They are susceptible to accidental exposure (e.g., in code repositories, logs, or configuration files).</li> <li>Certificates use asymmetric cryptography. The private key never leaves your environment, and only the public key is uploaded to Azure AD. This makes certificates much harder to compromise.</li> </ul>"},{"location":"azure-ad-certificate/#2-lifecycle-management","title":"2. Lifecycle Management","text":"<ul> <li>Secrets typically expire every 6-12 months, requiring regular rotation and updates in all dependent systems.</li> <li>Certificates can have longer lifespans (e.g., 1-2 years), and their expiration is easier to track and automate.</li> </ul>"},{"location":"azure-ad-certificate/#3-compliance-and-best-practices","title":"3. Compliance and Best Practices","text":"<ul> <li>Microsoft and most security frameworks recommend certificates for service-to-service authentication.</li> <li>Certificates support better auditing and can be managed centrally (e.g., via Azure Key Vault).</li> </ul>"},{"location":"azure-ad-certificate/#why-use-the-msal-library-and-not-a-specific-azure-sdk","title":"Why Use the MSAL Library (and Not a Specific Azure SDK)?","text":"<p>The MSAL (Microsoft Authentication Library) for Python is a lightweight, flexible library for acquiring tokens from Azure AD. It supports a wide range of authentication scenarios, including certificate-based authentication for confidential clients.</p> <ul> <li>Why MSAL?</li> <li>MSAL is the official library for handling authentication and token acquisition with Azure AD.</li> <li>It is not tied to a specific Azure service, making it ideal for generic authentication scenarios.</li> <li> <p>It supports advanced scenarios like certificate-based authentication, multi-tenant apps, and more.</p> </li> <li> <p>Why Not Use a Specific Azure SDK?</p> </li> <li>Some Azure SDKs (e.g., for Storage, Key Vault, etc.) provide their own authentication mechanisms, but they may not support all advanced scenarios or may require additional dependencies.</li> <li>Using MSAL directly gives you full control over the authentication flow and token management, and is more transparent for troubleshooting and customization.</li> </ul>"},{"location":"azure-ad-certificate/#code-example-certificate-based-authentication-in-python","title":"Code Example: Certificate-Based Authentication in Python","text":"<p>Below is the function used in this project to acquire an Azure AD access token using a certificate:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token_from_azure(client_id, authority, tenant_id, resource_scopes):\n    \"\"\"\n    Retrieves an access token from Azure Active Directory using a confidential client application.\n    This function uses certificate-based authentication to acquire an access token for the specified resource.\n    \"\"\"\n    try:\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n\n        app = ConfidentialClientApplication(\n            client_id=client_id,\n            authority=f\"{authority}{tenant_id}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n\n        result = app.acquire_token_for_client(scopes=resource_scopes)\n        if \"access_token\" in result:\n            return result[\"access_token\"]\n\n    except Exception as exception:\n        handle_global_exception(sys._getframe().f_code.co_name, exception)\n    finally:\n        pass\n</code></pre>"},{"location":"azure-ad-certificate/#key-points","title":"Key Points:","text":"<ul> <li>The private key is read from a secure file (<code>certs/*.key</code>).</li> <li>The certificate thumbprint and private key are passed to MSAL's <code>ConfidentialClientApplication</code>.</li> <li>No secrets or passwords are stored in code or configuration.</li> </ul>"},{"location":"azure-ad-certificate/#conclusion","title":"Conclusion","text":"<p>Certificate-based authentication is the recommended and most secure way to authenticate service applications with Azure AD. It reduces risk, simplifies management, and aligns with industry best practices. Migrating from secrets to certificates is straightforward and well-supported by both Azure and the MSAL Python library.</p>"},{"location":"azure-ad-certificate/#references","title":"References","text":"<ul> <li>MSAL Python Certificate Auth Sample</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> <li>OpenSSL Documentation</li> </ul>"},{"location":"azure-billing/","title":"Download Azure Bill","text":""},{"location":"azure-billing/#programmatically-downloading-and-storing-azure-billing-data","title":"Programmatically Downloading and Storing Azure Billing Data:","text":""},{"location":"azure-billing/#introduction","title":"Introduction","text":"<p>Automating the retrieval and storage of Azure billing data is essential for organizations seeking cost transparency and operational efficiency. This guide details a robust, production-grade approach to programmatically obtaining Azure billing data using Python, authenticating securely with certificates, and efficiently storing the results in a SQL Server database. </p>"},{"location":"azure-billing/#1-secure-authentication-acquiring-an-azure-access-token-with-certificates","title":"1. Secure Authentication: Acquiring an Azure Access Token with Certificates","text":"<p>The first step is to authenticate with Azure Active Directory (Azure AD) using certificate-based authentication. This is more secure than using client secrets and is recommended for automation and service-to-service scenarios. For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p> <p>Python Example:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token(client_id, authority, tenant_id, resource_scopes, cert_thumbprint, cert_key_path):\n    \"\"\"\n    Acquire an Azure AD access token using certificate-based authentication.\n    \"\"\"\n    with open(cert_key_path, \"r\") as key_file:\n        private_key = key_file.read()\n    app = ConfidentialClientApplication(\n        client_id=client_id,\n        authority=f\"{authority}{tenant_id}\",\n        client_credential={\n            \"thumbprint\": cert_thumbprint,\n            \"private_key\": private_key,\n        },\n    )\n    result = app.acquire_token_for_client(scopes=resource_scopes)\n    if \"access_token\" not in result:\n        raise Exception(f\"Token acquisition failed: {result}\")\n    return result[\"access_token\"]\n</code></pre> <ul> <li>Why certificates? They are more secure, support longer lifecycles, and are recommended for automation.</li> <li>MSAL Library: The Microsoft Authentication Library (MSAL) is used for token acquisition, providing flexibility and support for advanced scenarios.</li> </ul>"},{"location":"azure-billing/#2-generating-the-azure-cost-report-via-rest-api","title":"2. Generating the Azure Cost Report via REST API","text":"<p>Once authenticated, you can use the Azure Cost Management API to request a cost details report for your subscription. This involves making a POST request to the appropriate endpoint and polling until the report is ready.</p> <p>Python Example:</p> <pre><code>import requests\nimport time\nimport json\n\ndef generate_azure_cost_report(subscription_id, access_token, start_date, end_date, api_version=\"2022-05-01\"):\n    url = f\"https://management.azure.com/subscriptions/{subscription_id}/providers/Microsoft.CostManagement/generateCostDetailsReport?api-version={api_version}\"\n    payload = json.dumps({\"metric\": \"ActualCost\", \"timePeriod\": {\"start\": start_date, \"end\": end_date}})\n    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}\n    response = requests.post(url, headers=headers, data=payload)\n    # Poll until the report is ready\n    while response.status_code == 202:\n        location_url = response.headers.get('Location')\n        retry_after = int(response.headers.get('Retry-After', 30))\n        time.sleep(retry_after)\n        response = requests.get(url=location_url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to generate cost report: {response.status_code} - {response.text}\")\n    return response.json()\n</code></pre> <ul> <li>Polling: The API may return a 202 status, indicating the report is being generated. Poll the <code>Location</code> header until a 200 response is received.</li> <li>Error Handling: Always check for non-200 responses and handle errors appropriately.</li> </ul>"},{"location":"azure-billing/#3-downloading-the-cost-report-data","title":"3. Downloading the Cost Report Data","text":"<p>The response from the cost report API includes a manifest with one or more blob URLs. Download these blobs to obtain the actual cost data, typically in CSV format.</p> <p>Python Example:</p> <pre><code>import urllib3\n\ndef download_cost_report_blobs(manifest, output_path):\n    http = urllib3.PoolManager()\n    for blob in manifest['blobs']:\n        blob_url = blob['blobLink']\n        with open(output_path, 'wb') as out_file:\n            blob_response = http.request('GET', blob_url, preload_content=False)\n            out_file.write(blob_response.data)\n</code></pre> <ul> <li>Blob Download: Use a robust HTTP client (e.g., <code>urllib3</code>) to download the report data.</li> <li>Output: Save the CSV file to a secure, accessible location for further processing.</li> </ul>"},{"location":"azure-billing/#4-loading-the-cost-data-into-sql-server-efficiently","title":"4. Loading the Cost Data into SQL Server Efficiently","text":"<p>After downloading the cost report, the next step is to load the data into a SQL Server table. For large datasets, use a fast, batch insert method to optimize performance.</p> <p>Python Example:</p> <pre><code>import pyodbc\nimport csv\n\ndef load_csv_to_sql_server(csv_path, connection_string, table_name):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    with open(csv_path, 'r', encoding='utf-8-sig') as csvfile:\n        reader = csv.reader(csvfile)\n        columns = next(reader)  # Header row\n        insert_query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['?' for _ in columns])})\"\n        data = list(reader)\n        cursor.fast_executemany = True\n        cursor.executemany(insert_query, data)\n        conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Fast Insert: The <code>fast_executemany</code> flag in <code>pyodbc</code> enables high-performance bulk inserts.</li> <li>Schema Alignment: Ensure the CSV columns match the SQL table schema.</li> </ul>"},{"location":"azure-billing/#5-orchestrating-the-end-to-end-process","title":"5. Orchestrating the End-to-End Process","text":"<p>A typical workflow to automate Azure billing data retrieval and storage:</p> <pre><code>def fetch_and_update_azure_billing_data():\n    # Step 1: Get access token\n    access_token = get_access_token(\n        client_id=..., authority=..., tenant_id=..., resource_scopes=..., cert_thumbprint=..., cert_key_path=...\n    )\n    # Step 2: Generate cost report\n    report = generate_azure_cost_report(\n        subscription_id=..., access_token=access_token, start_date=..., end_date=...\n    )\n    # Step 3: Download report blob(s)\n    download_cost_report_blobs(report['manifest'], output_path=\"azure_billing.csv\")\n    # Step 4: Load into SQL Server\n    load_csv_to_sql_server(\n        csv_path=\"azure_billing.csv\", connection_string=..., table_name=\"AzureBilling\"\n    )\n</code></pre>"},{"location":"azure-billing/#6-additional-considerations","title":"6. Additional Considerations","text":"<ul> <li>Permissions: The Azure AD application must have the required API permissions (e.g., Cost Management Reader) and access to the subscription.</li> <li>Certificate Security: Store private keys securely and never commit them to source control.</li> <li>Error Handling: Implement robust error handling and logging for production use.</li> <li>Scheduling: Use a scheduler (e.g., cron, Azure Automation) to run the process regularly.</li> </ul>"},{"location":"azure-billing/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can securely and efficiently automate the retrieval and storage of Azure billing data using Python. This enables advanced reporting, cost analysis, and integration with enterprise data platforms.</p>"},{"location":"azure-billing/#references","title":"References","text":"<ul> <li>Azure Cost Management REST API</li> <li>MSAL Python Library</li> <li>pyodbc Documentation</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> </ul>"},{"location":"azure-resources/","title":"Download Azure Resource","text":""},{"location":"azure-resources/#programmatically-downloading-azure-resource-inventory-and-tag-management","title":"Programmatically Downloading Azure Resource Inventory and Tag Management","text":""},{"location":"azure-resources/#introduction","title":"Introduction","text":"<p>Maintaining an up-to-date inventory of Azure resources and their associated tags is critical for governance, cost management, and compliance. This article provides a detailed, production-grade approach to programmatically fetching Azure resource metadata and synchronizing it with a SQL Server database using Python. </p>"},{"location":"azure-resources/#1-authentication-secure-access-to-azure-apis","title":"1. Authentication: Secure Access to Azure APIs","text":"<p>Before accessing Azure resources, authenticate using a secure method. The function below demonstrates using the Azure Identity SDK's <code>ClientSecretCredential</code> for authentication. This is a common approach for automation scenarios, but for higher security, certificate-based authentication is recommended (see other article Certificate Based Authorization for Azure AD.)</p>"},{"location":"azure-resources/#deep-dive-get_azure_credential-function","title":"Deep Dive: <code>get_azure_credential</code> Function","text":"<p>The <code>get_azure_credential</code> function leverages the <code>azure-identity</code> Python SDK, which provides a unified way to authenticate to Azure services. Here, we use the <code>ClientSecretCredential</code> class, which is suitable for service principals (app registrations) with a client secret.</p> <p>Python Example:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_azure_credential(tenant_id, client_id, client_secret):\n    \"\"\"\n    Returns a credential object for authenticating with Azure SDKs.\n    Uses the azure-identity library's ClientSecretCredential.\n    \"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n</code></pre> <ul> <li>azure-identity SDK: This is the official Microsoft library for Azure authentication in Python. It supports multiple credential types, including secrets, certificates, managed identity, and interactive login.</li> <li>ClientSecretCredential: This class is used for service-to-service authentication using a client ID and secret. It is widely supported by Azure SDKs, including resource management, storage, and more.</li> <li>When to use: Use this for automation where a client secret is securely stored (e.g., in Azure Key Vault or environment variables). For higher security, use <code>CertificateCredential</code> instead.</li> </ul>"},{"location":"azure-resources/#2-fetching-azure-resource-inventory","title":"2. Fetching Azure Resource Inventory","text":"<p>Use the Azure SDK to enumerate all resources in a subscription. Extract key metadata such as resource ID, name, location, type, and tags.</p> <p>Python Example:</p> <pre><code>def fetch_azure_resources(credential, subscription_id):\n    client = ResourceManagementClient(credential, subscription_id)\n    resource_list = []\n    for item in client.resources.list():\n        type_parts = str(item.type).split('/')\n        type1, type2, type3, type4, type5 = (type_parts + [''] * 5)[:5]\n        resource_group_list = str(item.id).split('/')\n        resource_data = {\n            \"id\": str(item.id).replace(f'/subscriptions/{subscription_id}/', ''),\n            \"location\": item.location,\n            \"name\": item.name,\n            \"tags\": item.tags,\n            \"resourceGroup\": resource_group_list[4] if len(resource_group_list) &gt;= 4 else '',\n            \"type1\": type1,\n            \"type2\": type2,\n            \"type3\": type3,\n            \"type4\": type4,\n            \"type5\": type5,\n        }\n        resource_list.append(resource_data)\n    return resource_list\n</code></pre> <ul> <li>Resource Types: The code splits the resource type string to extract up to five type levels for flexible reporting.</li> <li>Tags: Tags are included for governance and cost allocation.</li> </ul>"},{"location":"azure-resources/#3-synchronizing-with-sql-server-fast-bulk-operations","title":"3. Synchronizing with SQL Server: Fast Bulk Operations","text":"<p>Efficiently update the SQL Server inventory table by marking all resources as inactive, then bulk updating existing resources and inserting new ones. This ensures the database reflects the current Azure state.</p> <p>Note: The <code>Dim_Resources</code> table is not a full load (truncate-and-reload) table. Instead, it is designed to retain records of resources that may have been deleted from Azure. By marking resources as inactive rather than removing them, you can track the lifecycle of resources, including those that have been deleted, for audit, compliance, and historical analysis purposes.</p> <p>Python Example:</p> <pre><code>import pyodbc\n\ndef sync_resources_to_sql(resource_list, connection_string):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    existing_resource_ids = {str(row[0]).lower() for row in cursor.execute(\"SELECT ResourceID FROM Dim_Resources\").fetchall()}\n    updateresources = [\n        [r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True, r['id']]\n        for r in resource_list if str(r['id']).lower() in existing_resource_ids\n    ]\n    newresources = [\n        [r['id'], r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True]\n        for r in resource_list if str(r['id']).lower() not in existing_resource_ids\n    ]\n    cursor.execute('UPDATE Dim_Resources SET Active = 0')\n    if updateresources:\n        query = '''UPDATE Dim_Resources SET Location=?, Name=?, ResourceGroup=?, Type1=?, Type2=?, Type3=?, Type4=?, Type5=?, Active=? WHERE ResourceId=?'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, updateresources)\n    if newresources:\n        query = '''INSERT INTO Dim_Resources (ResourceID, Location, Name, ResourceGroup, Type1, Type2, Type3, Type4, Type5, Active) VALUES (?,?,?,?,?,?,?,?,?,?)'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, newresources)\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Bulk Operations: Use <code>fast_executemany</code> for high-performance updates and inserts.</li> <li>Active Flag: Mark all resources as inactive before updating, then set active for current resources.</li> </ul>"},{"location":"azure-resources/#4-end-to-end-orchestration","title":"4. End-to-End Orchestration","text":"<p>A typical workflow for resource inventory management:</p> <pre><code>def fetch_and_store_resources():\n    credential = get_azure_credential(tenant_id=..., client_id=..., client_secret=...)\n    resource_list = fetch_azure_resources(credential, subscription_id=...)\n    sync_resources_to_sql(resource_list, connection_string=...)\n</code></pre>"},{"location":"azure-resources/#5-best-practices-and-considerations","title":"5. Best Practices and Considerations","text":"<ul> <li>Security: Use certificate-based authentication for automation when possible. Store credentials securely.</li> <li>Performance: Use bulk operations for large datasets.</li> <li>Data Quality: Regularly update the inventory to reflect the current Azure state.</li> <li>Scheduling: Automate the process with a scheduler (e.g., cron, Azure Automation).</li> <li>Auditing: Keep logs of changes and exceptions for compliance.</li> </ul>"},{"location":"azure-resources/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can automate the discovery and inventory of Azure resources, ensuring your SQL Server database remains a reliable source of truth for governance and reporting.</p>"},{"location":"azure-resources/#references","title":"References","text":"<ul> <li>Azure Resource Management Python SDK</li> <li>azure-identity Python SDK</li> <li>pyodbc Documentation</li> <li>Azure Tagging Best Practices</li> </ul>"},{"location":"meraki-nagios-device-sync/","title":"Automating Cisco Meraki Device Discovery and Nagios XI Monitoring Integration","text":""},{"location":"meraki-nagios-device-sync/#introduction","title":"Introduction","text":"<p>Keeping your network monitoring system in sync with your actual device inventory is critical for reliable operations. This article provides a deep dive into a robust Python workflow that:</p> <ul> <li>Discovers all current devices from the Cisco Meraki cloud API</li> <li>Uses SNMP OIDs to obtain Meraki hostnames</li> <li>Compares Meraki inventory to Nagios XI monitored hosts</li> <li>Adds missing devices to Nagios XI, including handling special device types</li> <li>Checks firmware status for compliance</li> </ul> <p>All code is provided and explained so you can adapt this solution for your own environment.</p>"},{"location":"meraki-nagios-device-sync/#required-python-libraries","title":"Required Python Libraries","text":"<p>This workflow uses the following Python libraries:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library. Used for all Meraki cloud API calls.</li> <li>requests: For making HTTP requests to the Nagios XI REST API.</li> <li>subprocess: To run SNMP commands (e.g., <code>snmpwalk</code>) from Python.</li> <li>re: For parsing SNMP command output with regular expressions.</li> </ul> <p>Install any missing libraries with pip:</p> <pre><code>pip install meraki requests\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1-authenticating-to-cisco-meraki-and-nagios-xi-apis","title":"1. Authenticating to Cisco Meraki and Nagios XI APIs","text":""},{"location":"meraki-nagios-device-sync/#cisco-meraki-api-authentication","title":"Cisco Meraki API Authentication","text":"<p>To connect to the Meraki Dashboard API, you need an API key. This key can be generated in your Meraki dashboard under Organization &gt; Settings &gt; Dashboard API access.</p> <pre><code>import meraki\n\nMERAKI_API_KEY = 'YOUR_MERAKI_API_KEY'  # Replace with your Meraki API key\nMERAKI_BASE_URL = 'https://api.meraki.com/api/v1/'\n\ndashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    base_url=MERAKI_BASE_URL,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre>"},{"location":"meraki-nagios-device-sync/#nagios-xi-api-authentication","title":"Nagios XI API Authentication","text":"<p>Nagios XI provides a REST API. You need an API key, which can be generated in the Nagios XI web interface under My Account &gt; API Keys.</p> <pre><code>import requests\n\nNAGIOS_XI_API_URL = 'https://your-nagios-server.example.com/nagiosxi/api/v1/'  # Replace with your Nagios XI URL\nNAGIOS_XI_API_KEY = 'YOUR_NAGIOS_API_KEY'  # Replace with your Nagios XI API key\n\ndef call_nagios_api(endpoint, method='GET', data=None):\n    url = f\"{NAGIOS_XI_API_URL}{endpoint}\"\n    headers = {'Authorization': f'Bearer {NAGIOS_XI_API_KEY}'}\n    if method == 'GET':\n        response = requests.get(url, headers=headers)\n    elif method == 'POST':\n        response = requests.post(url, headers=headers, json=data)\n    elif method == 'PUT':\n        response = requests.put(url, headers=headers, json=data)\n    elif method == 'DELETE':\n        response = requests.delete(url, headers=headers)\n    else:\n        raise ValueError('Unsupported HTTP method')\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1a-understanding-and-setting-up-meraki_dashboard_snmp_community_string","title":"1a. Understanding and Setting Up <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>","text":""},{"location":"meraki-nagios-device-sync/#what-is-meraki_dashboard_snmp_community_string","title":"What is <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>?","text":"<p>The <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code> is a shared secret (like a password) used for authenticating SNMP v2c queries to the Meraki cloud SNMP endpoint. It is required to retrieve device information via SNMP, such as hostnames and other device attributes.</p>"},{"location":"meraki-nagios-device-sync/#how-to-set-up-the-snmp-community-string-in-meraki-dashboard","title":"How to Set Up the SNMP Community String in Meraki Dashboard","text":"<ol> <li>Log in to your Meraki Dashboard</li> <li>Navigate to Organization &gt; Settings</li> <li>Scroll to the SNMP section</li> <li>Enable Cloud Monitoring (SNMP v2c)</li> <li>Set your desired SNMP Community String (e.g., <code>mysnmpcommunity</code>)</li> <li>Save your changes</li> <li>Whitelist your public IP address in the SNMP section to allow SNMP queries from your monitoring server</li> </ol> <p>Note: The SNMP community string acts as a password for SNMP v2c. Keep it secure and do not share it publicly.</p>"},{"location":"meraki-nagios-device-sync/#plugging-the-community-string-into-your-code","title":"Plugging the Community String into Your Code","text":"<p>In your Python code, set the value as follows:</p> <pre><code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING = 'mysnmpcommunity'  # Replace with your actual SNMP community string\n</code></pre> <p>This value is then used in SNMP queries, for example:</p> <pre><code>import subprocess\nimport re\n\ndef get_snmp_data(snmp_server, port, oid, community):\n    command = [\n        \"snmpwalk\",\n        \"-v\", \"2c\",\n        \"-c\", community,\n        f\"{snmp_server}:{port}\",\n        oid\n    ]\n    try:\n        result = subprocess.run(command, capture_output=True, text=True, check=True)\n        output = result.stdout\n        snmp_dict = {}\n        pattern = re.compile(r'(\\S+)\\s+=\\s+STRING:\\s+\"([^\"]+)\"')\n        for match in pattern.finditer(output):\n            oid = match.group(1)\n            string_value = match.group(2)\n            snmp_dict[string_value] = oid\n        return snmp_dict\n    except Exception as e:\n        print(f\"SNMP error: {e}\")\n        return None\n\nMERAKI_DASHBOARD_SNMP_HOST_NAME = 'snmp.meraki.com'\nMERAKI_DASHBOARD_SNMP_PORT = '16100'\n\nmerakihostnames = get_snmp_data(\n    MERAKI_DASHBOARD_SNMP_HOST_NAME,\n    MERAKI_DASHBOARD_SNMP_PORT,\n    '1.3.6.1.4.1.29671.1.1.4.1.2',\n    MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING\n)\n</code></pre> <p>If you change the community string in the Meraki dashboard, update it in your code as well.</p>"},{"location":"meraki-nagios-device-sync/#2-obtaining-all-current-devices-from-meraki","title":"2. Obtaining All Current Devices from Meraki","text":"<p>We use the official Meraki Dashboard API to fetch all organizations, devices, and networks:</p> <p><pre><code>gdepOrganizations = dashboard.organizations.getOrganizations()\norganizationid = gdepOrganizations[0]['id']\ngdepdevices = dashboard.organizations.getOrganizationDevices(organizationid, -1)\ngdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid, -1)\n</code></pre> - <code>getOrganizations()</code> returns all organizations your API key can access. - <code>getOrganizationDevices()</code> fetches all devices (appliances, switches, cameras, wireless, etc.). - <code>getOrganizationNetworks()</code> fetches all networks (logical groupings of devices).</p>"},{"location":"meraki-nagios-device-sync/#3-obtaining-meraki-hostnames-via-snmp-oid","title":"3. Obtaining Meraki Hostnames via SNMP OID","text":"<p>To get hostnames as seen by Meraki's SNMP dashboard, we use the SNMP OID <code>1.3.6.1.4.1.29671.1.1.4.1.2</code> (see code above).</p> <ul> <li>This function runs an <code>snmpwalk</code> command and parses the output into a dictionary of hostnames and OIDs.</li> <li>SNMP access must be enabled and your IP whitelisted in the Meraki dashboard.</li> </ul>"},{"location":"meraki-nagios-device-sync/#4-checking-firmware-status-for-each-network","title":"4. Checking Firmware Status for Each Network","text":"<p>For each network, we check the current firmware status of all products using the Meraki Dashboard API's <code>getNetworkFirmwareUpgrades</code> method.</p>"},{"location":"meraki-nagios-device-sync/#what-is-getnetworkfirmwareupgrades","title":"What is <code>getNetworkFirmwareUpgrades</code>?","text":"<p>This method retrieves the current and available firmware versions for all devices in a given Meraki network. It helps you: - Audit firmware compliance - Identify devices that need upgrades - Track which products are running which firmware</p>"},{"location":"meraki-nagios-device-sync/#example-usage","title":"Example Usage","text":"<pre><code># For each network, get firmware upgrade status\nfor network in gdepnetworks:\n    network_id = network['id']\n    networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network_id)\n    print(f\"Firmware info for network {network['name']}:\\n\", networkupgrades)\n    if 'products' in networkupgrades:\n        products = networkupgrades['products']\n        for product_type, firmware_info in products.items():\n            print(f\"Product: {product_type}\")\n            print(f\"Current Version: {firmware_info.get('currentVersion', {}).get('name', 'N/A')}\")\n            print(f\"Available Version: {firmware_info.get('availableVersion', {}).get('name', 'N/A')}\")\n            print(f\"Status: {firmware_info.get('status', 'N/A')}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#sample-output-structure","title":"Sample Output Structure","text":"<p>The returned dictionary typically looks like:</p> <pre><code>{\n  \"products\": {\n    \"appliance\": {\n      \"currentVersion\": {\"name\": \"MX 18.107.2\"},\n      \"availableVersion\": {\"name\": \"MX 18.107.4\"},\n      \"status\": \"Up to date\"\n    },\n    \"switch\": {\n      \"currentVersion\": {\"name\": \"MS 15.21\"},\n      \"availableVersion\": {\"name\": \"MS 15.22\"},\n      \"status\": \"Upgrade available\"\n    }\n  }\n}\n</code></pre> <ul> <li><code>currentVersion</code>: The firmware currently running on the product type.</li> <li><code>availableVersion</code>: The latest available firmware for that product type.</li> <li><code>status</code>: Whether the device is up to date or needs an upgrade.</li> </ul> <p>This information can be used to automate firmware compliance checks and trigger upgrades as needed.</p>"},{"location":"meraki-nagios-device-sync/#5-comparing-meraki-devices-to-nagios-xi-hosts","title":"5. Comparing Meraki Devices to Nagios XI Hosts","text":"<p>We fetch all hosts from Nagios XI and compare them to the Meraki inventory:</p> <p><pre><code>nagioshost = call_nagios_api('objects/host')\nfor device in gdepdevices:\n    nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n    if len(nagioshostitems) == 0:\n        # Device is missing from Nagios XI\n        # ...add to missing list and prepare for addition...\n</code></pre> - Devices not found in Nagios XI are flagged for addition. - Special handling for device types (appliance, switch, camera, wireless, etc.).</p>"},{"location":"meraki-nagios-device-sync/#6-adding-missing-devices-to-nagios-xi","title":"6. Adding Missing Devices to Nagios XI","text":"<p>For each missing device, we call helper functions to create/update hosts and services in Nagios XI:</p> <p><pre><code>if len(str(device['name']).strip()) != 0:\n    if (str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS):\n        if (str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS):\n            if (device['productType'] == 'appliance' and 'VMX' not in device['model']):\n                applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                if len(vlan999) == 0:\n                    device['lanIp'] = '0.0.0.0'\n                else:\n                    device['lanIp'] = vlan999[0]['applianceIp']\n            if device['lanIp'] is None:\n                device['lanIp'] = '0.0.0.0'\n            create_update_meraki_host(device, nagioshostitems, gdepnetworks, False)\n            nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n            create_update_meraki_host_services(device, nagiosserviceitems, False, merakihostnames)\n</code></pre> - <code>create_update_meraki_host()</code> and <code>create_update_meraki_host_services()</code> are responsible for adding/updating hosts and their services in Nagios XI. - VLAN and IP logic ensures correct addressing for appliances.</p>"},{"location":"meraki-nagios-device-sync/#7-applying-configuration","title":"7. Applying Configuration","text":"<p>After all additions/updates, we apply the Nagios XI configuration:</p> <pre><code>data = {'alias': 'Nagios XI', 'applyconfig': '1'}\ncall_nagios_api('config/host/localhost', method='PUT', data=data)\nnagioshost = call_nagios_api('objects/host')\n</code></pre>"},{"location":"meraki-nagios-device-sync/#8-full-function-code-add_missing_network_device_to_nagios","title":"8. Full Function Code: <code>add_missing_network_device_to_nagios</code>","text":"<p>Below is the complete function, ready to adapt for your own environment:</p> <pre><code>def add_missing_network_device_to_nagios():\n    try:\n        # OID to obtain all host names from Meraki SNMP Dashboard\n        merakihostnames = get_snmp_data(\n            MERAKI_DASHBOARD_SNMP_HOST_NAME, \n            MERAKI_DASHBOARD_SNMP_PORT,\n            '1.3.6.1.4.1.29671.1.1.4.1.2',\n            MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING)\n        gdepOrganizations = dashboard.organizations.getOrganizations()\n        organizationid = gdepOrganizations[0]['id']\n        gdepdevices = dashboard.organizations.getOrganizationDevices(organizationid,-1)\n        gdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid,-1)\n\n        for network in gdepnetworks:\n            networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network['id'])\n            if ('products' in networkupgrades):\n                products = networkupgrades['products']\n                # ...process firmware info as needed...\n\n        nagioshost = call_nagios_api('objects/host')\n        nagioshostservices = call_nagios_api('objects/service')\n        nagioshostconfig = call_nagios_api('config/host')\n        nagioshostgroupmembers = call_nagios_api('objects/hostgroupmembers')\n        nagioshostservicesconfig = call_nagios_api('config/service')\n\n        SKIP_MERAKI_HOSTS = ['tst','tes']\n\n        for device in gdepdevices:\n            if (device['productType'] == 'appliance'):\n                # ...handle appliance types...\n                pass\n            elif (device['productType'] == 'camera'):\n                pass\n            elif (device['productType'] == 'switch'):\n                pass\n            elif (device['productType'] == 'wireless'):\n                pass\n            # ...other device handling as needed...\n            nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n            if (len(nagioshostitems) == 0):\n                if (len(str(device['name']).strip()) != 0):\n                    if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                            # ...add to Nagios XI...\n                            pass\n            if (len(str(device['name']).strip()) != 0):\n                if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                    if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((device['productType'] == 'appliance') and ('VMX' not in device['model'])):\n                            applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                            vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                            if (len(vlan999) == 0):\n                                device['lanIp'] = '0.0.0.0'\n                            else:\n                                device['lanIp'] = vlan999[0]['applianceIp']\n                        if (device['lanIp'] is None):\n                            device['lanIp'] = '0.0.0.0'\n                        create_update_meraki_host(device,nagioshostitems,gdepnetworks,False)\n                        nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n                        create_update_meraki_host_services(device,nagiosserviceitems,False,merakihostnames)\n        data = {'alias': 'Nagios XI', 'applyconfig': '1'}\n        call_nagios_api('config/host/localhost', method='PUT', data=data)\n        nagioshost = call_nagios_api('objects/host')\n    except Exception as exception_obj:\n        print(f\"Error: {exception_obj}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#9-conclusion","title":"9. Conclusion","text":"<p>This workflow ensures your Nagios XI monitoring system is always in sync with your actual Meraki device inventory, with full visibility into firmware status and device types. By automating device discovery, comparison, and configuration, you can maintain a reliable, up-to-date monitoring environment with minimal manual effort.</p>"},{"location":"meraki-nagios-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Python meraki library</li> </ul>"},{"location":"sap-concur-expense-reports-aggregation/","title":"Automating SAP Concur Expense Report Aggregation and Adaptive Card Notifications","text":""},{"location":"sap-concur-expense-reports-aggregation/#introduction","title":"Introduction","text":"<p>This article provides a comprehensive, company-agnostic walkthrough for automating SAP Concur expense report aggregation and delivering actionable, interactive notifications to managers using Adaptive Cards. We\u2019ll cover:</p> <ul> <li>Securely connecting to SAP Concur with OAuth2</li> <li>Fetching and processing users and expense reports</li> <li>Aggregating by employee and by full management chain (organization-wide rollup)</li> <li>Creating and sending Adaptive Card emails with summary/detail toggles</li> <li>All supporting functions, with code and explanations</li> </ul> <p>By the end, you\u2019ll be able to connect to your own SAP Concur instance and deliver organization-wide expense insights to managers in a modern, interactive format.</p>"},{"location":"sap-concur-expense-reports-aggregation/#1-connecting-to-sap-concur-api","title":"1. Connecting to SAP Concur API","text":"<p>To fetch expense reports, you need to: - Obtain an OAuth2 access token using your SAP Concur client credentials and refresh token. - Use the access token to call the Concur API endpoints for users and expense reports.</p>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_scope","title":"Supporting Function: <code>get_scope()</code>","text":"<p>SAP Concur APIs require a specific OAuth2 scope string. This function returns the required scope for all expense and user operations:</p> <pre><code>def get_scope():\n    return (\n        \"openid USER user.read user.write LIST spend.list.read spend.listitem.read CONFIG EXPRPT FISVC \"\n        \"creditcardaccount.read IMAGE expense.exchangerate.writeonly profile.user.generaluser.read \"\n        \"profile.user.generalemployee.read expense.report.read expense.report.readwrite spend.list.write \"\n        \"spend.listitem.write identity.user.ids.read identity.user.core.read identity.user.coresensitive.read \"\n        \"identity.user.enterprise.read identity.user.event.read\"\n    )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_access_token","title":"Supporting Function: <code>get_access_token()</code>","text":"<p>This function retrieves an OAuth2 access token using your client ID, secret, and refresh token:</p> <pre><code>def get_access_token():\n    try:\n        return get_authentication_token(\n            client_id=SAP_CONCUR_CLIENT_APP_ID,\n            client_secret=SAP_CONCUR_CLIENT_SECRET,\n            refresh_token=SAP_CONCUR_REFRESH_TOKEN,\n            scope=get_scope(),\n        )\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_authentication_token","title":"Supporting Function: <code>get_authentication_token()</code>","text":"<p>Handles the actual OAuth2 token request:</p> <pre><code>def get_authentication_token(client_id, client_secret, refresh_token, scope):\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n        \"scope\": scope,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n    response = requests.post(SAP_CONCUR_OAUTH_END_POINT, headers=headers, data=data)\n    response.raise_for_status()\n    return response.json().get(\"access_token\")\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_cached_access_token","title":"Supporting Function: <code>get_cached_access_token()</code>","text":"<p>Caches the access token to avoid unnecessary requests:</p> <pre><code>access_token_cache = {\"token\": None, \"expires_at\": None}\n\ndef get_cached_access_token():\n    if access_token_cache[\"token\"] and access_token_cache[\"expires_at\"] &gt; datetime.now(timezone.utc):\n        return access_token_cache[\"token\"]\n    new_token = get_access_token()\n    if new_token:\n        access_token_cache[\"token\"] = new_token\n        access_token_cache[\"expires_at\"] = datetime.now(timezone.utc) + timedelta(hours=1)\n    return new_token\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#2-fetching-users-and-expense-reports","title":"2. Fetching Users and Expense Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_all_sap_concur_users","title":"Supporting Function: <code>get_all_sap_concur_users()</code>","text":"<p>Fetches all users from SAP Concur (with pagination):</p> <pre><code>def get_all_sap_concur_users():\n    try:\n        access_token = get_cached_access_token()\n        base_url = \"https://us.api.concursolutions.com/profile/identity/v4.1/Users\"\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\"\n        }\n        all_users = []\n        next_cursor = None\n        while True:\n            url = base_url\n            if next_cursor:\n                url += f\"?cursor={next_cursor}\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n            all_users.extend(data.get(\"items\", []))\n            next_cursor = data.get(\"nextCursor\")\n            if not next_cursor:\n                break\n        return all_users\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_expense_reports","title":"Supporting Function: <code>fetch_expense_reports()</code>","text":"<p>Fetches all expense reports for a given user:</p> <pre><code>def fetch_expense_reports(user_name, query_parameters):\n    access_token = get_cached_access_token()\n    base_url = f\"https://us.api.concursolutions.com/api/v3.0/expense/reports\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\", \"Accept\": \"application/json\"}\n    reports = []\n    next_page = f\"{base_url}?user={user_name}{query_parameters}\"\n    while next_page:\n        response = requests.get(next_page, headers=headers)\n        response.raise_for_status()\n        data = response.json()\n        reports.extend(data.get(\"Items\", []))\n        next_page = data.get(\"NextPage\")\n    return reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_all_expense_reports","title":"Supporting Function: <code>fetch_all_expense_reports()</code>","text":"<p>Fetches all reports for all users:</p> <pre><code>def fetch_all_expense_reports(user_mappings, query_parameters):\n    all_reports = []\n    for user in user_mappings:\n        reports = fetch_expense_reports(user, query_parameters)\n        for report in reports:\n            report[\"UserId\"] = user\n        all_reports.extend(reports)\n    return all_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#3-processing-and-aggregating-reports","title":"3. Processing and Aggregating Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-process_reports","title":"Supporting Function: <code>process_reports()</code>","text":"<p>Normalizes report data for aggregation:</p> <pre><code>def process_reports(all_reports):\n    return [\n        {\n            \"UserId\": report.get(\"UserId\"),\n            \"Name\": report.get(\"Name\"),\n            \"Total\": report.get(\"Total\"),\n            \"CurrencyCode\": report.get(\"CurrencyCode\"),\n            \"SubmitDate\": report.get(\"SubmitDate\"),\n            \"OwnerLoginID\": report.get(\"OwnerLoginID\"),\n            \"OwnerName\": report.get(\"OwnerName\"),\n            \"ApproverLoginID\": report.get(\"ApproverLoginID\"),\n            \"ApproverName\": report.get(\"ApproverName\"),\n            \"ApprovalStatusName\": report.get(\"ApprovalStatusName\"),\n            \"ApprovalStatusCode\": report.get(\"ApprovalStatusCode\"),\n            \"PaymentStatusName\": report.get(\"PaymentStatusName\"),\n            \"PaymentStatusCode\": report.get(\"PaymentStatusCode\"),\n            \"LastModifiedDate\": report.get(\"LastModifiedDate\"),\n            \"AmountDueEmployee\": report.get(\"AmountDueEmployee\"),\n            \"AmountDueCompanyCard\": report.get(\"AmountDueCompanyCard\"),\n            \"TotalClaimedAmount\": report.get(\"TotalClaimedAmount\"),\n            \"TotalApprovedAmount\": report.get(\"TotalApprovedAmount\"),\n            \"LedgerName\": report.get(\"LedgerName\"),\n            \"PolicyID\": report.get(\"PolicyID\"),\n            \"EverSentBack\": report.get(\"EverSentBack\"),\n            \"HasException\": report.get(\"HasException\"),\n            \"URI\": report.get(\"URI\"),\n        }\n        for report in all_reports\n    ]\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-employee-aggregate_expense_reports_by_employee","title":"Aggregating by Employee: <code>aggregate_expense_reports_by_employee()</code>","text":"<p>Groups and sums expense reports for each employee, optionally by approval status or by individual report.</p> <pre><code>def aggregate_expense_reports_by_employee(processed_reports, summary):\n    employee_reports = {}\n    for report in processed_reports:\n        user_name = str(report.get(\"OwnerLoginID\", \"\") or \"\").lower()\n        report_name = report.get(\"Name\", \"\")\n        report_id = report.get(\"ReportID\", \"\")\n        approval_status_code = str(report.get(\"ApprovalStatusCode\", \"\") or \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        key = (\n            f\"{approval_status_code}-({approval_status_name})\"\n            if summary\n            else f\"{report_name}-({report_id})-{approval_status_code}-({approval_status_name})\"\n        )\n        total = report.get(\"Total\", 0)\n        employee_reports.setdefault(user_name, {}).setdefault(key, 0)\n        employee_reports[user_name][key] += total\n    return employee_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-organization-aggregate_expense_reports_by_full_oraganization","title":"Aggregating by Organization: <code>aggregate_expense_reports_by_full_oraganization()</code>","text":"<p>Rolls up expense totals for each manager, including all direct and indirect reports, using a recursive helper.</p> <pre><code>def aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, summary):\n    object_organization_reports = {}\n    # Build a reverse mapping of manager to their direct reports\n    manager_to_reports = {}\n    for employee, details in management_upns.items():\n        manager = details.get(\"manager\")\n        if manager:\n            manager_to_reports.setdefault(manager.lower(), []).append(employee.lower())\n    def aggregate_totals_upwards(manager, visited):\n        if manager in visited:\n            return\n        visited.add(manager)\n        if manager not in object_organization_reports:\n            object_organization_reports[manager] = {}\n        for employee in manager_to_reports.get(manager, []):\n            aggregate_totals_upwards(employee, visited)\n            for status, total in object_organization_reports.get(employee, {}).items():\n                if status not in object_organization_reports[manager]:\n                    object_organization_reports[manager][status] = 0\n                object_organization_reports[manager][status] += total\n    # Populate initial totals for each employee based on processed reports\n    for report in processed_reports:\n        if report.get(\"UserManager\"):\n            user_manager = report.get(\"UserManager\", \"\").lower()\n        else:\n            continue\n        approval_status_code = report.get(\"ApprovalStatusCode\", \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        user_name = report.get(\"OwnerLoginID\", \"\")\n        key = f\"{approval_status_code}-({approval_status_name})\" if summary else f\"{user_name}-{approval_status_code}-({approval_status_name})\"\n        total = report.get(\"Total\", 0)\n        if user_manager not in object_organization_reports:\n            object_organization_reports[user_manager] = {}\n        if key not in object_organization_reports[user_manager]:\n            object_organization_reports[user_manager][key] = 0\n        object_organization_reports[user_manager][key] += total\n    # Aggregate totals upwards starting from all unique managers\n    visited = set()\n    for manager in manager_to_reports.keys():\n        aggregate_totals_upwards(manager, visited)\n    return object_organization_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#4-creating-adaptive-card-emails-summary-vs-detail-toggle","title":"4. Creating Adaptive Card Emails (Summary vs. Detail Toggle)","text":"<p>Adaptive Cards are JSON payloads that Outlook and Teams can render as interactive UI. Here\u2019s how to create a card with a summary and a toggle for details:</p> <pre><code>def create_adaptive_info_card_for_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    try:\n        summary_total = summary_by_organization.get(manager_email, 0)\n        detail_items = [\n            {\n                \"type\": \"TextBlock\",\n                \"text\": f\"{user}: {summary_by_employee.get(user, 0):,.2f}\",\n                \"wrap\": True\n            }\n            for user in detail_by_organization.get(manager_email, [])\n        ]\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.4\",\n            \"body\": [\n                {\"type\": \"TextBlock\", \"text\": \"Expense Report Summary\", \"weight\": \"Bolder\", \"size\": \"Large\"},\n                {\"type\": \"TextBlock\", \"text\": f\"Total for your organization: {summary_total:,.2f}\", \"wrap\": True},\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Click below to view details.\",\n                    \"wrap\": True,\n                    \"spacing\": \"Medium\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"id\": \"detailsContainer\",\n                    \"isVisible\": False,\n                    \"items\": detail_items\n                }\n            ],\n            \"actions\": [\n                {\n                    \"type\": \"Action.ToggleVisibility\",\n                    \"title\": \"Show/Hide Details\",\n                    \"targetElements\": [\"detailsContainer\"]\n                }\n            ]\n        }\n        return adaptive_card\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#sending-the-adaptive-card-email","title":"Sending the Adaptive Card Email","text":"<pre><code>def send_adaptive_info_email_to_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    adaptive_card = create_adaptive_info_card_for_manager(\n        manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports\n    )\n    email_payload = {\n        \"message\": {\n            \"subject\": \"Expense Report Summary\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                    f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                    f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                    f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n        }\n    }\n    send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#5-end-to-end-workflow-example","title":"5. End-to-End Workflow Example","text":"<p>Here\u2019s a high-level workflow you can adapt:</p> <pre><code>def main():\n    # 1. Fetch management hierarchy from your HR system\n    management_upns = fetch_management_upns()  # {employee: {\"manager\": manager_email, ...}}\n    # 2. Fetch all SAP Concur users\n    sap_concur_users = get_all_sap_concur_users()\n    # 3. Fetch all expense reports for all users\n    all_reports = fetch_all_expense_reports(sap_concur_users, \"&amp;submitDateAfter=2025-01-01\")\n    # 4. Normalize and process reports\n    processed_reports = process_reports(all_reports)\n    # 5. Aggregate by employee and organization\n    summary_by_employee = aggregate_expense_reports_by_employee(processed_reports, True)\n    summary_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, True)\n    detail_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, False)\n    # 6. Send Adaptive Card emails to each manager\n    for manager_email in summary_by_organization:\n        send_adaptive_info_email_to_manager(\n            manager_email, summary_by_employee, summary_by_organization, detail_by_organization, processed_reports\n        )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#references","title":"References","text":"<ul> <li>SAP Concur API Reference</li> <li>Microsoft Adaptive Cards</li> <li>Microsoft Graph API for Sending Mail</li> </ul>"},{"location":"sap-concur-expense-reports-aggregation/#conclusion","title":"Conclusion","text":"<p>With these patterns and supporting functions, you can connect to your own SAP Concur instance, fetch and aggregate expense reports by employee and by full reporting chain, and deliver actionable, interactive notifications to managers using Adaptive Cards. This enables powerful, organization-wide financial insights and automated reporting for managers at every level.</p>"},{"location":"sap-rfc-python-container/","title":"Installing the <code>PyRFC</code> Module for SAP Integration: A Step-by-Step Guide","text":"<p>Integrating Python with SAP systems using the <code>PyRFC</code> module can unlock powerful automation and data access capabilities. This article provides a clear, professional walkthrough for setting up the SAP NetWeaver RFC SDK and building the <code>PyRFC</code> Python package from scratch.</p>"},{"location":"sap-rfc-python-container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the SAP NetWeaver RFC SDK (download from the official SAP website)</li> <li>Basic familiarity with Linux command line</li> <li>Python 3.x and administrative privileges on your system</li> </ul>"},{"location":"sap-rfc-python-container/#1-download-the-netweaver-rfc-sdk","title":"1. Download the NetWeaver RFC SDK","text":"<ul> <li>Download the latest NetWeaver RFC SDK from the SAP website.</li> <li>Place the downloaded file (<code>nwrfc750P_14-70002752.zip</code>) in your repository's <code>assets</code> folder for easy access.</li> </ul>"},{"location":"sap-rfc-python-container/#2-prepare-the-sap-sdk-directory","title":"2. Prepare the SAP SDK Directory","text":"<p>Create the target directory for the SAP SDK:</p> <pre><code>sudo mkdir -p /usr/local/sap/\n</code></pre>"},{"location":"sap-rfc-python-container/#3-extract-and-copy-the-sdk","title":"3. Extract and Copy the SDK","text":"<ul> <li>Extract the <code>nwrfcsdk</code> folder from the ZIP file.</li> <li>Copy the extracted <code>nwrfcsdk</code> folder to <code>/usr/local/sap/</code>.</li> </ul>"},{"location":"sap-rfc-python-container/#4-configure-the-library-path","title":"4. Configure the Library Path","text":"<p>Create a configuration file for the dynamic linker and add the SDK library path:</p> <pre><code>sudo nano /etc/ld.so.conf.d/nwrfcsdk.conf\n\n# Add the following line to the file:\n/usr/local/sap/nwrfcsdk/lib\n</code></pre>"},{"location":"sap-rfc-python-container/#5-update-the-library-cache-and-set-environment-variable","title":"5. Update the Library Cache and Set Environment Variable","text":"<p>Update the system's library cache and set the required environment variable:</p> <pre><code>sudo ldconfig\n# Verify the path configuration should not have any error(s)\nldconfig -p | grep sap\n# Set Environment Variable\nexport SAPNWRFC_HOME=/usr/local/sap/nwrfcsdk\n</code></pre>"},{"location":"sap-rfc-python-container/#6-install-cython-and-build-essentials","title":"6. Install Cython and Build Essentials","text":"<p>Install the necessary build tools and Python dependencies:</p> <pre><code>pip install Cython\nsudo apt-get update\nsudo apt-get install -y build-essential python3-dev\n</code></pre>"},{"location":"sap-rfc-python-container/#7-build-and-install-pyrfc","title":"7. Build and Install <code>pyrfc</code>","text":"<p>Clone the PyRFC repository and build the package:</p> <pre><code>git clone https://github.com/SAP/PyRFC.git\ncd PyRFC\npython -m pip install --upgrade build\nPYRFC_BUILD_CYTHON=yes python -m build --wheel --sdist --outdir dist\npip install --upgrade --no-index --find-links=dist pyrfc\n</code></pre> <p>Pro Tip: Double-check all paths and environment variables before building. For troubleshooting, consult the PyRFC documentation or reach out to the SAP community forums.</p> <p>By following these steps, you\u2019ll have a working Python-to-SAP integration environment using the <code>pyrfc</code> module. Happy coding!</p>"},{"location":"sap-rfc-python/","title":"Calling SAP RFC Function Modules from Python Using PyRFC: A Step-by-Step Guide","text":"<p>Note: For details on installing and configuring the <code>PyRFC</code> module inside a container, see the companion article: Installing the PyRFC Module for SAP Integration</p>"},{"location":"sap-rfc-python/#introduction","title":"Introduction","text":"<p>SAP ECC systems expose powerful RFC (Remote Function Call) interfaces that allow external programs to interact with SAP data and business logic. Python, with the help of the PyRFC library, makes it possible to call these RFC function modules directly and process the results in a modern, flexible way.</p> <p>This article demonstrates how to: - Connect to an SAP ECC 6.0 (EHP 8) system from Python - Call a custom RFC function module  - Pass parameters to the RFC - Retrieve tabular data - Save the results to a CSV file</p> <p>We will use a modular, production-ready approach inspired by real-world enterprise integration scripts.</p>"},{"location":"sap-rfc-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an SAP ECC system with a custom RFC function module you can call</li> <li>SAP user credentials with RFC permissions</li> <li>The PyRFC library installed (see Installing the PyRFC Module for SAP Integration for setup)</li> <li>Python 3.7+</li> </ul>"},{"location":"sap-rfc-python/#example-extracting-data-from-sap-via-rfc","title":"Example: Extracting Data from SAP via RFC","text":"<p>Suppose you want to extract financial data from SAP using a custom RFC function module. The following example shows how to do this in a robust, reusable way.</p>"},{"location":"sap-rfc-python/#1-define-your-rfc-connection-and-extract-configuration","title":"1. Define Your RFC Connection and Extract Configuration","text":"<pre><code>from pyrfc import Connection, LogonError, ABAPApplicationError, ABAPRuntimeError\nimport csv\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"sap_rfc_extract\")\n\n# --- RFC Connection Parameters (replace with your SAP system details) ---\nSAP_CONN_PARAMS = {\n    'ashost': 'SAP_APP_SERVER_HOST',   # SAP application server\n    'sysnr': '00',                     # System number\n    'client': '100',                   # Client number\n    'user': 'SAP_USERNAME',            # SAP user\n    'passwd': 'SAP_PASSWORD',          # SAP password\n    'lang': 'EN',                      # Language\n}\n\n# --- RFC Extract Configuration ---\nEXTRACT_CONFIG = {\n    'example_extract': {\n        'function_module': 'ZMY_CUSTOM_RFC_MODULE',  # Replace with your RFC FM name\n        'table_name': 'IT_RESULT_TAB',              # The table returned by the RFC\n        'params': ['IM_CC', 'IM_YEAR', 'IM_PERIOD'],\n        'default_params': {'IM_CC': '1000', 'IM_YEAR': '2025', 'IM_PERIOD': '05'},\n        'filename_fmt': 'sap_extract_{cc}_{year}_{period}.csv',\n    },\n}\n</code></pre>"},{"location":"sap-rfc-python/#2-utility-functions-for-rfc-calls-and-csv-export","title":"2. Utility Functions for RFC Calls and CSV Export","text":"<pre><code>def call_rfc(conn_params, function_module, params):\n    try:\n        conn = Connection(**conn_params)\n        logger.info(f\"Calling RFC: {function_module} with params: {params}\")\n        return conn.call(function_module, **params)\n    except LogonError as e:\n        logger.error(f\"Logon Error: {e}\")\n    except ABAPApplicationError as e:\n        logger.error(f\"ABAP Application Error: {e}\")\n    except ABAPRuntimeError as e:\n        logger.error(f\"ABAP Runtime Error: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n    return None\n\ndef export_result_to_csv(table_data, filename):\n    if not table_data:\n        logger.warning(\"No data to export.\")\n        return\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=table_data[0].keys())\n        writer.writeheader()\n        writer.writerows(table_data)\n    logger.info(f\"Exported data to {filename}\")\n</code></pre>"},{"location":"sap-rfc-python/#3-main-script-running-the-extract","title":"3. Main Script: Running the Extract","text":"<pre><code>import argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run SAP RFC extract via PyRFC.\")\n    parser.add_argument('--im_cc', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_CC'], help='Company code')\n    parser.add_argument('--im_year', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_YEAR'], help='Fiscal year')\n    parser.add_argument('--im_period', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_PERIOD'], help='Fiscal period')\n    args = parser.parse_args()\n\n    # Prepare parameters for RFC call\n    params = {\n        'IM_CC': args.im_cc,\n        'IM_YEAR': args.im_year,\n        'IM_PERIOD': args.im_period,\n    }\n\n    config = EXTRACT_CONFIG['example_extract']\n    result = call_rfc(SAP_CONN_PARAMS, config['function_module'], params)\n    if result and config['table_name'] in result:\n        # Build filename\n        filename = config['filename_fmt'].format(\n            cc=args.im_cc, year=args.im_year, period=args.im_period\n        )\n        export_result_to_csv(result[config['table_name']], filename)\n    else:\n        logger.error(\"No data returned from RFC or table not found in result.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sap-rfc-python/#4-running-the-script","title":"4. Running the Script","text":"<p>You can run the script from the command line, specifying parameters as needed:</p> <pre><code>python sap_rfc_extract.py --im_cc=1000 --im_year=2025 --im_period=05\n</code></pre> <ul> <li>The script will connect to SAP, call the RFC, and save the results to a CSV file (e.g., <code>sap_extract_1000_2025_05.csv</code>).</li> <li>You can override any parameter using the command line.</li> </ul>"},{"location":"sap-rfc-python/#5-step-by-step-explanation","title":"5. Step-by-Step Explanation","text":"<ol> <li>Configuration:</li> <li>All SAP connection details and extract metadata are defined at the top for easy maintenance.</li> <li> <p>The RFC function module name and table name are generic placeholders\u2014replace them with your actual SAP details.</p> </li> <li> <p>Calling the RFC:</p> </li> <li>The <code>call_rfc</code> function establishes a connection and calls the RFC, handling common SAP errors.</li> <li> <p>Parameters are passed as a dictionary, matching the RFC signature.</p> </li> <li> <p>Exporting Data:</p> </li> <li> <p>The <code>export_result_to_csv</code> function writes the returned table to a CSV file, using the first row's keys as headers.</p> </li> <li> <p>Command-Line Interface:</p> </li> <li> <p>The script uses <code>argparse</code> to allow easy parameter overrides from the command line.</p> </li> <li> <p>Error Handling:</p> </li> <li>All errors are logged, and the script will not crash on SAP or network errors.</li> </ol>"},{"location":"sap-rfc-python/#conclusion","title":"Conclusion","text":"<p>With this approach, you can easily: - Call any SAP RFC function module from Python - Parameterize your extracts - Save results to CSV for downstream processing - Integrate SAP data into modern Python workflows</p> <p>For more advanced scenarios (multi-company code loops, dynamic extract configuration, etc.), see the full project code or reach out for further examples.</p>"},{"location":"sap-rfc-python/#further-reading","title":"Further Reading","text":"<ul> <li>PyRFC Documentation</li> <li>SAP RFC SDK</li> <li>Installing the PyRFC Module for SAP Integration \u2014 How to install and configure PyRFC in a container</li> </ul>"},{"location":"sharepoint-site-library-enumeration/","title":"SharePoint Files and Folders Inventory with Python and Microsoft Graph API","text":""},{"location":"sharepoint-site-library-enumeration/#introduction","title":"Introduction","text":"<p>This article provides a detailed, company-agnostic guide to inventorying all files and folders across all SharePoint sites and document libraries in a Microsoft 365 tenant using Python and the Microsoft Graph API. It focuses on the <code>get_all_files_from_sp</code> function and its supporting functions, with best practices for handling large environments, including recommendations for running the code in an Azure container.</p>"},{"location":"sharepoint-site-library-enumeration/#microsoft-graph-api-endpoint-constant","title":"Microsoft Graph API Endpoint Constant","text":"<p>The code uses the following constant for all Microsoft Graph API v1.0 calls:</p> <p><pre><code>AZURE_GRAPH_V1 = 'https://graph.microsoft.com/v1.0/'\n</code></pre> This ensures all API requests are made to the correct Microsoft Graph endpoint.</p>"},{"location":"sharepoint-site-library-enumeration/#key-functions-and-code-walkthrough","title":"Key Functions and Code Walkthrough","text":""},{"location":"sharepoint-site-library-enumeration/#1-get_all_files_from_sp","title":"1. <code>get_all_files_from_sp</code>","text":"<p>This is the main orchestration function for SharePoint inventory. It: - Retrieves all root SharePoint sites using <code>get_all_sp_sites</code>. - Expands the list to include all subsites with <code>fetch_all_sites_including_subsites</code>. - Iterates through every site and its document libraries, calling <code>process_document_library</code> for each. - Sends notification emails on progress and completion.</p> <p>Full Function Code: <pre><code>def get_all_files_from_sp():\n    try:\n        gdep_sharepoint_root_sites = get_all_sp_sites()\n        gdep_all_sites = fetch_all_sites_including_subsites(gdep_sharepoint_root_sites)\n\n        for site in gdep_all_sites:\n            site_id = site[\"id\"]\n            site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives\"\n\n            document_libraries = execute_odata_query_get(site_url)\n            for library in document_libraries:\n                process_document_library(site_id, library[\"id\"], library[\"name\"], gdep_all_sites)\n                send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n                    subject=f'Completed Doc Lib --&gt;{library[\"name\"]} - on site {site[\"webUrl\"]}',\n                    plain_message=f'Update on SP Library{library[\"name\"]} - for site {site_url}')\n\n        send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n            subject=f'Finished all Sites',\n            plain_message=f'Finished all Sites')\n\n    except Exception as e:\n        handle_global_exception(inspect.currentframe().f_code.co_name, e)\n    finally:\n        pass\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#explanation","title":"Explanation","text":"<ul> <li>Site Discovery: Uses <code>get_all_sp_sites()</code> to get all root sites, then <code>fetch_all_sites_including_subsites()</code> to get all subsites.</li> <li>Document Library Enumeration: For each site, queries all document libraries (drives) and processes them.</li> <li>Progress Notification: Sends emails after each library and when all sites are complete.</li> <li>Error Handling: All exceptions are logged and reported.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#2-get_all_sp_sites","title":"2. <code>get_all_sp_sites</code>","text":"<p>Fetches all root-level SharePoint sites in the tenant using the Microsoft Graph API: <pre><code>def get_all_sp_sites():\n    url = f\"{AZURE_GRAPH_V1}sites?search=*\"\n    return execute_odata_query_get(url)\n</code></pre> - Purpose: Returns a list of all root SharePoint sites. - API Used: List SharePoint Sites</p>"},{"location":"sharepoint-site-library-enumeration/#3-fetch_all_sites_including_subsites","title":"3. <code>fetch_all_sites_including_subsites</code>","text":"<p>Recursively discovers all subsites for each root site: <pre><code>def fetch_all_sites_including_subsites(sharepoint_root_sites):\n    all_sites = []\n    for site in sharepoint_root_sites:\n        logger.info(f\"Started site {site['webUrl']}\")\n        all_sites.append({\"id\": site[\"id\"], \"webUrl\": site[\"webUrl\"]})\n        sharepoint_subsites = get_sp_subsites(site[\"id\"])\n        if len(sharepoint_subsites) &gt; 0:\n            for subsite in sharepoint_subsites:\n                all_sites.append({\"id\": subsite[\"id\"], \"webUrl\": subsite[\"webUrl\"]})\n    return all_sites\n</code></pre> - Purpose: Ensures every site and subsite is included in the inventory. - API Used: List Subsites</p>"},{"location":"sharepoint-site-library-enumeration/#4-process_document_library","title":"4. <code>process_document_library</code>","text":"<p>Processes each document library (drive) for a site: <pre><code>def process_document_library(site_id, drive_id, drive_name, all_sites):\n    data = []\n    logger.info(f\"Started Document Library -- {drive_name}\")\n    start_time = time.perf_counter()\n    site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives/{drive_id}/root/delta{DOCUMENT_LIB_SELECT_QUERY}\"\n    search_results = execute_odata_query_get(site_url)\n    for item in search_results:\n        entry = {\n            \"site_id\": site_id,\n            \"webUrl\": next(site[\"webUrl\"] for site in all_sites if site[\"id\"] == site_id),\n            \"drive_id\": drive_id,\n            \"document_id\": item[\"id\"],\n            \"name\": item[\"name\"],\n            \"lastModifiedDateTime\": parse_iso_date(item.get(\"lastModifiedDateTime\")),\n            \"size\": item.get(\"size\") if \"file\" in item else \"\"\n        }\n        data.append(entry)\n    write_data_to_csv(data, SP_WITHOUT_VERSION_CSV_FILE_PATH)\n    elapsed_time = time.perf_counter() - start_time\n    logger.info(f\"Document Library '{drive_name}' took {elapsed_time:.2f} seconds to process.\")\n</code></pre> - Purpose:   - Queries all files in the document library using the Graph API delta endpoint.   - Collects metadata for each file.   - Writes results to a CSV for further processing or database import.   - Logs processing time for performance monitoring.</p>"},{"location":"sharepoint-site-library-enumeration/#supporting-utilities-full-implementations","title":"Supporting Utilities (Full Implementations)","text":""},{"location":"sharepoint-site-library-enumeration/#execute_odata_query_geturl","title":"<code>execute_odata_query_get(url)</code>","text":"<p>Handles authenticated GET requests to the Microsoft Graph API, including error handling and token refresh. <pre><code>def execute_odata_query_get(url):\n    try:\n        token = get_access_token_API_Access_AAD()\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json().get(\"value\", [])\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#parse_iso_datedate_str","title":"<code>parse_iso_date(date_str)</code>","text":"<p>Converts ISO 8601 date strings to Python datetime objects for easier manipulation and formatting. <pre><code>def parse_iso_date(date_str: str):\n    if not date_str:\n        return None\n    date_str = date_str.rstrip('Z')\n    formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\"]\n    for fmt in formats:\n        try:\n            return datetime.strptime(date_str, fmt).date()\n        except ValueError:\n            continue\n    return None\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#write_data_to_csvdata-file_path","title":"<code>write_data_to_csv(data, file_path)</code>","text":"<p>Appends data to a CSV file, writing headers if the file does not exist. <pre><code>def write_data_to_csv(data, file_path):\n    file_exists = os.path.isfile(file_path)\n    with open(file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n        fieldnames = [\"site_id\", \"webUrl\", \"drive_id\", \"document_id\", \"name\", \"lastModifiedDateTime\", \"size\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerows(data)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handle_global_exceptionfunctionname-exceptionobject","title":"<code>handle_global_exception(functionName, exceptionObject)</code>","text":"<p>Logs and emails details of any exception that occurs. <pre><code>def handle_global_exception(functionName, exceptionObject):\n    emailBody = f\"Function Name: {functionName}; Exception Description: {exceptionObject}\"\n    send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n               subject='Exception occured in code', \n               plain_message=emailBody)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#get_access_token_api_access_aadscopesnone","title":"<code>get_access_token_API_Access_AAD(scopes=None)</code>","text":"<p>Obtains an access token for Microsoft Graph API using MSAL or Azure Identity. (Example implementation:) <pre><code>def get_access_token_API_Access_AAD(scopes=None):\n    if scopes is None:\n        scopes = ['https://graph.microsoft.com/.default']\n    app = ConfidentialClientApplication(\n        client_id=AZURE_CONFIDENTIAL_APP_ID,\n        authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n        client_credential=AZURE_CONFIDENTIAL_SECRET\n    )\n    result = app.acquire_token_for_client(scopes=scopes)\n    return result[\"access_token\"]\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handling-large-sharepoint-environments","title":"Handling Large SharePoint Environments","text":"<p>Important: Large tenants with many sites, subsites, and document libraries can have tens or hundreds of thousands of files. Processing all of them can take significant time and resources.</p>"},{"location":"sharepoint-site-library-enumeration/#best-practices-for-large-document-libraries","title":"Best Practices for Large Document Libraries","text":"<ul> <li>Run in Azure: For large environments, it is highly recommended to run this inventory code in an Azure Container Instance or Azure VM. This ensures:</li> <li>Sufficient compute and memory resources.</li> <li>Proximity to Microsoft 365 services for faster API calls.</li> <li>Ability to scale or schedule the job as needed.</li> <li>Batch Processing: The code is designed to process and write data in batches, minimizing memory usage and allowing for partial progress in case of interruptions.</li> <li>Progress Notifications: The function sends email notifications after each document library and when all sites are complete, so you can monitor long-running jobs.</li> <li>Error Handling: All exceptions are logged and reported, ensuring that issues with individual sites or libraries do not halt the entire process.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#example-end-to-end-inventory-flow","title":"Example: End-to-End Inventory Flow","text":"<ol> <li>Discover Sites:</li> <li><code>get_all_sp_sites()</code> \u2192 returns all root sites.</li> <li>Expand to Subsites:</li> <li><code>fetch_all_sites_including_subsites()</code> \u2192 returns all sites and subsites.</li> <li>Process Each Library:</li> <li>For each site, enumerate all document libraries and call <code>process_document_library()</code>.</li> <li>Write Results:</li> <li>Metadata for each file is written to a CSV file for further analysis or database import.</li> </ol>"},{"location":"sharepoint-site-library-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API: List SharePoint Sites</li> <li>Microsoft Graph API: List Drive Items</li> <li>Azure Container Instances Documentation</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#conclusion","title":"Conclusion","text":"<p>The <code>get_all_files_from_sp</code> function and its supporting helpers provide a robust, scalable way to inventory all files and folders across a Microsoft 365 tenant's SharePoint environment. For large tenants, running this code in an Azure container or VM is strongly recommended to ensure reliability and performance.</p>"},{"location":"sharepoint-sites-enumeration/","title":"How to Retrieve All SharePoint Sites in Your Microsoft 365 Tenant","text":""},{"location":"sharepoint-sites-enumeration/#introduction","title":"Introduction","text":"<p>Retrieving a complete list of SharePoint sites in your Microsoft 365 (M365) tenant is essential for IT automation, reporting, and governance. This article provides a detailed, company-agnostic, step-by-step guide to programmatically enumerate all SharePoint sites using Python and the Microsoft Graph API. All code samples are generic and ready to use in any tenant.</p>"},{"location":"sharepoint-sites-enumeration/#prerequisites","title":"Prerequisites","text":""},{"location":"sharepoint-sites-enumeration/#1-azure-entra-application-registration","title":"1. Azure Entra Application Registration","text":"<ul> <li>Register an application in Azure Entra (Azure AD).</li> <li>Assign the following Microsoft Graph API permissions:</li> <li><code>Sites.Read.All</code> (Application permission)</li> <li><code>Sites.ReadWrite.All</code> (if you need to write/update)</li> <li>Grant admin consent for these permissions.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#2-certificate-based-authentication","title":"2. Certificate-Based Authentication","text":"<ul> <li>Upload a certificate to your Azure Entra application.</li> <li>Use the certificate thumbprint and private key for authentication.</li> <li>For a detailed guide and code on certificate-based authentication, see: Certificate Auth for Microsoft Graph API</li> </ul>"},{"location":"sharepoint-sites-enumeration/#3-python-environment","title":"3. Python Environment","text":"<ul> <li>Install the required packages:   <pre><code>pip install requests msal\n</code></pre></li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-1-authenticate-and-get-an-access-token","title":"Step 1: Authenticate and Get an Access Token","text":"<p>You need to authenticate as your Azure Entra application and obtain an access token for Microsoft Graph. This is best done using certificate-based authentication for security.</p> <p>Below is a full, reusable function for certificate-based authentication. (Replace the placeholders with your actual values.)</p> <pre><code>import msal\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    # Replace these with your app's values\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n</code></pre> <p>See this blog post for a full explanation and troubleshooting tips for certificate-based authentication.</p>"},{"location":"sharepoint-sites-enumeration/#step-2-query-the-microsoft-graph-api-for-sharepoint-sites","title":"Step 2: Query the Microsoft Graph API for SharePoint Sites","text":"<p>The Microsoft Graph API endpoint to list all sites is:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites?search=*\n</code></pre> <p>This returns a paginated list of root SharePoint sites in your tenant.</p>"},{"location":"sharepoint-sites-enumeration/#helper-function-execute-odata-query","title":"Helper Function: Execute OData Query","text":"<pre><code>import requests\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#retrieve-all-sites-with-pagination","title":"Retrieve All Sites (with Pagination)","text":"<pre><code>def get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation","title":"Explanation:","text":"<ul> <li><code>get_all_sp_sites</code> starts with the root search URL.</li> <li>It uses the access token for authentication.</li> <li>It loops through all pages using the <code>@odata.nextLink</code> property for pagination.</li> <li>All sites are collected in the <code>sites</code> list.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-3-retrieve-subsites-for-each-site","title":"Step 3: Retrieve Subsites for Each Site","text":"<p>To enumerate subsites for a given site, use:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{site-id}/sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#function-to-get-subsites","title":"Function to Get Subsites","text":"<pre><code>def get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation_1","title":"Explanation:","text":"<ul> <li>For each site, call <code>get_sp_subsites(site_id)</code> to get its direct subsites.</li> <li>You can recursively call this function to build a full site tree.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-4-full-example-enumerate-all-sites-and-subsites","title":"Step 4: Full Example - Enumerate All Sites and Subsites","text":"<p>Here is a complete script you can copy, edit, and run in your own environment:</p> <pre><code>import msal\nimport requests\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n\ndef get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n\ndef enumerate_all_sites_and_subsites():\n    all_sites = get_all_sp_sites()\n    all_sites_with_subsites = []\n    for site in all_sites:\n        site_id = site['id']\n        subsites = get_sp_subsites(site_id)\n        site['subsites'] = subsites\n        all_sites_with_subsites.append(site)\n    return all_sites_with_subsites\n\nif __name__ == \"__main__\":\n    all_sites = enumerate_all_sites_and_subsites()\n    print(json.dumps(all_sites, indent=2))\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#step-by-step-code-walkthrough","title":"Step-by-Step Code Walkthrough","text":"<ol> <li>get_access_token_API_Access_AAD: Authenticates using your Azure Entra app and certificate, returning a valid access token for Microsoft Graph.</li> <li>execute_odata_query_get: Sends a GET request to the specified Microsoft Graph endpoint using the access token, returning the parsed JSON response.</li> <li>get_all_sp_sites: Uses the <code>/sites?search=*</code> endpoint to retrieve all root SharePoint sites, handling pagination.</li> <li>get_sp_subsites: For each site, retrieves its direct subsites.</li> <li>enumerate_all_sites_and_subsites: Combines the above to build a list of all sites and their subsites.</li> <li>Main block: Runs the enumeration and prints the result as formatted JSON.</li> </ol>"},{"location":"sharepoint-sites-enumeration/#required-permissions-recap","title":"Required Permissions Recap","text":"<ul> <li><code>Sites.Read.All</code> (Application permission, admin consent required)</li> <li>The Azure Entra app must be granted consent by a tenant admin</li> <li>The app must authenticate using a certificate or secret (certificate recommended)</li> </ul>"},{"location":"sharepoint-sites-enumeration/#troubleshooting-and-tips","title":"Troubleshooting and Tips","text":"<ul> <li>If you get a 403 error, check that your app registration has admin consent for <code>Sites.Read.All</code>.</li> <li>If you get a 401 error, check your certificate and app credentials.</li> <li>The <code>search=*</code> parameter is required to enumerate all sites, not just the root site.</li> <li>For large tenants, always handle pagination using <code>@odata.nextLink</code>.</li> <li>You can extend the code to recursively enumerate subsites to any depth.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API - List sites</li> <li>Microsoft Graph API - List subsites</li> <li>Microsoft Graph permissions reference</li> <li>Register an application with the Microsoft identity platform</li> <li>Certificate credentials for application authentication</li> <li>MSAL for Python documentation</li> <li>Microsoft Graph Explorer</li> </ul>"},{"location":"sharepoint-sites-enumeration/#summary","title":"Summary","text":"<ul> <li>Register an Azure Entra application and grant it <code>Sites.Read.All</code> permission</li> <li>Authenticate using a certificate (see this blog post)</li> <li>Use the Microsoft Graph API <code>/sites?search=*</code> endpoint to enumerate all SharePoint sites</li> <li>Use <code>/sites/{site-id}/sites</code> to enumerate subsites</li> <li>Handle pagination using <code>@odata.nextLink</code></li> </ul> <p>This approach is secure, scalable, and works in any Microsoft 365 tenant. You can now automate SharePoint site inventory, reporting, or governance tasks in your own environment.</p>"}]}