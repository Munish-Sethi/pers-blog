{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Follow me on LinkedIn</p>"},{"location":"#ai","title":"\ud83e\udd16 AI","text":"<ul> <li>Data Analysis with LLM via MCP Server - Part 1</li> <li>Data Analysis with LLM via MCP Server - Part 2</li> <li>Data Analysis with LLM via MCP Server - Part 3</li> </ul>"},{"location":"#azure","title":"\ud83d\udd37 Azure","text":"<ul> <li>Certificate Based Authentication</li> <li>Query Recovery Services Vault</li> <li>Download Subscription Bill</li> <li>Download Azure Resource Listing</li> <li>Entra Users, User Groups and License assignments</li> <li>Entra User Devices</li> <li>Orchestrating Scheduled Jobs (Container Instances)</li> <li>DevOps Build Container</li> <li>DevOps Deploy Container IaC</li> <li>Building and Publishing a Custom Image - Part 1</li> <li>Deploying Azure Virtual Desktop (AVD) Desktops - Part 2</li> <li>Cleanup Obsolete FSLogix Profiles</li> <li>Business Continuity Program / Disaster Recovery - Part 1</li> <li>Business Continuity Program / Disaster Recovery - Part 2</li> <li>Business Continuity Program / Disaster Recovery - Git Hub Action</li> <li>DevOps Deploy Container Github Actions</li> <li>Restore VM from RSV Backup - Part 1</li> <li>Restore VM from RSV Backup - Part 2</li> </ul>"},{"location":"#fabric","title":"\ud83c\udfed Fabric","text":"<ul> <li>Warehouse data import</li> </ul>"},{"location":"#cisco-meraki-nagios-xi-manage-engine","title":"\ud83c\udf10 Cisco Meraki, Nagios XI &amp; Manage Engine","text":"<ul> <li>Cisco Meraki Unused Devices</li> <li>Cisco Meraki Devices Discovery and Nagios Integration</li> <li>Integrating Monitoring System with ITSM System</li> <li>Deploying Cisco Meraki vMX</li> </ul>"},{"location":"#microsoft-365","title":"\ud83c\udfe2 Microsoft 365","text":"<ul> <li>Outlook Actionable Adaptive Card - Part 1</li> <li>Outlook Actionable Adaptive Card - Part 2</li> <li>Sharepoint Sites Enumeration</li> <li>Sharepoint Document Library Enumeration</li> <li>Sharepoint Document Library Copy to Azure File Share</li> <li>Get all Teams Phone Assignment(s)</li> <li>Deprovison User</li> <li>Tenant Allow Block List - False Positive</li> <li>O365 and Proof Point Essentials - Part 1</li> <li>O365 and Proof Point Essentials - Part 2</li> <li>O365 and Proof Point Essentials - Part 3</li> <li>O365 and Proof Point Essentials - Part 4</li> <li>O365 and Proof Point Essentials - Part 5</li> <li>O365/Excel Automation via SPO</li> </ul>"},{"location":"#sap","title":"\ud83c\udfed SAP","text":"<ul> <li>Setup PyRFC in your Container</li> <li>Calling SAP RFC Function Modules from Python</li> <li>Concur Expense Report Aggregation</li> <li>Automating SAP System Start/Stop Operations</li> </ul>"},{"location":"#ukg","title":"\ud83d\udc65 UKG","text":"<ul> <li>Secure File Transfer with UKG Dimensions</li> <li>Employee Verification via API</li> <li>Automating Integration Execution and Dataview extract</li> </ul>"},{"location":"#active-directory-domain-services","title":"\ud83c\udfdb\ufe0f Active Directory Domain Services","text":"<ul> <li>Create User and assign groups</li> <li>Update User</li> </ul>"},{"location":"#miscellaneous","title":"\ud83d\udd27 Miscellaneous","text":"<ul> <li>Proof Point User Management</li> <li>Understanding DMARC/Spoofing, O365 with ProofPoint</li> <li>Clean GitHub Deployments</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/","title":"Reviewing Consultants via Adaptive Card (Actionable Outlook Messages) \u2013 Part 1","text":"<p>In this article, we\u2019ll walk through a real-world Python implementation for reviewing consultants using Adaptive Cards in Outlook. This solution enables managers to receive an actionable email, review their consultants, and submit decisions directly from their inbox. We'll cover the end-to-end process, focusing on how to build and send an actionable Adaptive Card email using Python and Microsoft Graph.</p> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part1/#overview","title":"Overview","text":"<p>The workflow consists of the following steps:</p> <ol> <li>Fetch consultants grouped by manager using <code>fetch_manager_consultants</code>.</li> <li>Build an Adaptive Card for each manager using <code>create_adaptive_card_outlook</code>.</li> <li>Send the Adaptive Card email using <code>send_adaptive_card_email</code>.</li> </ol> <p>Let\u2019s dive into each step and the code behind it.</p>"},{"location":"adaptive-card-consultant-review-part1/#1-fetching-consultants-grouped-by-manager","title":"1. Fetching Consultants Grouped by Manager","text":"<p>The function <code>fetch_manager_consultants(frequency)</code> retrieves consultants from the database and groups them by their manager\u2019s email.</p> <pre><code>def fetch_manager_consultants(frequency):\n    \"\"\"Fetch consultants grouped by their manager's email.\"\"\"\n    get_consultants_sql_statement = get_consultants_sql(frequency)\n    consultants_data = execute_Select_SQL_statement(get_consultants_sql_statement)[0]\n    manager_to_consultants = {}\n\n    try:\n        for row in consultants_data:\n            manager_email = row[5]\n            consultant_info = {\n                \"in_adp\": row[0],\n                \"name\": row[1],\n                \"last_logon\": row[2],\n                \"email\": row[3],\n                \"last_password_change\": row[4],\n                \"hire_date\": row[6],\n            }\n            manager_to_consultants.setdefault(manager_email, []).append(consultant_info)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, Exception)\n\n    return manager_to_consultants\n</code></pre> <ul> <li>Key Points:</li> <li>The function queries the database for consultant data.</li> <li>It organizes consultants by their manager\u2019s email, returning a dictionary mapping each manager to their consultants.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#2-building-the-adaptive-card","title":"2. Building the Adaptive Card","text":"<p>The function <code>create_adaptive_card_outlook(manager_email, consultants)</code> constructs an Adaptive Card JSON payload for Outlook. This card allows managers to review each consultant and select an action (keep active or deactivate).</p> <pre><code>def create_adaptive_card_outlook(manager_email, consultants):\n    \"\"\"Create an Adaptive Card with consultant details and actions.\"\"\"\n    try:\n        manager_name = manager_email.split('@')[0].split('.')[0]  # Extract manager's first name\n        inputs = []\n        action_data = {}\n\n        for consultant in consultants:\n            consultant_id = consultant[\"email\"].replace(\"@\", \"_\").replace(\".\", \"_\")\n            last_logon = consultant['last_logon'] or 'N/A'\n            hire_date = consultant['hire_date'] or 'N/A'\n\n            inputs.extend([\n                {\n                    \"type\": \"TextBlock\",\n                    \"wrap\": True,\n                    \"weight\": \"Bolder\",\n                    \"color\": \"Warning\",\n                    \"spacing\": \"Medium\",\n                    \"text\": \"****\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"padding\": \"None\",\n                    \"spacing\": \"None\",\n                    \"items\": [\n                        {\n                            \"type\": \"TextBlock\",\n                            \"text\": f\"**{consultant['name']}** ({consultant['email']}), with Last Logon: {last_logon} and Hire Date: {hire_date}\",\n                            \"weight\": \"Bolder\",\n                            \"wrap\": True\n                        },\n                        {\n                            \"type\": \"Input.ChoiceSet\",\n                            \"id\": f\"decision_{consultant_id}\",\n                            \"isMultiSelect\": False,\n                            \"value\": \"keep\",\n                            \"choices\": [\n                                {\"title\": \"Keep Active\", \"value\": \"keep\"},\n                                {\"title\": \"Deactivate\", \"value\": \"deactivate\"}\n                            ],\n                            \"style\": \"expanded\",\n                            \"spacing\": \"None\",\n                        }\n                    ]\n                }\n            ])\n            action_data[consultant_id] = {\n                \"decision\": f\"{{{{decision_{consultant_id}.value}}}}\",\n                \"email\": consultant[\"email\"],\n                \"manageremail\": manager_email,\n                \"managername\": manager_name,\n            }\n\n        inputs.append({\n            \"type\": \"TextBlock\",\n            \"wrap\": True,\n            \"weight\": \"Bolder\",\n            \"color\": \"Warning\",\n            \"spacing\": \"Medium\",\n            \"text\": \"****\"\n        })\n\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.0\",\n            \"originator\": ORGINATOR_ID,\n            \"body\": [\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Consultant Review\",\n                    \"weight\": \"bolder\",\n                    \"size\": \"extraLarge\",\n                    \"color\": \"attention\",\n                    \"separator\": True,\n                    \"horizontalAlignment\": \"center\",\n                    \"spacing\": \"small\",\n                    \"wrap\": True\n                },\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": (\n                        f\"Hello {manager_name}, please review the details of your consultants and select the appropriate action. \"\n                        \"Some consultants may not be in the HR system as they were set up directly as Guest accounts, so their hire date will show as N/A. \"\n                        \"Please review all consultants and provide feedback so that appropriate action can be taken if they no longer require network access.\"\n                    ),\n                    \"wrap\": True,\n                    \"color\": \"Default\",\n                    \"spacing\": \"Medium\",\n                    \"weight\": \"Bolder\"\n                }\n            ] + inputs,\n            \"actions\": [\n                {\n                    \"type\": \"Action.Http\",\n                    \"title\": \"Submit Consultant Actions\",\n                    \"headers\": [\n                        {\"name\": \"Content-Type\", \"value\": \"application/json\"},\n                        {\"name\": \"Authorization\", \"value\": \"\"}\n                    ],\n                    \"method\": \"POST\",\n                    \"url\": \"https://api.example.com/consultant-review-confirmation\",\n                    \"body\": \"\"\n                }\n            ],\n            \"style\": \"default\"\n        }\n\n        # Prepare the action data for the body\n        action_data_str = json.dumps(action_data)\n        adaptive_card['actions'][0]['body'] = action_data_str\n\n        email_payload = {\n            \"message\": {\n            \"subject\": \"Consultant Review - Action Required\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n            \"bccRecipients\": [{\"emailAddress\": {\"address\": \"audit@example.com\"}}]\n            }\n        }\n\n        return email_payload\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        return None\n</code></pre> <ul> <li>Key Points:</li> <li>The card is dynamically built for each manager and their consultants.</li> <li>Each consultant has a choice set for the manager to select \"Keep Active\" or \"Deactivate\".</li> <li>The card is embedded in the email as a <code>&lt;script type='application/adaptivecard+json'&gt;...&lt;/script&gt;</code> block, which is required for actionable messages in Outlook.</li> <li>The card uses an <code>Action.Http</code> action to POST the manager\u2019s decisions to a specified endpoint.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#3-sending-the-adaptive-card-email","title":"3. Sending the Adaptive Card Email","text":"<p>The function <code>send_adaptive_card_email(email_payload)</code> sends the constructed Adaptive Card email using the Microsoft Graph API.</p> <pre><code>def send_adaptive_card_email(email_payload):\n    \"\"\"Send an email with an embedded Adaptive Card using Microsoft Graph API.\"\"\"\n    try:\n        user_id = \"your-user-guid\"\n        graph_api_url = f\"https://graph.microsoft.com/v1.0/users/{user_id}/sendMail\"\n        access_token = get_access_token_API_Access_AAD()\n\n        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n\n        response = requests.post(graph_api_url, json=email_payload, headers=headers)\n\n        if response.status_code == 202:\n            print(\"Email sent successfully!\")\n        else:\n            print(f\"Failed to send email: {response.status_code}, {response.text}\")\n\n    except requests.exceptions.RequestException as req_error:\n        handle_global_exception(sys._getframe().f_code.co_name, req_error)\n        print(f\"Request error occurred: {req_error}\")\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        print(f\"An unexpected error occurred: {error}\")\n</code></pre> <ul> <li>Key Points:</li> <li>The function authenticates using an Azure AD access token.</li> <li>It sends the email via the Microsoft Graph <code>/sendMail</code> endpoint.</li> <li>The Adaptive Card is delivered as an actionable message in Outlook.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#end-to-end-example","title":"End-to-End Example","text":"<p>Here\u2019s how you might orchestrate the process:</p> <pre><code>def process_consultants(frequency):\n    manager_to_consultants = fetch_manager_consultants(frequency)\n    for manager_email, consultants in manager_to_consultants.items():\n        email_payload = create_adaptive_card_outlook(manager_email, consultants)\n        if email_payload:\n            send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part1/#conclusion","title":"Conclusion","text":"<p>This article (Part 1) demonstrated how to:</p> <ul> <li>Fetch consultants grouped by manager.</li> <li>Build an Adaptive Card for actionable review in Outlook.</li> <li>Send the Adaptive Card email using Microsoft Graph.</li> </ul> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part2/","title":"Company-Agnostic Adaptive Card Consultant Review Blog (Part 2, Deep Dive)","text":""},{"location":"adaptive-card-consultant-review-part2/#introduction","title":"Introduction","text":"<p>In Part 1, we covered how to send actionable Adaptive Card emails for consultant review. In this Part 2, we focus on the backend: how to securely receive, verify, and process the manager's response when the Adaptive Card is submitted. This article recursively examines each function involved in the request processing chain, providing a complete, end-to-end understanding of how an incoming Adaptive Card request is handled\u2014with all relevant Python code included and all company-specific references replaced with generic placeholders (e.g., <code>mycompany.com</code>).</p>"},{"location":"adaptive-card-consultant-review-part2/#1-endpoint-consultant-review-confirmation","title":"1. Endpoint: <code>/consultant-review-confirmation</code>","text":"<p>When a manager submits the Adaptive Card, the card's action posts the data to the <code>/consultant-review-confirmation</code> endpoint:</p> <pre><code>@app.route('/consultant-review-confirmation', methods=['POST'])\ndef consultant_review_confirmation():\n    try:\n        payload, client_ip, error_response, status_code = process_request_headers_and_payload(request)\n        if error_response:\n            return error_response, status_code\n        process_adaptive_card_payload(payload, client_ip)\n        return jsonify({\"status\": \"success\", \"message\": \"Actions processed successfully\"}), 200\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>This route does two things: 1. Verifies the request and extracts the payload using <code>process_request_headers_and_payload</code>. 2. Processes the submitted data using <code>process_adaptive_card_payload</code>.</p>"},{"location":"adaptive-card-consultant-review-part2/#2-deep-dive-process_request_headers_and_payload","title":"2. Deep Dive: <code>process_request_headers_and_payload</code>","text":"<p>This function is responsible for: - Extracting and logging request headers. - Validating the JWT Bearer token in the <code>Action-Authorization</code> header. - Decoding the token and verifying its authenticity. - Extracting the JSON payload from the request.</p> <pre><code>def process_request_headers_and_payload(request):\n    headers = dict(request.headers)\n    logger.info(f\"Request headers: {headers}\")\n    action_auth_header = headers.get(\"Action-Authorization\", \"\")\n    client_ip = headers.get(\"X-Forwarded-For\", \"\")\n    logger.info(f\"Incoming request from IP: {client_ip}\")\n    logger.info(f\"Action Authorization: {action_auth_header}\")\n    if not action_auth_header.startswith(\"Bearer \"):\n        logger.error(f\"Missing or invalid Bearer token in Action-Authorization header from {client_ip}\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Missing Bearer token\"}), 401\n    token = action_auth_header.split(\" \", 1)[1]\n    log_jwt_payload(token)\n    public_key = fetch_public_key(token)\n    if not public_key:\n        logger.error(\"Public key not found!\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    if not validate_token(token, public_key):\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    payload = request.get_json()\n    logger.info(f\"Payload: {payload}\")\n    return payload, client_ip, None, None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#21-log_jwt_payloadtoken","title":"2.1. <code>log_jwt_payload(token)</code>","text":"<p>Logs the decoded JWT payload (without verifying the signature) for debugging and traceability.</p> <pre><code>def log_jwt_payload(token):\n    \"\"\"Logs the decoded JWT payload without verification.\"\"\"\n    payload = jwt.decode(token, options={\"verify_signature\": False})\n    for key, value in payload.items():\n        logger.info(f\"{key}: {value}\")\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#22-fetch_public_keytoken","title":"2.2. <code>fetch_public_key(token)</code>","text":"<p>Extracts the key ID (<code>kid</code>) from the JWT header, fetches the public keys from the identity provider's JWKS endpoint, and finds the matching key for signature verification.</p> <pre><code>def fetch_public_key(token):\n    \"\"\"Fetches the public key for the given token.\"\"\"\n    try:\n        header = jwt.get_unverified_header(token)\n        key_id = header.get(\"kid\")\n        jwks_url = 'https://substrate.office.com/sts/common/discovery/keys'  # Replace with your IdP's JWKS endpoint if needed\n        jwks = requests.get(jwks_url).json()\n        for key in jwks[\"keys\"]:\n            if key[\"kid\"] == key_id:\n                return RSAAlgorithm.from_jwk(json.dumps(key))\n    except Exception as e:\n        raise Exception(f\"Error fetching public key: {e}\")\n    return None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#23-validate_tokentoken-public_key","title":"2.3. <code>validate_token(token, public_key)</code>","text":"<p>Decodes and verifies the JWT signature using the public key, checks the token's issuer and audience, and raises an error if the token is expired or invalid.</p> <pre><code>def validate_token(token, public_key):\n    \"\"\"Validates the JWT token using the public key.\"\"\"\n    try:\n        decoded_token = jwt.decode(\n            token, public_key, algorithms=[\"RS256\"], audience=\"https://api.mycompany.com\"\n        )\n        if decoded_token.get(\"iss\") != \"https://substrate.office.com/sts/\":  # Replace with your IdP's issuer if needed\n            raise Exception(\"Invalid issuer!\")\n        return True\n    except jwt.ExpiredSignatureError:\n        raise Exception(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        raise Exception(\"Invalid token!\")\n</code></pre> <p>Summary: Only requests with a valid JWT token (issued by your identity provider) are accepted. The payload is only processed if authentication passes. All actions are logged for traceability.</p>"},{"location":"adaptive-card-consultant-review-part2/#3-deep-dive-process_adaptive_card_payload","title":"3. Deep Dive: <code>process_adaptive_card_payload</code>","text":"<p>This function is responsible for: - Iterating through the submitted consultant actions. - Extracting manager and consultant details from the payload. - Taking the appropriate action (e.g., sending confirmation emails, saving to disk, triggering downstream automation).</p> <pre><code>def process_adaptive_card_payload(payload, client_ip):\n    for consultant_id, values in payload.items():\n        manager_email = values.get(\"manageremail\")\n        manager_name = values.get(\"managername\")\n        # ... process each consultant's action ...\n    send_email_to_manager(payload, manager_email, manager_name)\n    save_payload_to_disk(payload, manager_email)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#31-send_email_to_managerpayload-manager_email-manager_name","title":"3.1. <code>send_email_to_manager(payload, manager_email, manager_name)</code>","text":"<p>Builds an HTML summary of the manager's actions for all consultants and sends a confirmation email to the manager with a table of decisions (keep/deactivate).</p> <pre><code>def send_email_to_manager(payload, manager_email, manager_name):\n    \"\"\"Sends an HTML formatted email to the manager.\"\"\"\n    try:\n        subject = \"Consultant Review Actions Summary\"\n        body = f\"\"\"\n        &lt;html&gt;\n        &lt;body style=\\\"font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;p&gt;Dear {manager_name},&lt;/p&gt;\n            &lt;p&gt;Thank you for reviewing the consultants. Below is a summary of your actions:&lt;/p&gt;\n            &lt;table border=\\\"1\\\" style=\\\"border-collapse: collapse; width: 100%; font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Consultant Email&lt;/th&gt;\n                &lt;th&gt;Decision&lt;/th&gt;\n            &lt;/tr&gt;\n        \"\"\"\n        for consultant_id, values in payload.items():\n            body += f\"&lt;tr&gt;&lt;td&gt;{values.get('email')}&lt;/td&gt;&lt;td&gt;{values.get('decision')}&lt;/td&gt;&lt;/tr&gt;\"\n        body += \"\"\"\n            &lt;/table&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n        send_email(recipients=[manager_email], subject=subject, html_message=body)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#32-save_payload_to_diskpayload-manager_email","title":"3.2. <code>save_payload_to_disk(payload, manager_email)</code>","text":"<p>Serializes the entire payload to a JSON file and saves it to a mounted share or persistent storage for auditing and further processing.</p> <pre><code>def save_payload_to_disk(payload, manager_email):\n    \"\"\"Saves the entire payload to the mounted share as a single JSON file.\"\"\"\n    try:\n        import os, json, datetime\n        filename = f\"{manager_email}_consultant_review_{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}.json\"\n        path = os.path.join(UNPROCESSED_PATH, filename)\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(payload, f, ensure_ascii=False, indent=2)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#4-downstream-automation-process_deactived_consultants","title":"4. Downstream Automation: <code>process_deactived_consultants</code>","text":"<p>This function is typically run on a schedule to process all submitted consultant reviews: - Loads all unprocessed review files from disk. - For each consultant marked for deactivation, adds them to a deactivation list. - Sends a summary email to HR and IT for further action. - Moves processed files to an archive location.</p> <pre><code>def process_deactived_consultants():\n    deactivate_list = []\n    manager_consultants_files = fetch_and_ignore_unprocessed_review_files()\n    for file in manager_consultants_files:\n        file_path, file_name = file.rsplit('/', 1)\n        file_time_utc = os.path.getmtime(file)\n        file_time = datetime.fromtimestamp(file_time_utc, pytz.utc).astimezone(pytz.timezone('America/Chicago'))\n        with open(file, 'r') as f:\n            file_content = f.read()\n        consultants_data = json.loads(file_content)\n        for consultant, details in consultants_data.items():\n            if details.get('decision') == 'deactivate':\n                deactivate_list.append({\n                    'manager_email': details.get('manageremail'),\n                    'consultant_email': details.get('email'),\n                    'approval_time': file_time.strftime('%Y-%m-%d %H:%M:%S')\n                })\n    if deactivate_list:\n        send_email_to_hr_and_it(deactivate_list)\n    for file in manager_consultants_files:\n        file_name = os.path.basename(file)\n        processed_file_path = os.path.join(PROCESSED_PATH, file_name)\n        os.rename(file, processed_file_path)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#5-error-handling-handle_global_exception","title":"5. Error Handling: <code>handle_global_exception</code>","text":"<p>All major functions use <code>handle_global_exception</code> to log and report errors, ensuring that issues are traceable and do not silently fail.</p> <pre><code>def handle_global_exception(function_name, exception_obj):\n    logger.error(f\"Exception in {function_name}: {exception_obj}\")\n    # Optionally, send an alert email or take other action\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#6-recap-full-request-processing-chain","title":"6. Recap: Full Request Processing Chain","text":"<ol> <li>Adaptive Card submission posts to <code>/consultant-review-confirmation</code>.</li> <li><code>process_request_headers_and_payload</code> authenticates and extracts the payload.<ul> <li>Calls <code>log_jwt_payload</code>, <code>fetch_public_key</code>, <code>validate_token</code>.</li> </ul> </li> <li><code>process_adaptive_card_payload</code> processes the payload.<ul> <li>Calls <code>send_email_to_manager</code>, <code>save_payload_to_disk</code>.</li> </ul> </li> <li><code>process_deactived_consultants</code> (scheduled) processes all reviews and notifies HR/IT.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#7-example-end-to-end-flow","title":"7. Example: End-to-End Flow","text":"<ol> <li>Manager receives Adaptive Card, reviews consultants, and submits actions.</li> <li>Submission is POSTed to <code>/consultant-review-confirmation</code> with a JWT Bearer token.</li> <li>The backend verifies the token, extracts the payload, and logs all actions.</li> <li>The manager receives a confirmation email summarizing their decisions.</li> <li>The payload is saved for auditing and further automation (e.g., account deactivation).</li> <li>HR/IT are notified of deactivation approvals as needed.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#conclusion","title":"Conclusion","text":"<p>By recursively examining each function and providing the full code, you can see how the system securely and reliably processes Adaptive Card submissions. This approach is company-agnostic and can be adapted to any workflow requiring secure, actionable messaging in Outlook.</p> <ul> <li>Always validate and log incoming requests.</li> <li>Process and audit all actions.</li> <li>Automate downstream actions as needed.</li> </ul> <p>This completes the deep dive into the end-to-end workflow for actionable consultant review using Adaptive Cards and Python.</p>"},{"location":"adds-user-creation/","title":"Automating Active Directory User Creation and Group Assignment","text":""},{"location":"adds-user-creation/#introduction","title":"Introduction","text":"<p>Automating user provisioning in Active Directory Domain Services (ADDS) is a common requirement for IT teams managing large organizations. Python, with its rich ecosystem of libraries, makes it possible to programmatically create users and assign them to groups in ADDS. This article provides a detailed walkthrough of a working Python implementation for creating new ADDS users and adding them to groups, using the <code>ldap3</code> library and related tools.</p>"},{"location":"adds-user-creation/#overview-of-the-workflow","title":"Overview of the Workflow","text":"<p>The core function, <code>create_new_users_adds</code>, orchestrates the process of:</p> <ol> <li>Establishing a secure connection to the ADDS server.</li> <li>Creating a new user account with the required attributes.</li> <li>Setting the user's password and enabling the account.</li> <li>Adding the user to one or more ADDS groups.</li> </ol> <p>This workflow is modular, with each step handled by a dedicated function or library call, making it easy to adapt for different environments.</p>"},{"location":"adds-user-creation/#step-1-establishing-a-connection-to-adds","title":"Step 1: Establishing a Connection to ADDS","text":"<p>The function <code>get_adds_Connection</code> uses the <code>ldap3</code> library to connect to the ADDS server over SSL. Credentials are securely retrieved (in this codebase, from Azure Key Vault, but you can use environment variables or other secure stores):</p> <pre><code>server = ldap3.Server(dc_ip, use_ssl=True)\nconn = ldap3.Connection(server, user=LDAP_USER_ID, password=LDAP_USER_PASSWORD)\nif not conn.bind():\n    print('Error in bind', conn.result)\n</code></pre> <p>This returns a connection object used for all subsequent LDAP operations.</p>"},{"location":"adds-user-creation/#step-2-creating-a-new-user-in-adds","title":"Step 2: Creating a New User in ADDS","text":"<p>The function <code>create_adds_user</code> (called within <code>create_new_users_adds</code>) performs the following:</p> <ul> <li> <p>Adds the user object: <pre><code>conn.add(\n    distinguished_name,\n    ['top', 'person', 'organizationalPerson', 'user'],\n    {\n        'givenName': first_name,\n        'sn': last_name,\n        'sAMAccountName': sam_account_name,\n        'userPrincipalName': upn_name,\n        'mail': upn_name\n    }\n)\n</code></pre>   The <code>distinguished_name</code> (DN) specifies the user's location in the directory tree (OU). For generalization, replace any organization-specific OUs with your own structure.</p> </li> <li> <p>Enables the account and sets the password: <pre><code>conn.modify(\n    distinguished_name,\n    {\n        'userAccountControl': [(ldap3.MODIFY_REPLACE, [512])],  # Enable the account\n        'unicodePwd': [(ldap3.MODIFY_REPLACE, [f'\"{default_password}\"'.encode('utf-16-le')])]\n    }\n)\n</code></pre>   The password must be encoded in UTF-16-LE and quoted. The <code>userAccountControl</code> value of 512 enables the account.</p> </li> </ul>"},{"location":"adds-user-creation/#step-3-adding-the-user-to-adds-groups","title":"Step 3: Adding the User to ADDS Groups","text":"<p>After the user is created, the code assigns them to one or more groups using the <code>add_members_to_group</code> function from <code>ldap3.extend.microsoft.addMembersToGroups</code>:</p> <p><pre><code>add_members_to_group(conn, [distinguished_name], group_dns, fix=True)\n</code></pre> - <code>conn</code>: The active LDAP connection. - <code>[distinguished_name]</code>: A list of user DNs to add. - <code>group_dns</code>: A list of group DNs (distinguished names) to which the user should be added. - <code>fix=True</code>: Ensures the function will attempt to fix any inconsistencies in group membership.</p> <p>This function performs the necessary LDAP modifications to add the user as a member of each specified group. It is robust and handles group membership updates according to Microsoft's AD schema.</p>"},{"location":"adds-user-creation/#error-handling-and-best-practices","title":"Error Handling and Best Practices","text":"<ul> <li>Error Handling: Each step is wrapped in try/except blocks, and errors are logged or emailed to administrators. This is critical for production automation.</li> <li>Security: Credentials are not hardcoded. Use secure storage for service accounts and passwords.</li> <li>Generalization: Replace any organization-specific OUs or group names with your own. The logic is portable to any ADDS environment.</li> </ul>"},{"location":"adds-user-creation/#example-creating-and-assigning-a-user","title":"Example: Creating and Assigning a User","text":"<p>Here is a simplified, generalized version of the workflow:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\nfrom ldap3.extend.microsoft.addMembersToGroups import ad_add_members_to_groups as add_members_to_group\n\n# Connect to ADDS\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\n# Create user\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nconn.add(dn, ['top', 'person', 'organizationalPerson', 'user'], {\n    'givenName': 'John',\n    'sn': 'Doe',\n    'sAMAccountName': 'jdoe',\n    'userPrincipalName': 'jdoe@example.com',\n    'mail': 'jdoe@example.com'\n})\n\n# Enable account and set password\nconn.modify(dn, {\n    'userAccountControl': [(MODIFY_REPLACE, [512])],\n    'unicodePwd': [(MODIFY_REPLACE, ['\"YourPassword123!\"'.encode('utf-16-le')])]\n})\n\n# Add to groups\ngroup_dns = ['CN=YourGroup,OU=Groups,DC=example,DC=com']\nadd_members_to_group(conn, [dn], group_dns, fix=True)\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-creation/#conclusion","title":"Conclusion","text":"<p>With Python and the <code>ldap3</code> library, you can fully automate the process of creating users and managing group memberships in Active Directory. This approach is scalable, secure, and adaptable to any ADDS environment. By modularizing each step and handling errors robustly, you can integrate this workflow into larger HR or IT automation pipelines.</p>"},{"location":"adds-user-creation/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"adds-user-update/","title":"Updating Active Directory User Attributes","text":""},{"location":"adds-user-update/#introduction","title":"Introduction","text":"<p>Active Directory Domain Services (ADDS) is the backbone of identity management in many organizations. While user creation and group assignment are common automation tasks, updating user attributes\u2014both standard (delivered) and custom\u2014is equally important for keeping directory data accurate and useful. This article explains, with practical Python code, how to update ADDS user attributes using the <code>ldap3</code> library, focusing on the function <code>update_existing_users_adds</code>.</p>"},{"location":"adds-user-update/#understanding-adds-attributes-delivered-vs-custom","title":"Understanding ADDS Attributes: Delivered vs. Custom","text":"<ul> <li>Delivered (Standard) Attributes:</li> <li>These are built-in attributes provided by Microsoft, such as <code>givenName</code>, <code>sn</code>, <code>title</code>, <code>department</code>, <code>telephoneNumber</code>, etc.</li> <li>They are part of the default AD schema and are widely supported by tools and scripts.</li> <li>Custom Attributes:</li> <li>Organizations can extend the AD schema to include custom attributes (e.g., <code>extensionAttribute1</code>, <code>departmentNumber</code>).</li> <li>These are used for business-specific data not covered by standard attributes.</li> </ul> <p>Both types can be updated using the same LDAP operations.</p>"},{"location":"adds-user-update/#the-python-approach-using-ldap3","title":"The Python Approach: Using ldap3","text":"<p>The <code>ldap3</code> library provides a high-level, Pythonic interface for interacting with ADDS. The function <code>update_existing_users_adds</code> demonstrates how to:</p> <ol> <li>Build a dictionary of user attributes to update (both standard and custom).</li> <li>Connect to ADDS securely.</li> <li>Use the <code>modify</code> method to update attributes for each user.</li> <li>Handle errors and notify administrators if updates fail.</li> </ol>"},{"location":"adds-user-update/#step-by-step-updating-user-attributes","title":"Step-by-Step: Updating User Attributes","text":""},{"location":"adds-user-update/#1-prepare-the-attribute-dictionary","title":"1. Prepare the Attribute Dictionary","text":"<p>For each user, a dictionary is built with the attributes to update. This can include both delivered and custom attributes:</p> <pre><code>item = {\n    'displayName': display_name,           # Standard\n    'givenName': first_name,               # Standard\n    'sn': last_name,                       # Standard\n    'title': title,                        # Standard\n    'department': department,              # Standard\n    'employeeType': employee_type,         # Standard\n    'extensionAttribute1': is_mgmt_position, # Custom\n    'manager': manager_dn,                 # Standard (DN of manager)\n    # ... add more as needed ...\n}\n</code></pre>"},{"location":"adds-user-update/#2-connect-to-adds","title":"2. Connect to ADDS","text":"<pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n</code></pre>"},{"location":"adds-user-update/#3-update-attributes-with-modify","title":"3. Update Attributes with <code>modify</code>","text":"<p>The <code>modify</code> method is used to update one or more attributes for a user. The changes dictionary maps attribute names to a tuple specifying the operation (e.g., <code>MODIFY_REPLACE</code>) and the new value(s):</p> <p><pre><code>changes = {key: (MODIFY_REPLACE, [value]) for key, value in item.items() if value}\nconn.modify(dn=distinguished_name, changes=changes)\n</code></pre> - <code>dn</code>: The distinguished name of the user to update. - <code>changes</code>: A dictionary of attribute updates.</p>"},{"location":"adds-user-update/#4-error-handling-and-notification","title":"4. Error Handling and Notification","text":"<p>After each modify operation, the result is checked. If the update fails, an email notification is sent to administrators:</p> <pre><code>if conn.result['result'] != 0:\n    send_email(\n        recipients=['admin@example.com'],\n        subject=f'Error while updating user {distinguished_name}',\n        plain_message=f\"An error occurred while modifying user: {conn.result}\",\n    )\n</code></pre>"},{"location":"adds-user-update/#example-updating-a-users-attributes","title":"Example: Updating a User's Attributes","text":"<p>Here is a simplified, generalized example:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nchanges = {\n    'title': (MODIFY_REPLACE, ['Senior Engineer']),\n    'department': (MODIFY_REPLACE, ['Engineering']),\n    'extensionAttribute1': (MODIFY_REPLACE, ['Project Lead'])\n}\nconn.modify(dn=dn, changes=changes)\n\nif conn.result['result'] != 0:\n    print(f\"Error updating user: {conn.result}\")\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-update/#best-practices","title":"Best Practices","text":"<ul> <li>Batch Updates: You can update multiple attributes in a single <code>modify</code> call for efficiency.</li> <li>Custom Attributes: Ensure custom attributes exist in your AD schema before attempting to update them.</li> <li>Error Handling: Always check the result of LDAP operations and log or notify on failure.</li> <li>Security: Never hardcode credentials; use secure storage.</li> </ul>"},{"location":"adds-user-update/#conclusion","title":"Conclusion","text":"<p>Updating user attributes in ADDS with Python and <code>ldap3</code> is straightforward and powerful. Whether you are updating standard or custom attributes, the process is the same. By following the approach in <code>update_existing_users_adds</code>, you can automate directory maintenance and ensure your AD data stays current and accurate.</p>"},{"location":"adds-user-update/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part1/","title":"Secure, On-Premises Data Analysis with LLM and a Custom MCP Server","text":""},{"location":"ai-claude-mcp-analytic-server-part1/#part-1-csvparquet-files","title":"Part 1: CSV/Parquet files","text":""},{"location":"ai-claude-mcp-analytic-server-part1/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: CSV/Parquet files (Current)</li> <li>Part 2: CSV/Parquet &amp; Database</li> <li>Part 3: HTTPS-Based MCP Server with Azure OAuth</li> </ul> <p>Inspired by and adapted from mcp-analyst. This guide documents a customized, production-ready approach for secure, LLM-powered analytics on enterprise CSV/Parquet data, including new tools and deployment strategies for real-world business needs.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#introduction","title":"Introduction","text":"<p>Many organizations accumulate large numbers of CSV and Parquet files\u2014sometimes related, sometimes not. These files are often daily or periodic extracts from a variety of systems, created for business leaders (like the CIO) who need insights but lack the time or tools to analyze them manually. For example, you might have daily extracts of:</p> <ul> <li>Office 365 license assignments</li> <li>OneDrive storage usage</li> <li>Account properties (e.g., password last set, last login)</li> <li>Device inventories from Entra ID</li> <li>HR or SAP exports</li> <li>And much more</li> </ul> <p>While these files can be loaded into Excel for pivots or imported into a database, this process is time-consuming and often impractical for busy executives. This solution enables natural language queries directly on your CSV/Parquet files\u2014no manual analysis or report building required.</p> <p>This guide shows how to empower business users to analyze all their data, securely, using a local MCP (Model Context Protocol) server and Claude Desktop.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#full-code-srcanalystpy-with-customizations","title":"Full Code: <code>src/analyst.py</code> (with Customizations)","text":"<p>The following code is based on mcp-analyst but has been extended for real-world enterprise use. Notably, it adds the <code>get_data_catalog</code> tool for rich business context, SAP CSV delimiter handling, and is designed for packaging as a Windows executable.</p> <p>Below is the complete, up-to-date code for the MCP server. This code exposes tools for file listing, schema discovery, business context, and SQL query execution. It uses the high-performance Polars library for data processing and is ready for packaging as a Windows executable for easy deployment.</p> <pre><code>import argparse\nfrom typing import List, Dict, Any, Optional\nfrom pydantic import Field\nfrom mcp.server.fastmcp import FastMCP\nfrom glob import glob\nimport polars as pl\nimport json\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--file_location\", type=str, default=\"data/*.csv\")\nargs = parser.parse_args()\n\nmcp = FastMCP(\"analyst\", dependencies=[\"polars\"])\n\n@mcp.tool()\ndef get_files_list() -&gt; str:\n    \"\"\"\n    Get the list of files that are source of data\n    \"\"\"\n    files_list = glob(args.file_location)\n    return \"\\n\".join(files_list)\n\n@mcp.tool()\ndef get_data_catalog() -&gt; str:\n    \"\"\"\n    Get the data catalog with descriptions of all available datasets.\n\n    Use this tool to understand:\n    - What each CSV file contains and its business purpose\n    - Column meanings, data types, and business rules\n    - Data relationships and join keys between files\n    - Common query patterns and best practices\n\n    This catalog provides business context. Always combine with get_schema() \n    to see the actual current columns in each file.\n\n    DATA CATALOG STRUCTURE:\n    The data_catalog.json file follows this schema:\n\n    {\n      \"version\": \"X.X\",                    // Catalog version number\n      \"last_updated\": \"YYYY-MM-DD\",        // Last modification date\n      \"datasets\": {\n        \"dataset_key\": {                   // Lowercase key (not filename)\n          \"filename\": \"Actual_File.csv\",   // Real filename (case-sensitive)\n          \"category\": \"Category_Name\",     // From categories section\n          \"description\": \"...\",            // Business purpose\n          \"source_system\": \"...\",          // Origin system(s)\n          \"delimiter\": \",\" or \";\",         // CSV delimiter\n          \"update_frequency\": \"...\",       // Optional: refresh schedule\n\n          \"columns\": {                     // Column documentation\n            \"_note\": \"...\",                // ALWAYS start with _note explaining schema approach\n\n            // Then document ONLY the important/non-obvious columns:\n            \"critical_column_name\": {\n              \"description\": \"...\",        // What this column means\n              \"data_type\": \"...\",          // string|integer|boolean|date\n              \"required\": true|false,      // Is it always populated?\n              \"importance\": \"CRITICAL\",    // Mark important columns\n              \"possible_values\": [...],    // Optional: enum values\n              \"constant_value\": \"...\",     // Optional: if always same\n              \"format\": \"...\",             // Optional: date formats etc\n              \"usage_note\": \"...\",         // Optional: how to use it\n              \"source\": \"...\"              // Optional: for prefixed columns (adds_, entra_, etc.)\n            }\n            // Leave out self-descriptive columns - LLM will discover via get_schema()\n          },\n\n          \"business_rules\": [...],         // Important constraints\n          \"common_queries\": [...],         // Example questions\n          \"device_types_included\": [...],  // Optional: for inventories\n\n          \"joins\": {                       // Optional: join definitions\n            \"join_name\": {\n              \"target_dataset\": \"...\",\n              \"join_condition\": \"...\",\n              \"description\": \"...\"\n            }\n          },\n\n          \"related_datasets\": [...],       // Related file keys\n          \"usage_tip\": \"...\",              // Consumption guidance\n          \"comparison_note\": \"...\"         // Optional: vs other files\n        }\n      },\n\n      \"categories\": {                      // Category definitions\n        \"Category_Name\": {\n          \"description\": \"...\"\n        }\n      }\n    }\n\n    IMPORTANT CONVENTIONS:\n    - Dataset keys are lowercase with underscores (e.g., \"idp_hr\")\n    - Filenames preserve original casing (e.g., \"IDP_HR.csv\")\n    - **Columns section philosophy**: Document only important/non-obvious columns\n      * ALWAYS start with \"_note\" explaining the schema approach\n      * Document critical columns (joins, filters, business rules)\n      * Skip self-descriptive columns - LLM discovers them via get_schema()\n      * This reduces maintenance and focuses LLM attention on what matters\n    - Always include \"usage_tip\" directing users to call get_schema()\n    - Column prefixes (adds_, entra_, hr_) indicate source systems\n    - \"importance\": \"CRITICAL\" marks key columns for joins/filters\n    - Delimiter usually \",\" but \";\" for SAP files (sap_* prefix)\n\n    Returns comprehensive documentation for all datasets in JSON format.\n    \"\"\"\n\n    # Use the same directory logic as the CSV files\n    file_pattern = args.file_location\n    data_dir = os.path.dirname(file_pattern)\n    catalog_path = os.path.join(data_dir, \"data_catalog.json\")\n\n    if os.path.exists(catalog_path):\n        with open(catalog_path, 'r', encoding='utf-8') as f:\n            catalog = json.load(f)\n        return json.dumps(catalog, indent=2)\n    else:\n        return json.dumps({\n            \"error\": \"Data catalog not found\",\n            \"path_checked\": catalog_path,\n            \"note\": \"Ensure data_catalog.json is in the same folder as CSV files\",\n            \"schema_help\": \"See get_data_catalog() docstring for data_catalog.json structure\"\n        })\n\n        return pl.concat(dfs)\n    elif file_type == \"parquet\":\n        dfs = pl.read_parquet(file_locations)\n        return dfs\n    else:\n        raise ValueError(f\"Unsupported file type: {file_type}\")\n\n@mcp.tool()\ndef get_schema(\n    file_location: str,\n    file_type: str = Field(\n        description=\"The type of the file to be read. Supported types are csv and parquet\",\n        default=\"csv\",\n    ),\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get the schema of a single data file from the given file location\n    \"\"\"\n    df = read_file(file_location, file_type)\n    schema = df.schema\n    schema_dict = {}\n    for key, value in schema.items():\n        schema_dict[key] = str(value)\n    return [schema_dict]\n\n# ... (Polars SQL function lists omitted for brevity)\n\n@mcp.tool()\ndef execute_polars_sql(\n    file_locations: List[str],\n    query: str = Field(\n        description=\"The polars sql query to be executed.\",\n    ),\n    file_type: str = Field(\n        description=\"The type of the file to be read. Supported types are csv and parquet\",\n        default=\"csv\",\n    ),\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Reads the data from the given file locations. Executes the given polars sql query and returns the result.\n    \"\"\"\n    df = read_file_list(file_locations, file_type)\n    op_df = df.sql(query)\n    output_records = op_df.to_dicts()\n    return output_records\n\ndef main():\n    mcp.run()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ai-claude-mcp-analytic-server-part1/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"ai-claude-mcp-analytic-server-part1/#1-file-listing-get_files_list","title":"1. File Listing (<code>get_files_list</code>)","text":"<p>Lists all available CSV/Parquet files for the LLM to consider. Only files in the specified directory are accessible.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#2-data-catalog-get_data_catalog","title":"2. Data Catalog (<code>get_data_catalog</code>)","text":"<p>This is the most important customization in this solution.</p> <ul> <li>Purpose: Supplies the LLM with business context, column definitions, relationships, and usage tips for each dataset, as defined in <code>data/data_catalog.json</code>.</li> <li>How it works:<ul> <li>The tool reads the JSON catalog and returns it as a string.</li> <li>The catalog documents not just columns, but also business rules, join keys, and common queries.</li> <li>This enables the LLM to:<ul> <li>Understand which columns are critical for joins or filters</li> <li>Know which datasets are related (e.g., HR and IDP)</li> <li>Avoid common mistakes (e.g., duplicate records, wrong filters)</li> <li>Provide more accurate, context-aware answers</li> </ul> </li> </ul> </li> <li>Why not just use <code>get_schema</code>?<ul> <li><code>get_schema</code> only shows column names and types. It cannot explain business meaning, relationships, or best practices.</li> <li>The catalog is especially valuable for large, complex datasets and when onboarding new users or LLMs.</li> </ul> </li> <li>Security: Only metadata is exposed, not actual data.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part1/#3-schema-discovery-get_schema","title":"3. Schema Discovery (<code>get_schema</code>)","text":"<p>Lets the LLM see the actual columns and types in each file. Handles files with millions of rows efficiently using Polars.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#4-data-reading-sap-exception","title":"4. Data Reading (SAP Exception)","text":"<p>SAP systems often export CSVs with <code>;</code> instead of <code>,</code>\u2014this logic ensures correct parsing for SAP files.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#5-sql-query-execution-execute_polars_sql","title":"5. SQL Query Execution (<code>execute_polars_sql</code>)","text":"<p>Executes LLM-generated Polars SQL queries on one or more files. Handles large datasets (millions of rows) with high performance.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#data-catalog-example-datadata_catalogjson","title":"Data Catalog Example (<code>data/data_catalog.json</code>)","text":"<pre><code>{\n  \"version\": \"1.1\",\n  \"last_updated\": \"2025-10-19\",\n  \"datasets\": {\n    \"idp_hr\": {\n      \"filename\": \"idp_hr.csv\",\n      \"category\": \"Identity_Management\",\n      \"description\": \"Master identity dataset merging ADDS, Entra ID, and HR system attributes...\",\n      \"columns\": {\n        \"_note\": \"Most columns are self-descriptive with prefixes indicating source system (adds_, entra_, hr_, onedrive_). Critical columns documented below.\",\n        \"entra_id\": {\n          \"description\": \"Entra user GUID. JOIN KEY from entra_user_devices.csv (user_id column).\",\n          \"data_type\": \"string\",\n          \"importance\": \"CRITICAL - Primary key for joins\"\n        }\n      },\n      \"business_rules\": [\n        \"One row per person - all ADDS, Entra, and HR attributes merged\"\n      ],\n      \"common_queries\": [\n        \"Users in ADDS but not Entra (source_adds_entra = 'adds')\"\n      ],\n      \"usage_tip\": \"This is the PRIMARY user lookup table. Use column prefixes to understand data source. Use get_schema() to explore all columns.\"\n    }\n  }\n}\n</code></pre>"},{"location":"ai-claude-mcp-analytic-server-part1/#security-and-privacy","title":"Security and Privacy","text":"<ul> <li>Data never leaves the network: All computation happens locally; only query results are sent to the user.</li> <li>No cloud LLM risk: Unlike SaaS analytics or cloud LLMs, your sensitive data is never uploaded.</li> <li>Fine-grained access: Only files in the specified directory are accessible.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part1/#deployment-and-usage-packaging-as-a-windows-executable","title":"Deployment and Usage: Packaging as a Windows Executable","text":"<p>One of the key requirements for enterprise adoption is zero Python installation on user PCs. To achieve this:</p> <ol> <li>Package the MCP server as a single Windows executable using PyInstaller:         <pre><code>pyinstaller --onefile --name cio-mcp_server src\\analyst.py\n</code></pre>         - This bundles Python, all dependencies, and your code into a single <code>.exe</code> file.         - No Python, pip, or library installs are needed on the user's machine.</li> <li>Deploy the executable and all CSV/Parquet files (plus <code>data_catalog.json</code>) on a secure server share.</li> <li>Users only need Claude Desktop and access to the server share.</li> <li>Configure Claude Desktop to use the MCP server executable as a local tool.</li> <li>All computation and data access remain on-premises.</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part1/#lessons-learned-and-best-practices","title":"Lessons Learned and Best Practices","text":"<ul> <li>LLMs can often infer schema, but the data catalog is essential for complex, cross-file queries and business logic.</li> <li>Polars handled millions of rows with ease\u2014far better than Excel or Power BI for ad hoc queries.</li> <li>SAP CSV delimiter handling is critical: Many SAP exports use <code>;</code> instead of <code>,</code>. This is handled automatically in the code.</li> <li>Even with well-defined semantic models, neither Power BI's LLM tools nor Fabric matched the flexibility and accuracy of this approach.</li> <li>Business users can answer their own questions\u2014no more waiting for IT to build new reports.</li> <li>Security: No data leaves the network. Only query results are returned to the user.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part1/#conclusion","title":"Conclusion","text":"<p>By combining a local MCP server (customized from mcp-analyst), Claude Desktop, and a well-maintained data catalog, you can deliver secure, flexible, and powerful analytics on enterprise CSV/Parquet data. Business users can ask any question, across any dataset, without moving data or waiting for IT. The addition of the <code>get_data_catalog</code> tool is a game-changer for context-aware analytics, enabling the LLM to reason about business logic, relationships, and best practices.</p>"},{"location":"ai-claude-mcp-analytic-server-part1/#references-and-credits","title":"References and Credits","text":"<ul> <li>mcp-analyst GitHub repository \u2014 Original codebase and inspiration for this solution.</li> <li>Polars DataFrame library \u2014 High-performance data processing in Python.</li> <li>Claude Desktop \u2014 LLM client for natural language analytics.</li> <li>PyInstaller \u2014 For packaging Python code as a Windows executable.</li> </ul> <p>For more details, see the full code in <code>src/analyst.py</code> and the data catalog in <code>data/data_catalog.json</code>.</p>"},{"location":"ai-claude-mcp-analytic-server-part2/","title":"Secure, On-Premises Data Analysis with LLM and a Custom MCP Server","text":""},{"location":"ai-claude-mcp-analytic-server-part2/#part-2-csvparquet-database","title":"Part 2: CSV/Parquet &amp; Database","text":""},{"location":"ai-claude-mcp-analytic-server-part2/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: CSV/Parquet</li> <li>Part 2: CSV/Parquet &amp; Database (Current)</li> <li>Part 3: HTTPS-Based MCP Server with Azure OAuth</li> </ul> <p>This is Part 2 of the series. Part 1 focused on CSV/Parquet file analytics. Here, we extend the solution to support hybrid queries across both files and SQL Server databases, enabling even richer business intelligence and operational reporting.</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#introduction","title":"Introduction","text":"<p>As organizations mature, data is increasingly distributed across both flat files (CSV, Parquet) and enterprise databases (SQL Server). Business leaders need answers from all these sources\u2014sometimes in combination. This article shows how to extend your MCP server to support:</p> <ul> <li>Hybrid queries across CSV/Parquet files and SQL Server databases</li> <li>Dynamic connection management and authentication</li> <li>Catalog-driven tool selection for each dataset</li> <li>Unified schema discovery and query execution</li> </ul> <p>This guide demonstrates how to empower users to query both files and databases using natural language, with all data remaining on-premises.</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#solution-architecture-hybrid-data-access","title":"Solution Architecture: Hybrid Data Access","text":"<ol> <li>MCP server now supports both file and database sources, with connection info and catalog-driven logic.</li> <li>Claude Desktop interacts with the MCP server, automatically selecting the right tool for each dataset.</li> <li>User asks questions (e.g., \"Show me terminated employees from HR database and their device inventory from CSV files\").</li> <li>MCP server tools provide file lists, database table lists, schema, and business context.</li> <li>LLM generates Polars SQL or T-SQL queries based on the source type.</li> <li>MCP server executes the query on the correct source(s), returning results to the user.</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part2/#the-role-of-the-data-catalog-json","title":"The Role of the Data Catalog JSON","text":"<p>The <code>data/data_catalog.json</code> file is the key to business context and technical guidance for the LLM. It: - Lists all available datasets, with a <code>source_type</code> field (<code>file</code> or <code>database</code>) - Provides business descriptions, columns, rules, and relationships - Tells the LLM which tool to use for each dataset</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#example-file-based-dataset","title":"Example: File-based Dataset","text":"<p><pre><code>\"accounts_to_be_deleted\": {\n  \"source_type\": \"file\",\n  \"filename\": \"accounts_to_be_deleted.csv\",\n  \"category\": \"IT_Operations\",\n  \"description\": \"Active Directory accounts eligible for deletion. Contains employees terminated more than 90 days ago.\",\n  \"common_queries\": [\"How many accounts are pending deletion?\", \"Accounts terminated in a specific date range\"],\n  \"related_datasets\": [\"identity_master\"],\n  \"usage_tip\": \"Use get_schema() to see all columns.\"\n}\n</code></pre> - LLM sees <code>source_type: file</code> and knows to use <code>execute_polars_sql()</code> - Query example: <code>SELECT COUNT(*) FROM self WHERE hr_TerminationDate &lt; '2025-07-01'</code></p>"},{"location":"ai-claude-mcp-analytic-server-part2/#example-database-based-dataset","title":"Example: Database-based Dataset","text":"<p><pre><code>\"adp_employee_daily_snapshot\": {\n  \"source_type\": \"database\",\n  \"database_schema\": \"mcp\",\n  \"database_table\": \"vw_ADP_Employee_Daily_Snapshot\",\n  \"category\": \"HR_Analytics\",\n  \"description\": \"Denormalized daily snapshot of HR system data from ADP. This is the same data as hr_data.csv but stored in SQL Server for better performance on large queries.\",\n  \"common_queries\": [\"Employee tenure calculations using Hire_Date\"],\n  \"related_datasets\": [\"user_license_assignments\"],\n  \"usage_tip\": \"Use get_schema_db(schema_name='mcp', table_name='vw_ADP_Employee_Daily_Snapshot') to see all available columns.\"\n}\n</code></pre> - LLM sees <code>source_type: database</code> and knows to use <code>execute_database_query()</code> - Query example: <code>SELECT COUNT(*) FROM mcp.vw_ADP_Employee_Daily_Snapshot WHERE Position_Status = 'Active'</code></p>"},{"location":"ai-claude-mcp-analytic-server-part2/#full-code-srcanalystpy-hybrid-version","title":"Full Code: <code>src/analyst.py</code> (Hybrid Version)","text":"<p>Below is the complete code for the MCP server, with detailed explanations for all relevant sections, especially those handling database connectivity and hybrid logic.</p> <pre><code>from typing import List, Dict, Any, Optional\nfrom pydantic import Field\nfrom mcp.server.fastmcp import FastMCP\nfrom glob import glob\nimport argparse\nimport os\nimport pyodbc\nimport polars as pl\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--file_location\", type=str, default=\"data/*.csv\")\nparser.add_argument(\"--sql_server\", type=str, default=None,\n          help=\"SQL Server instance (e.g., localhost or server.domain.com)\")\nparser.add_argument(\"--sql_database\", type=str, default=None,\n          help=\"SQL Server database name\")\nparser.add_argument(\"--sql_auth\", type=str, default=\"windows\",\n          choices=[\"windows\", \"sql\"],\n          help=\"Authentication type: 'windows' or 'sql'\")\nparser.add_argument(\"--sql_username\", type=str, default=None,\n          help=\"SQL Server username (only for SQL auth)\")\nparser.add_argument(\"--sql_password\", type=str, default=None,\n          help=\"SQL Server password (only for SQL auth)\")\nparser.add_argument(\"--catalog_path\", type=str, default=\"data/data_catalog.json\")\nargs = parser.parse_args()\n\nmcp = FastMCP(\"analystwithsql\", dependencies=[\"polars\", \"pyodbc\"])\n\n# Connection management\n_connection = None\n_connection_string = None\n\ndef build_connection_string() -&gt; str:\n  \"\"\"Build SQL Server connection string based on arguments\"\"\"\n  if not args.sql_server or not args.sql_database:\n    return None\n  drivers = [\n    \"ODBC Driver 18 for SQL Server\",\n    \"ODBC Driver 17 for SQL Server\",\n    \"SQL Server Native Client 11.0\",\n    \"SQL Server\"\n  ]\n  available_driver = None\n  try:\n    installed_drivers = [d for d in pyodbc.drivers()]\n    for driver in drivers:\n      if driver in installed_drivers:\n        available_driver = driver\n        break\n  except:\n    available_driver = \"SQL Server\"  # Fallback\n  if args.sql_auth == \"windows\":\n    conn_str = (\n      f\"Driver={{{available_driver}}};\"\n      f\"Server={args.sql_server};\"\n      f\"Database={args.sql_database};\"\n      f\"Trusted_Connection=yes;\"\n    )\n  else:\n    if not args.sql_username or not args.sql_password:\n      return None\n    conn_str = (\n      f\"Driver={{{available_driver}}};\"\n      f\"Server={args.sql_server};\"\n      f\"Database={args.sql_database};\"\n      f\"UID={args.sql_username};\"\n      f\"PWD={args.sql_password};\"\n    )\n  if \"ODBC Driver 17\" in available_driver or \"ODBC Driver 18\" in available_driver:\n    conn_str += \"Encrypt=yes;TrustServerCertificate=yes;\"\n  return conn_str\n\ndef get_db_connection():\n  \"\"\"Get or create database connection with connection pooling\"\"\"\n  global _connection, _connection_string\n  if _connection_string is None:\n    _connection_string = build_connection_string()\n  if _connection_string is None:\n    return None\n  try:\n    if _connection is not None:\n      _connection.cursor().execute(\"SELECT 1\")\n  except:\n    _connection = None\n  if _connection is None:\n    _connection = pyodbc.connect(_connection_string)\n  return _connection\n\ndef is_database_configured() -&gt; bool:\n  \"\"\"Check if database connection is configured\"\"\"\n  return args.sql_server is not None and args.sql_database is not None\n\n@mcp.tool()\ndef get_connection_info() -&gt; Dict[str, Any]:\n  \"\"\"\n  Get information about configured data sources and connection status.\n  \"\"\"\n  info = {\n    \"file_sources\": {\n      \"enabled\": True,\n      \"pattern\": args.file_location,\n      \"available_files\": glob(args.file_location)\n    },\n    \"database_sources\": {\n      \"enabled\": is_database_configured(),\n      \"server\": args.sql_server,\n      \"database\": args.sql_database,\n      \"auth_type\": args.sql_auth if is_database_configured() else None\n    }\n  }\n  if is_database_configured():\n    try:\n      conn = get_db_connection()\n      cursor = conn.cursor()\n      cursor.execute(\"SELECT @@VERSION\")\n      info[\"database_sources\"][\"status\"] = \"connected\"\n      info[\"database_sources\"][\"server_version\"] = cursor.fetchone()[0]\n    except Exception as e:\n      info[\"database_sources\"][\"status\"] = f\"error: {e}\"\n  return info\n\n@mcp.tool()\ndef get_data_catalog() -&gt; str:\n  \"\"\"\n  Get the comprehensive data catalog with descriptions of all available datasets.\n  \"\"\"\n  if os.path.exists(args.catalog_path):\n    with open(args.catalog_path, \"r\") as f:\n      return f.read()\n  else:\n    return \"Catalog file not found.\"\n\n@mcp.tool()\ndef get_files_list() -&gt; str:\n  \"\"\"\n  Get the list of CSV/Parquet files available in the file system.\n  \"\"\"\n  files_list = glob(args.file_location)\n  if not files_list:\n    return \"No files found.\"\n  return \"\\n\".join(files_list)\n\n@mcp.tool()\ndef get_database_tables() -&gt; str:\n  \"\"\"\n  Get the list of database tables available in SQL Server.\n  \"\"\"\n  if not is_database_configured():\n    return \"Database not configured.\"\n  try:\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'\")\n    tables = cursor.fetchall()\n    result = []\n    for schema, table in tables:\n      result.append(f\"{schema}.{table}\")\n    return \"\\n\".join(result)\n  except Exception as e:\n    return f\"Error: {e}\"\n\n@mcp.tool()\ndef get_schema(file_location: str, file_type: str = \"csv\") -&gt; List[Dict[str, Any]]:\n  \"\"\"\n  Get the technical schema of a CSV or Parquet file.\n  \"\"\"\n  try:\n    df = pl.read_csv(file_location) if file_type == \"csv\" else pl.read_parquet(file_location)\n    return [{col: str(df[col].dtype) for col in df.columns}]\n  except Exception as e:\n    return [{\"error\": str(e)}]\n\n@mcp.tool()\ndef get_schema_db(schema_name: str, table_name: str) -&gt; List[Dict[str, Any]]:\n  \"\"\"\n  Get the technical schema of a SQL Server database table.\n  \"\"\"\n  if not is_database_configured():\n    return [{\"error\": \"Database not configured.\"}]\n  try:\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    cursor.execute(f\"SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA=? AND TABLE_NAME=?\", (schema_name, table_name))\n    columns = cursor.fetchall()\n    return [{col[0]: {\"data_type\": col[1], \"nullable\": col[2]} for col in columns}]\n  except Exception as e:\n    return [{\"error\": str(e)}]\n\n@mcp.tool()\ndef execute_polars_sql(file_locations: List[str], query: str, file_type: str = \"csv\") -&gt; List[Dict[str, Any]]:\n  \"\"\"\n  Execute a Polars SQL query on one or more files.\n  \"\"\"\n  # Implementation omitted for brevity\n  pass\n\n@mcp.tool()\ndef execute_database_query(query: str, max_rows: int = 1000) -&gt; List[Dict[str, Any]]:\n  \"\"\"\n  Execute a T-SQL query on the configured SQL Server database.\n  \"\"\"\n  if not is_database_configured():\n    return [{\"error\": \"Database not configured.\"}]\n  try:\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    cursor.execute(query)\n    columns = [desc[0] for desc in cursor.description]\n    rows = cursor.fetchmany(max_rows)\n    return [dict(zip(columns, row)) for row in rows]\n  except Exception as e:\n    return [{\"error\": str(e)}]\n\ndef main():\n  \"\"\"Main entry point for the MCP server\"\"\"\n  mcp.run()\n\nif __name__ == \"__main__\":\n  main()\n</code></pre>"},{"location":"ai-claude-mcp-analytic-server-part2/#code-walkthrough-database-connectivity-and-hybrid-logic","title":"Code Walkthrough: Database Connectivity and Hybrid Logic","text":""},{"location":"ai-claude-mcp-analytic-server-part2/#1-connection-management","title":"1. Connection Management","text":"<ul> <li><code>build_connection_string()</code>: Dynamically builds the ODBC connection string for SQL Server, supporting both Windows and SQL authentication.</li> <li><code>get_db_connection()</code>: Manages connection pooling and reconnects if needed.</li> <li><code>is_database_configured()</code>: Checks if database parameters are set.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#2-data-catalog-tool","title":"2. Data Catalog Tool","text":"<ul> <li><code>get_data_catalog()</code>: Returns the full JSON catalog, giving the LLM all business and technical context for every dataset.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#3-file-and-database-listing","title":"3. File and Database Listing","text":"<ul> <li><code>get_files_list()</code>: Lists available CSV/Parquet files.</li> <li><code>get_database_tables()</code>: Lists available SQL Server tables.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#4-schema-discovery","title":"4. Schema Discovery","text":"<ul> <li><code>get_schema()</code>: Returns column names and types for files.</li> <li><code>get_schema_db()</code>: Returns column names and types for database tables.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#5-query-execution","title":"5. Query Execution","text":"<ul> <li><code>execute_polars_sql()</code>: Executes Polars SQL queries on files (implementation omitted for brevity).</li> <li><code>execute_database_query()</code>: Executes T-SQL queries on the database, returning results as dictionaries.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#how-the-llm-uses-context-to-select-the-right-tool","title":"How the LLM Uses Context to Select the Right Tool","text":"<p>The LLM (Claude Desktop or any compatible client) relies on the data catalog JSON to make intelligent decisions about which tool to use for each user query. Here\u2019s how the process works in detail:</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#1-catalog-driven-context-discovery","title":"1. Catalog-Driven Context Discovery","text":"<ul> <li>The LLM first calls the MCP tool <code>get_data_catalog()</code>, which returns the full contents of <code>data/data_catalog.json</code>.</li> <li>This JSON file contains a dictionary of all datasets, each with metadata fields such as:</li> <li><code>source_type</code>: Indicates if the data is in a file (<code>file</code>) or a database (<code>database</code>).</li> <li><code>filename</code> (for files) or <code>database_schema</code>/<code>database_table</code> (for databases): Specifies the location or table name.</li> <li><code>category</code>, <code>description</code>, <code>columns</code>, <code>business_rules</code>, <code>common_queries</code>, <code>related_datasets</code>, and <code>usage_tip</code>: Provide business and technical context.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#2-context-extraction-and-tool-selection","title":"2. Context Extraction and Tool Selection","text":"<ul> <li>The LLM parses the catalog to identify which dataset(s) are relevant to the user\u2019s question.</li> <li>For each relevant dataset, it checks the <code>source_type</code> field:</li> <li>If <code>source_type</code> is <code>file</code>, the LLM knows to use the <code>execute_polars_sql()</code> tool for querying, and can call <code>get_schema()</code> for column details.</li> <li>If <code>source_type</code> is <code>database</code>, the LLM uses the <code>execute_database_query()</code> tool, and can call <code>get_schema_db()</code> for table schema.</li> <li>The LLM also uses other fields:</li> <li><code>business_rules</code> help the LLM understand how to filter or interpret the data (e.g., only include terminated employees older than 90 days).</li> <li><code>common_queries</code> provide examples that guide the LLM in generating correct SQL or Polars SQL syntax.</li> <li><code>related_datasets</code> inform the LLM about possible joins or multi-source queries.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#3-example-workflow","title":"3. Example Workflow","text":"<p>Suppose a user asks: \"How many accounts are pending deletion?\" - The LLM calls <code>get_data_catalog()</code> and finds the <code>accounts_to_be_deleted</code> dataset. - It sees <code>source_type: file</code> and <code>filename: accounts_to_be_deleted.csv</code>. - It uses <code>execute_polars_sql(file_locations=[\"data/accounts_to_be_deleted.csv\"], query=\"SELECT COUNT(*) FROM self WHERE hr_TerminationDate &lt; '2025-07-01'\")</code>.</p> <p>For a database example, if the user asks: \"How many active employees are in the HR system?\" - The LLM finds the <code>adp_employee_daily_snapshot</code> dataset, sees <code>source_type: database</code>, and extracts <code>database_schema: mcp</code>, <code>database_table: vw_ADP_Employee_Daily_Snapshot</code>. - It uses <code>execute_database_query(query=\"SELECT COUNT(*) FROM mcp.vw_ADP_Employee_Daily_Snapshot WHERE Position_Status = 'Active'\")</code>.</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#4-why-this-matters","title":"4. Why This Matters","text":"<ul> <li>The JSON catalog acts as both a business glossary and a technical map, ensuring the LLM never guesses or misuses a tool.</li> <li>It enforces correct tool selection, query syntax, and business logic, making analytics reliable and secure.</li> <li>The LLM can answer complex questions, join datasets, and respect business rules\u2014all by following the context provided in the catalog.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#security-and-privacy","title":"Security and Privacy","text":"<ul> <li>All data remains on-premises: No data leaves your network.</li> <li>Connection info and authentication are managed securely.</li> <li>Catalog-driven logic prevents accidental data leakage or misuse.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#deployment-and-usage","title":"Deployment and Usage","text":"<ul> <li>Package as a Windows executable using PyInstaller (now includes pyodbc dependency):   <pre><code>pyinstaller --onefile --name cio-mcp_server src/analyst.py\n</code></pre></li> <li>Deploy alongside your data folder and update <code>data_catalog.json</code> to include database tables.</li> <li>Configure Claude Desktop as in Part 1.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#lessons-learned-and-best-practices","title":"Lessons Learned and Best Practices","text":"<ul> <li>Hybrid access enables richer analytics and reporting.</li> <li>Catalog-driven tool selection is critical for reliability and security.</li> <li>Unified schema discovery simplifies query generation for LLMs.</li> <li>Connection info tool helps with troubleshooting and onboarding.</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part2/#conclusion","title":"Conclusion","text":"<p>With hybrid file and database support, your MCP server becomes a true enterprise analytics gateway. Business users can ask questions spanning all data sources, with natural language and instant results. This approach future-proofs your analytics stack and empowers users to self-serve securely.</p>"},{"location":"ai-claude-mcp-analytic-server-part2/#references-and-credits","title":"References and Credits","text":"<ul> <li>mcp-analyst GitHub repository \u2014 Original codebase and inspiration for this solution.</li> <li>Polars DataFrame library \u2014 High-performance data processing in Python.</li> <li>Claude Desktop \u2014 LLM client for natural language analytics.</li> <li>PyInstaller \u2014 For packaging Python code as a Windows executable.</li> <li>pyodbc \u2014 For SQL Server connectivity.</li> </ul> <p>For more details, see the full code in <code>src/analyst.py</code> and the data catalog in <code>data/data_catalog.json</code>.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/","title":"Secure, On-Premises Data Analysis with LLM and a Custom MCP Server","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#part-3-https-based-mcp-server-with-oauth-20-azure-entra-id-and-multi-tenant-architecture","title":"Part 3: HTTPS-Based MCP Server with OAuth 2.0 (Azure Entra ID) and Multi-Tenant Architecture","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: CSV/Parquet files</li> <li>Part 2: CSV/Parquet &amp; Database</li> <li>Part 3: HTTPS-Based MCP Server with OAuth 2.0 (Azure Entra ID) (Current)</li> </ul> <p>This is Part 3 of the series. Here, we extend the solution to use MCP Server over HTTPS</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#introduction","title":"Introduction","text":"<p>From a business end-user perspective, the core problem this solution addresses is enabling anyone in the organization to ask questions of our internal data\u2014whether in files or databases\u2014using natural language, without needing to know SQL or technical query languages. By leveraging a large language model (LLM), users can simply describe what they want to know, and the LLM translates those requests into the correct queries, helping us analyze our data more efficiently and intuitively. This democratizes access to analytics, reduces dependency on technical staff, and accelerates business insights.</p> <p>The technical solution ensures this is done securely and at scale: deploying an MCP server over HTTP(S) with enterprise authentication, so all data remains protected and accessible only to authorized users.</p> <p>While the stdio-based MCP server from Parts 1 and 2 works well for individual users on Windows desktops, enterprise environments demand more:</p> <ul> <li>Centralized deployment: One server for all users, not executables on every PC</li> <li>Secure authentication: Enterprise SSO via Azure Entra ID (formerly Azure AD)</li> <li>Multi-tenant architecture: Support for multiple business functions (IT, HR, Finance) with isolated data access</li> <li>Cloud-native deployment: Containerized for Azure Container Instances or Kubernetes</li> <li>HTTPS connectivity: Compatible with Claude Desktop (paid tier), web clients, and API integrations</li> </ul> <p>This article shows how to transform the local MCP server into a production-ready, enterprise service that: - Authenticates users via OAuth 2.0 (Azure Entra ID) - Routes requests to function-specific data catalogs (IT, HR, FIN) - Runs as a containerized service with TLS offloading - Integrates with Claude Desktop's custom HTTPS connector</p> <p>All data and computation remain on-premises or in your Azure tenant\u2014no third-party cloud services involved.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#architecture-from-stdio-to-https","title":"Architecture: From Stdio to HTTPS","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#previous-architecture-parts-1-2","title":"Previous Architecture (Parts 1 &amp; 2)","text":"<p><pre><code>[Claude Desktop] \u2190\u2192 [stdio] \u2190\u2192 [analyst.exe] \u2190\u2192 [Local Files/Database]\n</code></pre> - Simple, single-user, Windows-only - No authentication - Manual executable deployment per user</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#new-architecture-part-3","title":"New Architecture (Part 3)","text":"<pre><code>[Claude Desktop (Paid)] \u2190\u2192 [HTTPS/OAuth] \u2190\u2192 [Azure Container Instance]\n                                                    \u2193\n                                              [FortiGate TLS]\n                                                    \u2193\n                                            [MCP Server + Auth]\n                                                    \u2193\n                               [Business Function Router (IT/HR/FIN)]\n                                    \u2193              \u2193            \u2193\n                          [IT Data Catalog] [HR Catalog] [FIN Catalog]\n                                    \u2193              \u2193            \u2193\n                            [Files + Database per Function]\n</code></pre> <p>Key improvements: - OAuth 2.0 (Azure Entra ID): Handles authentication and authorization - Multi-tenancy: Single server supports multiple business functions with isolated data - Cloud deployment: Runs as a container in Azure Container Instances - TLS offloading: FortiGate/FortiWeb handles SSL/TLS termination - HTTPS connectivity: Any HTTP client can connect (Claude Desktop, web apps, APIs)</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#why-https-over-stdio","title":"Why HTTPS over Stdio?","text":"Feature Stdio (Parts 1 &amp; 2) HTTPS (Part 3) Deployment Executable per user Single centralized service Authentication None OAuth 2.0 (Azure Entra ID) Client support Claude Desktop (local) Claude Desktop (paid), web, mobile, APIs Multi-user No Yes, with per-user context Scalability One user per process Hundreds of concurrent users Security Local file access Enterprise SSO + role-based access Updates Redeploy to every PC Deploy once, all users benefit"},{"location":"ai-claude-mcp-analytic-server-part3/#azure-entra-app-registration-setup-and-gotchas","title":"Azure Entra App Registration: Setup and Gotchas","text":"<p>Before deploying the MCP server, you must configure Azure Entra ID (formerly Azure AD) for OAuth 2.0 authentication.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-1-create-app-registration","title":"Step 1: Create App Registration","text":"<ol> <li>Navigate to Azure Portal \u2192 Entra ID \u2192 App Registrations \u2192 New Registration</li> <li>Configure basic settings:</li> <li>Name: <code>MCP-Analyst-Server</code></li> <li>Supported account types: Accounts in this organizational directory only (single tenant)</li> <li>Redirect URI: Leave blank for now (we'll add it next)</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-2-configure-api-permissions","title":"Step 2: Configure API Permissions","text":"<ol> <li>Go to API Permissions \u2192 Add a permission \u2192 Microsoft Graph</li> <li>Add Delegated permissions:</li> <li><code>User.Read</code> (to read user profile)</li> <li><code>email</code>, <code>profile</code>, <code>openid</code> (standard OAuth scopes)</li> <li>Grant admin consent for your organization</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-3-create-client-secret","title":"Step 3: Create Client Secret","text":"<ol> <li>Go to Certificates &amp; secrets \u2192 New client secret</li> <li>Description: <code>MCP Server Secret</code></li> <li>Expires: 24 months (or per your security policy)</li> <li>Copy the secret value immediately (it won't be shown again)</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-4-configure-redirect-uris","title":"Step 4: Configure Redirect URIs","text":"<p>This is critical for Claude Desktop integration:</p> <ol> <li>Go to Authentication \u2192 Add a platform \u2192 Web</li> <li>Add the following Redirect URIs: <pre><code>https://mcp.gdenergyproducts.com/oauth/callback\nhttps://claude.ai/api/mcp/auth_callback\n</code></pre></li> <li>First URI: Your MCP server's OAuth callback endpoint</li> <li> <p>Second URI: Required for Claude Desktop (paid tier) to act as OAuth proxy</p> <ul> <li>Claude Desktop intercepts the OAuth flow and forwards tokens to your MCP server</li> <li>Without this, Claude Desktop cannot complete authentication</li> </ul> </li> <li> <p>Enable ID tokens and Access tokens checkboxes</p> </li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-5-expose-api-and-define-scopes","title":"Step 5: Expose API and Define Scopes","text":"<ol> <li>Go to Expose an API \u2192 Add a scope</li> <li>Scope name: <code>access_as_user</code></li> <li>Who can consent: Admins and users</li> <li>Admin consent display name: Access MCP Analyst as user</li> <li>Description: Allows the application to access MCP Analyst on behalf of the signed-in user</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#gotcha-1-oauth-version-must-be-20","title":"\ud83d\udea8 Gotcha #1: OAuth Version Must Be 2.0","text":"<p>By default, Azure Entra creates apps with OAuth version 1.0, but MCP servers require version 2.0 for proper token handling.</p> <p>Manual fix required:</p> <ol> <li>Go to App Registration \u2192 Manifest</li> <li>Find the line: <code>\"accessTokenAcceptedVersion\": null</code> or <code>\"accessTokenAcceptedVersion\": 1</code></li> <li>Change it to: <code>\"accessTokenAcceptedVersion\": 2</code></li> <li>Save the manifest</li> </ol> <p>Why this matters: - OAuth 1.0 tokens use different claims and formats - MCP's <code>AzureProvider</code> expects v2 tokens with <code>preferred_username</code>, <code>oid</code>, and <code>name</code> claims - Without this change, authentication will fail with cryptic token validation errors</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#gotcha-2-claude-desktop-callback-url","title":"\ud83d\udea8 Gotcha #2: Claude Desktop Callback URL","text":"<p>When testing with Claude Desktop (paid tier), you must add this specific callback URL:</p> <pre><code>https://claude.ai/api/mcp/auth_callback\n</code></pre> <p>Why this is needed: - Claude Desktop acts as an OAuth proxy for custom MCP servers - When a user clicks \"Connect\" in Claude Desktop, it:   1. Opens the Azure Entra login page in the browser   2. User authenticates   3. Azure redirects to <code>https://claude.ai/api/mcp/auth_callback</code> with auth code   4. Claude Desktop exchanges the code for tokens   5. Claude Desktop forwards tokens to your MCP server - Without this callback URL, the OAuth flow breaks at step 3</p> <p>Security note: - Claude Desktop only proxies the OAuth flow; it does not store or access your data - All MCP queries and data remain between Claude Desktop \u2192 Your MCP Server \u2192 Your Data</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#step-6-note-your-configuration","title":"Step 6: Note Your Configuration","text":"<p>Save these values for later use (will be stored in Azure Key Vault):</p> <pre><code>Application (client) ID: &lt;your-client-id&gt;\nClient Secret: &lt;your-secret-value&gt;\nDirectory (tenant) ID: &lt;your-tenant-id&gt;\n</code></pre>"},{"location":"ai-claude-mcp-analytic-server-part3/#multi-tenant-architecture-business-function-routing","title":"Multi-Tenant Architecture: Business Function Routing","text":"<p>One of the key innovations in this implementation is business function isolation. Instead of one data catalog for everyone, the server dynamically routes users to function-specific catalogs based on URL parameters.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#business-functions","title":"Business Functions","text":"<p>This implementation supports three business functions:</p> <ol> <li>IT (<code>it</code>): Infrastructure, device inventory, license management, Active Directory</li> <li>HR (<code>hr</code>): Employee data, terminations, organizational charts, ADP snapshots</li> <li>Finance (<code>fin</code>): Budget, expenses, procurement, SAP exports</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#connection-url-examples","title":"Connection URL Examples","text":"<ul> <li>IT users: <code>https://mcp.gdenergyproducts.com/mcp?function=it</code></li> <li>HR users: <code>https://mcp.gdenergyproducts.com/mcp?function=hr</code></li> <li>Finance users: <code>https://mcp.gdenergyproducts.com/mcp?function=fin</code></li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part3/#data-isolation-structure","title":"Data Isolation Structure","text":"<pre><code>/mnt/azure/mcp/\n\u251c\u2500\u2500 it/\n\u2502   \u251c\u2500\u2500 data_catalog.json\n\u2502   \u251c\u2500\u2500 device_inventory.csv\n\u2502   \u251c\u2500\u2500 licenses.csv\n\u2502   \u2514\u2500\u2500 accounts_to_be_deleted.csv\n\u251c\u2500\u2500 hr/\n\u2502   \u251c\u2500\u2500 data_catalog.json\n\u2502   \u251c\u2500\u2500 idp_hr.csv\n\u2502   \u2514\u2500\u2500 terminations.csv\n\u2514\u2500\u2500 fin/\n    \u251c\u2500\u2500 data_catalog.json\n    \u251c\u2500\u2500 budget.csv\n    \u2514\u2500\u2500 sap_expenses.csv\n</code></pre> <p>Each function has: - Its own <code>data_catalog.json</code> with function-specific datasets - Isolated file storage (IT users can't access HR files) - Separate database views (e.g., <code>mcp.it_employees</code>, <code>mcp.hr_employees</code>)</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#full-code-walkthrough-analystpy-https-version","title":"Full Code Walkthrough: <code>analyst.py</code> (HTTPS Version)","text":"<p>Below is a comprehensive walkthrough of the production code, covering 85%+ of the implementation.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#1-logging-and-initialization","title":"1. Logging and Initialization","text":"<pre><code>import os\nimport logging\nimport warnings\nfrom pathlib import Path\nfrom logging.handlers import RotatingFileHandler\n\nLOG_SEPARATOR = \"=\" * 80\n\ndef setup_logger(name, log_file, level=logging.DEBUG):\n    \"\"\"Configure structured logging with rotation\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(levelname)s - %(name)s - \"\n        \"%(funcName)s:%(lineno)d - %(message)s\"\n    )\n\n    if not logger.handlers:\n        # File handler with 10MB rotation, keep 5 backups\n        handler = RotatingFileHandler(\n            log_file, \n            maxBytes=10*1024*1024, \n            backupCount=5\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n        # Console handler for development\n        console_handler = logging.StreamHandler()\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n\n    return logger\n\nlogger = setup_logger(\"gdepmcp\", \"/mnt/azure/logs/gdepmcp.log\")\nlogger.info(LOG_SEPARATOR)\nlogger.info(\"GDEP MCP Server Starting\")\nlogger.info(LOG_SEPARATOR)\n\n# Suppress deprecation warnings in production\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n</code></pre> <p>Key points: - Structured logging: Every request, tool call, and error is logged with context - Log rotation: Prevents disk fill-up in production (10MB per file, 5 backups) - Dual output: File for auditing, console for development/debugging - Function/line tracking: Easy debugging with function name and line number in logs</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#2-azure-key-vault-integration","title":"2. Azure Key Vault Integration","text":"<pre><code>from azure.identity import ClientSecretCredential\nfrom azure.keyvault.secrets import SecretClient\n\ndef get_azure_kv_secret(name):\n    \"\"\"Retrieve secrets from Azure Key Vault\"\"\"\n    try:\n        vault_url = \"https://kv-gdep-peus-interfaces.vault.azure.net/\"\n\n        # Service principal credentials from environment variables\n        client_id = os.environ.get(\"application_interface_clientid\")\n        client_secret = os.environ.get(\"application_interface_clientsecret\")\n        tenant_id = os.environ.get(\"application_interface_tenantid\")\n\n        credential = ClientSecretCredential(\n            client_id=client_id,\n            client_secret=client_secret,\n            tenant_id=tenant_id\n        )\n\n        secret_client = SecretClient(\n            vault_url=vault_url, \n            credential=credential\n        )\n        return secret_client.get_secret(name).value\n    except Exception as exc:\n        logger.exception(f\"Error retrieving Azure KV secret '{name}': {exc}\")\n        return None\n\n# Load OAuth credentials from Key Vault\nAZURE_CONFIDENTIAL_APP_ID = str(\n    get_azure_kv_secret('mcp-authentication-clientid')\n)\nAZURE_CONFIDENTIAL_SECRET = str(\n    get_azure_kv_secret('mcp-authentication-clientsecret')\n)\n</code></pre> <p>Why Key Vault? - Never hardcode secrets: Client IDs and secrets are injected at runtime - Centralized management: Rotate secrets without redeploying code - Audit trail: Key Vault logs all secret access - Role-based access: Only authorized apps/users can read secrets</p> <p>Required environment variables (set in container): <pre><code>application_interface_clientid=&lt;service-principal-client-id&gt;\napplication_interface_clientsecret=&lt;service-principal-secret&gt;\napplication_interface_tenantid=&lt;azure-tenant-id&gt;\n</code></pre></p>"},{"location":"ai-claude-mcp-analytic-server-part3/#3-business-function-middleware","title":"3. Business Function Middleware","text":"<pre><code>from starlette.requests import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom contextvars import ContextVar\n\ncurrent_function = ContextVar('current_function', default='it')\n\nclass BusinessFunctionExtractionMiddleware(BaseHTTPMiddleware):\n    \"\"\"Extract business function from query string and store in context\"\"\"\n\n    async def dispatch(self, request: Request, call_next):\n        host = request.headers.get(\"host\", \"\")\n        business_function = \"na\"\n\n        logger.info(\n            f\"Incoming request - Host: {host}, \"\n            f\"Path: {request.url.path}, Method: {request.method}\"\n        )\n\n        # Extract function from query string\n        function_via_query_string = request.query_params.get(\"function\")\n\n        if function_via_query_string in {\"it\", \"hr\", \"fin\"}:\n            business_function = function_via_query_string\n            logger.info(\n                f\"DEBUG MODE: Using business_function from query \"\n                f\"parameter: {business_function}\"\n            )\n\n        # Store in request context (async-safe via ContextVar)\n        request.state.business_function = business_function\n        current_function.set(business_function)\n\n        logger.debug(\n            f\"Set request.state.business_function = {business_function}\"\n        )\n\n        response = await call_next(request)\n\n        logger.info(\n            f\"Response status: {response.status_code} \"\n            f\"for business_function: {business_function}\"\n        )\n\n        return response\n\ndef get_current_business_function() -&gt; str:\n    \"\"\"Get business function from context variable (thread-safe)\"\"\"\n    function = current_function.get()\n    logger.debug(\n        f\"Retrieved current business function from context: {function}\"\n    )\n    return function\n</code></pre> <p>How this works: 1. Request arrives at <code>/mcp?function=it</code> 2. Middleware intercepts before MCP tools execute 3. Extracts <code>function</code> parameter from query string 4. Validates against allowed functions (<code>it</code>, <code>hr</code>, <code>fin</code>) 5. Stores in context using Python's <code>ContextVar</code> (async-safe) 6. All subsequent tool calls read from context to route to correct data</p> <p>Security: - Invalid functions (<code>na</code>) result in \"no data found\" responses - No cross-contamination between business functions - Each user's requests are isolated in async context - Supports concurrent requests from multiple users</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#4-oauth-20-azure-entra-id-configuration","title":"4. OAuth 2.0 (Azure Entra ID) Configuration","text":"<pre><code>from fastmcp import FastMCP\nfrom fastmcp.server.auth.providers.azure import AzureProvider\n\nlogger.info(\"Configuring OAuth 2.0 (Azure Entra ID) Provider\")\ntry:\n    azure_auth = AzureProvider(\n        client_id=AZURE_CONFIDENTIAL_APP_ID,\n        client_secret=AZURE_CONFIDENTIAL_SECRET,\n        tenant_id=\"00b1a755-0b06-4d05-9a59-259ebf7f9e00\",\n        base_url=\"https://mcp.gdenergyproducts.com\",\n        required_scopes=[\"access_as_user\"]\n    )\n    logger.info(\"OAuth 2.0 (Azure Entra ID) Provider configured successfully\")\nexcept Exception as e:\n    logger.error(\n    f\"Failed to configure OAuth 2.0 (Azure Entra ID) Provider: {str(e)}\", \n        exc_info=True\n    )\n    raise\n\n# Initialize FastMCP with authentication\nmcp = FastMCP(\"gdep-analyst\", auth=azure_auth)\nlogger.info(\"FastMCP instance initialized successfully\")\n</code></pre> <p>Configuration breakdown: - <code>client_id</code>: Your Azure App Registration's Application ID - <code>client_secret</code>: Secret created in App Registration - <code>tenant_id</code>: Your organization's Azure tenant ID - <code>base_url</code>: Public URL of your MCP server (for OAuth callbacks) - <code>required_scopes</code>: API scopes defined in \"Expose an API\" section</p> <p>What happens during OAuth: 1. User connects in Claude Desktop \u2192 redirected to Azure login 2. User authenticates with corporate credentials 3. Azure validates and issues tokens with <code>access_as_user</code> scope 4. MCP server validates token signature and claims 5. Request proceeds with user context (email, name, object ID)</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#5-database-connection-management","title":"5. Database Connection Management","text":"<pre><code>import pyodbc\n\n_connection = None\n_connection_string = None\n\ndef build_connection_string() -&gt; str:\n    \"\"\"Build SQL Server connection string from Key Vault\"\"\"\n    logger.debug(\"Building database connection string\")\n    conn_str = str(get_azure_kv_secret('mcp-sql-conn-string'))\n    logger.info(\"Database connection string built successfully\")\n    return conn_str\n\ndef get_db_connection():\n    \"\"\"Get or create database connection with connection pooling\"\"\"\n    global _connection, _connection_string\n\n    logger.debug(\"Getting database connection\")\n\n    # Lazy initialization\n    if _connection_string is None:\n        logger.info(\"Connection string not initialized, building new one\")\n        _connection_string = build_connection_string()\n\n    if _connection_string is None:\n        logger.error(\"Failed to build connection string\")\n        return None\n\n    # Test existing connection\n    try:\n        if _connection is not None:\n            logger.debug(\"Testing existing connection\")\n            _connection.cursor().execute(\"SELECT 1\")\n            logger.debug(\"Existing connection is valid\")\n    except Exception as e:\n        logger.warning(\n            f\"Existing connection test failed: {str(e)}, \"\n            \"will create new connection\"\n        )\n        _connection = None\n\n    # Create new connection if needed\n    if _connection is None:\n        logger.info(\"Creating new database connection\")\n        try:\n            _connection = pyodbc.connect(_connection_string)\n            logger.info(\"Database connection established successfully\")\n        except Exception as e:\n            logger.error(\n                f\"Failed to establish database connection: {str(e)}\",\n                exc_info=True\n            )\n            raise\n\n    return _connection\n\ndef is_database_configured() -&gt; bool:\n    \"\"\"Check if database connection is configured\"\"\"\n    logger.debug(\"Checking if database is configured\")\n    return True  # Always true in this deployment\n</code></pre> <p>Connection pooling strategy: - Singleton pattern: Reuse connections across requests - Health checks: Test connection before use (<code>SELECT 1</code>) - Automatic reconnection: If connection dies, create new one - Secure credentials: Connection string stored in Key Vault</p> <p>Example connection string (from Key Vault): <pre><code>Driver={ODBC Driver 18 for SQL Server};\nServer=10.27.18.5,61002;\nDatabase=ADP_AD_AAD;\nUID=mcp_reader;\nPWD=&lt;from-key-vault&gt;;\nEncrypt=yes;\nTrustServerCertificate=yes;\n</code></pre></p>"},{"location":"ai-claude-mcp-analytic-server-part3/#6-file-reading-with-sap-support","title":"6. File Reading with SAP Support","text":"<pre><code>import polars as pl\nfrom pathlib import Path\n\ndef read_file(file_location: str, file_type: str = \"csv\") -&gt; pl.DataFrame:\n    \"\"\"Read CSV or Parquet file into Polars DataFrame\"\"\"\n    logger.info(f\"Reading file: {file_location}, type: {file_type}\")\n\n    try:\n        if file_type == \"csv\":\n            filename = Path(file_location).name.lower()\n\n            # SAP files use semicolon delimiter\n            separator = ';' if filename.startswith('sap_') else ','\n            logger.debug(\n                f\"Using separator '{separator}' for file: {filename}\"\n            )\n\n            df = pl.read_csv(\n                file_location,\n                separator=separator,\n                truncate_ragged_lines=True,  # Handle malformed rows\n                infer_schema_length=10000,   # Sample 10k rows for types\n                ignore_errors=True           # Skip unparseable rows\n            )\n            logger.info(\n                f\"Successfully read CSV file: {file_location}, \"\n                f\"shape: {df.shape}\"\n            )\n            return df\n\n        if file_type == \"parquet\":\n            df = pl.read_parquet(file_location)\n            logger.info(\n                f\"Successfully read Parquet file: {file_location}, \"\n                f\"shape: {df.shape}\"\n            )\n            return df\n\n        logger.error(f\"Unsupported file type: {file_type}\")\n        raise ValueError(f\"Unsupported file type: {file_type}\")\n\n    except Exception as e:\n        logger.exception(f\"Failed to read file {file_location}\")\n        raise\n\ndef read_file_list(\n    file_locations: List[str], \n    file_type: str = \"csv\"\n) -&gt; pl.DataFrame:\n    \"\"\"Read multiple files with same schema and concatenate\"\"\"\n    logger.info(\n        f\"Reading file list: {len(file_locations)} files of type {file_type}\"\n    )\n    logger.debug(f\"Files to read: {file_locations}\")\n\n    try:\n        if file_type == \"csv\":\n            dfs = []\n            for i, file_location in enumerate(file_locations):\n                logger.debug(\n                    f\"Reading file {i+1}/{len(file_locations)}: \"\n                    f\"{file_location}\"\n                )\n                filename = Path(file_location).name.lower()\n                separator = ';' if filename.startswith('sap_') else ','\n\n                df = pl.read_csv(\n                    file_location,\n                    separator=separator,\n                    truncate_ragged_lines=True,\n                    infer_schema_length=10000,\n                    ignore_errors=True\n                )\n                dfs.append(df)\n                logger.debug(f\"File {i+1} shape: {df.shape}\")\n\n            result = pl.concat(dfs)\n            logger.info(\n                f\"Successfully concatenated {len(dfs)} CSV files, \"\n                f\"final shape: {result.shape}\"\n            )\n            return result\n\n        if file_type == \"parquet\":\n            result = pl.read_parquet(file_locations)\n            logger.info(\n                f\"Successfully read {len(file_locations)} Parquet files, \"\n                f\"shape: {result.shape}\"\n            )\n            return result\n\n        logger.error(f\"Unsupported file type: {file_type}\")\n        raise ValueError(f\"Unsupported file type: {file_type}\")\n\n    except Exception as e:\n        logger.exception(\"Failed to read file list\")\n        raise\n</code></pre> <p>Key features: - SAP detection: Files prefixed with <code>sap_</code> automatically use <code>;</code> delimiter - Robust parsing: Handles malformed CSVs gracefully - Type inference: Samples 10,000 rows to detect column types accurately - Performance: Polars is 10-50x faster than Pandas on large files - Multi-file support: Concatenates files with same schema efficiently</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#7-user-context-extraction","title":"7. User Context Extraction","text":"<pre><code>from fastmcp.server.dependencies import get_access_token\n\ndef get_current_user_info() -&gt; dict:\n    \"\"\"Returns information about the authenticated Azure user\"\"\"\n    token = get_access_token()\n\n    return {\n        \"email\": token.claims.get(\"preferred_username\"),\n        \"name\": token.claims.get(\"name\"),\n        \"oid\": token.claims.get(\"oid\")  # Azure object ID\n    }\n</code></pre> <p>Available claims from Azure token: - <code>preferred_username</code>: User's email (e.g., <code>john.doe@company.com</code>) - <code>name</code>: Display name (e.g., <code>John Doe</code>) - <code>oid</code>: Unique Azure object ID (use for database joins/auditing)</p> <p>Usage example in tools: <pre><code>@mcp.tool()\ndef sensitive_tool():\n    user_info = get_current_user_info()\n    logger.info(\n        f\"Tool called by: {user_info['name']} ({user_info['email']})\"\n    )\n\n    # Implement per-user authorization logic\n    if user_info['email'] not in AUTHORIZED_USERS:\n        return {\"error\": \"Unauthorized\"}\n\n    # ... rest of tool logic\n</code></pre></p>"},{"location":"ai-claude-mcp-analytic-server-part3/#8-mcp-tool-connection-info","title":"8. MCP Tool: Connection Info","text":"<pre><code>from typing import Dict, Any\nfrom glob import glob\n\n@mcp.tool()\ndef get_connection_info() -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about configured data sources and connection status.\n    Use to verify available data sources and check connection health.\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"get_connection_info called for \"\n        f\"business_function: {business_function}\"\n    )\n\n    file_pattern = f\"/mnt/azure/mcp/{business_function}/*.csv\"\n    logger.debug(f\"File pattern: {file_pattern}\")\n\n    available_files = glob(file_pattern)\n    logger.info(f\"Found {len(available_files)} files matching pattern\")\n\n    info = {\n        \"business_function\": business_function,\n        \"file_sources\": {\n            \"enabled\": True,\n            \"pattern\": file_pattern,\n            \"available_files\": available_files\n        },\n        \"database_sources\": {\n            \"enabled\": is_database_configured(),\n            \"server\": \"10.27.18.5,61002\",\n            \"database\": \"ADP_AD_AAD\"\n        }\n    }\n\n    # Test database connection and list tables\n    if is_database_configured():\n        logger.info(\"Database is configured, retrieving available objects\")\n        try:\n            conn = get_db_connection()\n            cursor = conn.cursor()\n\n            # List tables for this business function\n            db_objects_query = f\"\"\"\n                SELECT TABLE_SCHEMA, TABLE_NAME \n                FROM INFORMATION_SCHEMA.TABLES \n                WHERE TABLE_TYPE IN ('BASE TABLE', 'VIEW')\n                AND TABLE_SCHEMA = 'mcp' \n                AND TABLE_NAME LIKE '{business_function}_%'\n                ORDER BY TABLE_SCHEMA, TABLE_NAME\n            \"\"\"\n            cursor.execute(db_objects_query)\n            db_objects = cursor.fetchall()\n\n            objects_list = [\n                {\"schema\": row[0], \"table\": row[1]} \n                for row in db_objects\n            ]\n\n            info[\"database_sources\"][\"status\"] = \"connected\"\n            info[\"database_sources\"][\"available_objects\"] = objects_list\n            logger.info(\n                f\"Database connection successful, \"\n                f\"found {len(objects_list)} objects\"\n            )\n        except Exception as e:\n            info[\"database_sources\"][\"status\"] = \"error\"\n            info[\"database_sources\"][\"error\"] = str(e)\n            logger.error(\n                f\"Database object listing failed: {str(e)}\", \n                exc_info=True\n            )\n    else:\n        logger.info(\"Database not configured\")\n\n    # Log user for auditing\n    try:\n        user_info = get_current_user_info()\n        logger.info(\n            f\"Tool get_connection_info was called by: \"\n            f\"{user_info['name']} with email {user_info['email']}\"\n        )\n    except Exception as e:\n        logger.warning(f\"Could not retrieve user info: {str(e)}\")\n\n    logger.debug(f\"Returning connection info: {json.dumps(info, indent=2)}\")\n    return info\n</code></pre> <p>Returns example: <pre><code>{\n  \"business_function\": \"it\",\n  \"file_sources\": {\n    \"enabled\": true,\n    \"pattern\": \"/mnt/azure/mcp/it/*.csv\",\n    \"available_files\": [\n      \"/mnt/azure/mcp/it/device_inventory.csv\",\n      \"/mnt/azure/mcp/it/licenses.csv\"\n    ]\n  },\n  \"database_sources\": {\n    \"enabled\": true,\n    \"server\": \"10.27.18.5,61002\",\n    \"database\": \"ADP_AD_AAD\",\n    \"status\": \"connected\",\n    \"available_objects\": [\n      {\"schema\": \"mcp\", \"table\": \"it_employees\"},\n      {\"schema\": \"mcp\", \"table\": \"it_devices\"}\n    ]\n  }\n}\n</code></pre></p>"},{"location":"ai-claude-mcp-analytic-server-part3/#9-mcp-tool-data-catalog-auto-discovery","title":"9. MCP Tool: Data Catalog (Auto-Discovery)","text":"<pre><code>import json\nimport os\n\n@mcp.tool()\ndef get_data_catalog() -&gt; str:\n    \"\"\"\n    Get the complete data catalog showing all available datasets.\n\n    **ALWAYS CALL THIS TOOL FIRST before answering any data question.**\n\n    Returns JSON catalog with:\n    - \"source_type\": \"file\" or \"database\"\n    - \"query_tool\": execute_polars_sql or execute_database_query\n    - \"table_reference\": 'self' for files, 'schema.table' for database\n    - \"example_query\": Sample query to adapt\n    - Business descriptions, columns, rules, relationships\n\n    The catalog contains all business context needed to write correct queries.\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"get_data_catalog called for business_function: {business_function}\"\n    )\n\n    catalog_path = f\"/mnt/azure/mcp/{business_function}/data_catalog.json\"\n    logger.debug(f\"Looking for catalog file: {catalog_path}\")\n\n    # Load catalog file if exists\n    catalog_json = {}\n    if os.path.exists(catalog_path):\n        try:\n            with open(catalog_path, 'r', encoding='utf-8') as f:\n                catalog_json = json.load(f)\n            logger.info(f\"Loaded catalog file: {catalog_path}\")\n        except Exception as e:\n            logger.error(\n                f\"Failed to load catalog file: {str(e)}\", \n                exc_info=True\n            )\n            catalog_json = {}\n\n    # Build catalog with metadata from JSON + auto-discovery\n    catalog = {\n        \"version\": catalog_json.get(\"version\", \"2.0\"),\n        \"last_updated\": catalog_json.get(\"last_updated\", \"auto-generated\"),\n        \"README\": catalog_json.get(\"README\", {}),\n        \"query_decision_tree\": catalog_json.get(\"query_decision_tree\", {}),\n        \"anti_patterns\": catalog_json.get(\"anti_patterns\", {}),\n        \"categories\": catalog_json.get(\"categories\", {}),\n        \"datasets\": {}\n    }\n\n    # Auto-discover files on disk\n    files = (\n        glob(f\"/mnt/azure/mcp/{business_function}/*.csv\") + \n        glob(f\"/mnt/azure/mcp/{business_function}/*.parquet\")\n    )\n    logger.info(\n        f\"Found {len(files)} files on disk for \"\n        f\"business_function {business_function}\"\n    )\n\n    file_keys_on_disk = set()\n    for file in files:\n        basename = os.path.basename(file)\n        key = basename.replace('.csv', '').replace('.parquet', '').lower()\n        file_keys_on_disk.add(key)\n\n        # Use metadata from JSON if available, else minimal auto-discovery\n        meta = catalog_json.get(\"datasets\", {}).get(key, {})\n\n        dataset = {\n            \"source_type\": \"file\",\n            \"query_tool\": meta.get(\"query_tool\", \"execute_polars_sql\"),\n            \"table_reference\": meta.get(\"table_reference\", \"self\"),\n            \"filename\": basename,\n            \"category\": meta.get(\"category\"),\n            \"description\": meta.get(\n                \"description\", \n                \"Auto-discovered file - no metadata available\"\n            ),\n            \"source_system\": meta.get(\"source_system\"),\n            \"delimiter\": meta.get(\"delimiter\", \",\"),\n            \"columns\": meta.get(\"columns\", {}),\n            \"business_rules\": meta.get(\"business_rules\", []),\n            \"common_queries\": meta.get(\"common_queries\", []),\n            \"related_datasets\": meta.get(\"related_datasets\", []),\n            \"usage_tip\": meta.get(\n                \"usage_tip\", \n                f\"Use get_schema(file_location='{file}') for detailed schema\"\n            )\n        }\n\n        # Add any extra fields from JSON\n        for extra_field in meta:\n            if extra_field not in dataset:\n                dataset[extra_field] = meta[extra_field]\n\n        catalog[\"datasets\"][key] = dataset\n        logger.debug(f\"Added/updated file dataset: {key}\")\n\n    # Auto-discover database tables\n    db_keys = set()\n    if is_database_configured():\n        try:\n            conn = get_db_connection()\n            cursor = conn.cursor()\n\n            cursor.execute(f\"\"\"\n                SELECT TABLE_SCHEMA, TABLE_NAME \n                FROM INFORMATION_SCHEMA.TABLES \n                WHERE TABLE_TYPE IN ('BASE TABLE', 'VIEW')\n                AND TABLE_SCHEMA = 'mcp' \n                AND TABLE_NAME LIKE '{business_function}_%'\n                ORDER BY TABLE_SCHEMA, TABLE_NAME\n            \"\"\")\n            tables = cursor.fetchall()\n\n            logger.info(\n                f\"Found {len(tables)} database tables for \"\n                f\"business_function {business_function}\"\n            )\n\n            for schema, table in tables:\n                key = f\"{schema}_{table}\".lower()\n                db_keys.add(key)\n\n                meta = catalog_json.get(\"datasets\", {}).get(key, {})\n\n                dataset = {\n                    \"source_type\": \"database\",\n                    \"query_tool\": meta.get(\n                        \"query_tool\", \n                        \"execute_database_query\"\n                    ),\n                    \"table_reference\": meta.get(\n                        \"table_reference\", \n                        f\"{schema}.{table}\"\n                    ),\n                    \"database_schema\": schema,\n                    \"database_table\": table,\n                    \"category\": meta.get(\"category\"),\n                    \"description\": meta.get(\n                        \"description\", \n                        \"Auto-discovered database table - no metadata available\"\n                    ),\n                    \"source_system\": meta.get(\"source_system\"),\n                    \"update_frequency\": meta.get(\"update_frequency\"),\n                    \"columns\": meta.get(\"columns\", {}),\n                    \"business_rules\": meta.get(\"business_rules\", []),\n                    \"common_queries\": meta.get(\"common_queries\", []),\n                    \"related_datasets\": meta.get(\"related_datasets\", []),\n                    \"example_query\": meta.get(\"example_query\"),\n                    \"semantic_examples\": meta.get(\"semantic_examples\", {}),\n                    \"usage_tip\": meta.get(\n                        \"usage_tip\", \n                        f\"Use get_schema_db(schema_name='{schema}', \"\n                        f\"table_name='{table}') for detailed schema\"\n                    )\n                }\n\n                for extra_field in meta:\n                    if extra_field not in dataset:\n                        dataset[extra_field] = meta[extra_field]\n\n                catalog[\"datasets\"][key] = dataset\n                logger.debug(f\"Added/updated database dataset: {key}\")\n\n        except Exception as e:\n            error_msg = f\"Database table discovery failed: {str(e)}\"\n            catalog[\"database_discovery_error\"] = error_msg\n            logger.error(error_msg, exc_info=True)\n\n    # Only include datasets that are actually present (on disk or in db)\n    logger.info(\n        f\"Reconciling catalog: {len(catalog['datasets'])} \"\n        \"total before filtering\"\n    )\n    present_keys = file_keys_on_disk | db_keys\n    catalog[\"datasets\"] = {\n        k: v for k, v in catalog[\"datasets\"].items() \n        if k in present_keys\n    }\n    logger.info(\n        f\"Final catalog has {len(catalog['datasets'])} datasets \"\n        \"after reconciliation\"\n    )\n\n    return json.dumps(catalog, indent=2)\n</code></pre> <p>Key features: - Hybrid auto-discovery: Combines JSON metadata with filesystem/database introspection - Business function isolation: Only shows datasets for current function - Graceful degradation: Works even without <code>data_catalog.json</code> file - Rich metadata: Provides all context LLM needs to write correct queries - Tool routing: Tells LLM exactly which execution tool to use</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#10-mcp-tool-get-schema-files","title":"10. MCP Tool: Get Schema (Files)","text":"<pre><code>from typing import List\n\n@mcp.tool()\ndef get_schema(\n    file_location: str,\n    file_type: str = Field(default=\"csv\", description=\"csv or parquet\"),\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get technical schema of a CSV or Parquet file.\n\n    **Use for file-based datasets only.** \n    For database tables, use get_schema_db().\n\n    Returns column names and Polars data types.\n    Call after get_data_catalog() to get detailed column information.\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"get_schema called for file: {file_location}, \"\n        f\"type: {file_type}, business_function: {business_function}\"\n    )\n\n    # Enforce that file_location is within the correct business function\n    allowed_prefix = f\"/mnt/azure/mcp/{business_function}/\"\n    if not os.path.abspath(file_location).startswith(\n        os.path.abspath(allowed_prefix)\n    ):\n        error_msg = (\n            f\"Access denied: file_location '{file_location}' is not \"\n            f\"within the allowed business_function folder '{allowed_prefix}'. \"\n            \"You may only request schema for files in your assigned \"\n            \"business_function folder.\"\n        )\n        logger.error(error_msg)\n        return [{\"error\": error_msg}]\n\n    try:\n        df = read_file(file_location, file_type)\n        schema = df.schema\n        schema_dict = {}\n        for key, value in schema.items():\n            schema_dict[key] = str(value)\n\n        logger.info(\n            f\"Successfully retrieved schema with {len(schema_dict)} columns\"\n        )\n        logger.debug(f\"Schema: {schema_dict}\")\n        return [schema_dict]\n    except Exception as e:\n        error_msg = f\"Failed to read schema: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return [{\"error\": error_msg}]\n</code></pre> <p>Security enforcement: - Path validation: Prevents directory traversal attacks - Business function isolation: IT users can't read HR files - Explicit error messages: Clear feedback on authorization failures</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#11-mcp-tool-get-schema-database","title":"11. MCP Tool: Get Schema (Database)","text":"<pre><code>@mcp.tool()\ndef get_schema_db(\n    schema_name: str = Field(\n        description=\"Database schema name (e.g., 'mcp', 'dbo')\"\n    ),\n    table_name: str = Field(description=\"Database table name\"),\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get technical schema of a SQL Server database table.\n\n    **Use for database tables only.** \n    For CSV/Parquet files, use get_schema().\n\n    Returns column names, data types, nullability, and primary key info.\n    Call after get_data_catalog() to get detailed column information.\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"get_schema_db called for {schema_name}.{table_name}, \"\n        f\"business_function: {business_function}\"\n    )\n\n    if not is_database_configured():\n        logger.error(\"Database not configured\")\n        return [{\"error\": \"No database connection configured.\"}]\n\n    # Enforce table_name starts with business_function_\n    if not table_name.startswith(f\"{business_function}_\"):\n        error_msg = (\n            f\"Table name {table_name} does not match \"\n            f\"business_function {business_function}\"\n        )\n        logger.error(error_msg)\n        return [{\"error\": error_msg}]\n\n    try:\n        conn = get_db_connection()\n        cursor = conn.cursor()\n\n        query = \"\"\"\n            SELECT \n                c.COLUMN_NAME, \n                c.DATA_TYPE,\n                c.CHARACTER_MAXIMUM_LENGTH,\n                c.NUMERIC_PRECISION,\n                c.NUMERIC_SCALE,\n                c.IS_NULLABLE,\n                c.COLUMN_DEFAULT,\n                CASE WHEN pk.COLUMN_NAME IS NOT NULL \n                     THEN 'YES' ELSE 'NO' END as IS_PRIMARY_KEY\n            FROM INFORMATION_SCHEMA.COLUMNS c\n            LEFT JOIN (\n                SELECT ku.TABLE_SCHEMA, ku.TABLE_NAME, ku.COLUMN_NAME\n                FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc\n                JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE ku \n                    ON tc.CONSTRAINT_NAME = ku.CONSTRAINT_NAME\n                WHERE tc.CONSTRAINT_TYPE = 'PRIMARY KEY'\n            ) pk ON c.TABLE_SCHEMA = pk.TABLE_SCHEMA \n                AND c.TABLE_NAME = pk.TABLE_NAME \n                AND c.COLUMN_NAME = pk.COLUMN_NAME\n            WHERE c.TABLE_SCHEMA = ? AND c.TABLE_NAME = ?\n            ORDER BY c.ORDINAL_POSITION\n        \"\"\"\n        logger.debug(f\"Executing schema query for {schema_name}.{table_name}\")\n\n        cursor.execute(query, (schema_name, table_name))\n        columns = cursor.fetchall()\n\n        if not columns:\n            error_msg = f\"Table not found: {schema_name}.{table_name}\"\n            logger.error(error_msg)\n            return [{\"error\": error_msg}]\n\n        logger.info(\n            f\"Found {len(columns)} columns in {schema_name}.{table_name}\"\n        )\n\n        schema = {}\n        for col in columns:\n            col_name = col[0]\n            data_type = col[1]\n            char_len = col[2]\n            num_precision = col[3]\n            num_scale = col[4]\n            nullable = col[5] == \"YES\"\n            default = col[6]\n            is_pk = col[7] == \"YES\"\n\n            # Format type string\n            type_str = data_type\n            if char_len:\n                type_str += f\"({char_len})\"\n            elif num_precision:\n                if num_scale:\n                    type_str += f\"({num_precision},{num_scale})\"\n                else:\n                    type_str += f\"({num_precision})\"\n\n            schema[col_name] = {\n                \"data_type\": type_str,\n                \"nullable\": nullable,\n                \"is_primary_key\": is_pk,\n                \"default\": default if default else None\n            }\n            logger.debug(\n                f\"Column: {col_name}, type: {type_str}, \"\n                f\"nullable: {nullable}, pk: {is_pk}\"\n            )\n\n        return [schema]\n    except Exception as e:\n        error_msg = f\"Error retrieving schema: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return [{\"error\": error_msg}]\n</code></pre> <p>Comprehensive schema information: - Data types: Including length/precision/scale - Nullability: Whether NULL values allowed - Primary keys: Identifies join columns - Defaults: Column default values</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#12-mcp-tool-execute-polars-sql-files","title":"12. MCP Tool: Execute Polars SQL (Files)","text":"<pre><code>query_description = \"\"\"Polars SQL query. MUST use 'self' as table name. \nSupports: avg, count, sum, max, min, where, group by, order by, joins, etc.\"\"\"\n\n@mcp.tool()\ndef execute_polars_sql(\n    file_locations: List[str],\n    query: str = Field(description=query_description),\n    file_type: str = Field(default=\"csv\", description=\"csv or parquet\"),\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Execute SQL query on CSV or Parquet files using Polars.\n\n    **Use for file-based datasets only.** \n    For database tables, use execute_database_query().\n\n    **CRITICAL:** Query MUST use 'self' as table name, NOT the filename.\n\n    Example: execute_polars_sql(\n        file_locations=[\"data/employees.csv\"],\n        query=\"SELECT COUNT(*) FROM self WHERE IsActive = true\"\n    )\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"execute_polars_sql called for business_function: {business_function}\"\n    )\n    logger.info(f\"File locations ({len(file_locations)}): {file_locations}\")\n    logger.info(f\"Query: {query}\")\n    logger.debug(f\"File type: {file_type}\")\n\n    # Validation: Check for common mistakes\n    query_lower = query.lower()\n\n    # Check if query references database schema (wrong tool)\n    if \"mcp.\" in query or \"dbo.\" in query or any(\n        schema in query for schema in [\"mcp.\", \"dbo.\", \"hr.\", \"sales.\"]\n    ):\n        error_msg = (\n            \"WRONG TOOL: This query references a database table \"\n            \"(schema.table format). Use execute_database_query() instead.\"\n        )\n        logger.error(error_msg)\n        return [{\n            \"error\": error_msg,\n            \"hint\": \"File queries must use 'self' as table name, \"\n                    \"not schema.table\",\n            \"your_query\": query\n        }]\n\n    # Check if query uses 'self' (required for Polars)\n    if \"self\" not in query_lower:\n        error_msg = (\n            \"INVALID QUERY: Polars SQL queries must use 'self' \"\n            \"as the table name.\"\n        )\n        logger.error(error_msg)\n        return [{\n            \"error\": error_msg,\n            \"hint\": \"Change your query to: SELECT ... FROM self WHERE ...\",\n            \"your_query\": query\n        }]\n\n    # Enforce that all file_locations are within business function folder\n    allowed_prefix = os.path.abspath(\n        f\"/mnt/azure/mcp/{business_function}/\"\n    )\n    for file_location in file_locations:\n        if not os.path.abspath(file_location).startswith(allowed_prefix):\n            error_msg = (\n                f\"Access denied: file_location '{file_location}' is not \"\n                f\"within the allowed business_function folder \"\n                f\"'{allowed_prefix}'. You may only query files in your \"\n                \"assigned business_function folder.\"\n            )\n            logger.error(error_msg)\n            return [{\"error\": error_msg, \"your_query\": query}]\n\n    try:\n        logger.debug(\"Reading files into DataFrame\")\n        df = read_file_list(file_locations, file_type)\n        logger.info(f\"DataFrame loaded, shape: {df.shape}\")\n\n        logger.debug(\"Executing SQL query on DataFrame\")\n        op_df = df.sql(query)\n        logger.info(f\"Query executed successfully, result shape: {op_df.shape}\")\n\n        output_records = op_df.to_dicts()\n        logger.info(f\"Returning {len(output_records)} records\")\n        logger.debug(\n            f\"First record (if any): \"\n            f\"{output_records[0] if output_records else 'No records'}\"\n        )\n\n        return output_records\n    except Exception as e:\n        error_msg = f\"Query execution failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return [{\"error\": error_msg, \"your_query\": query}]\n</code></pre> <p>Validation and security: - Tool detection: Catches wrong tool usage (database vs file queries) - Syntax validation: Ensures 'self' is used as table name - Path validation: Prevents access to files outside business function - Error handling: Returns helpful error messages to LLM</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#13-mcp-tool-execute-database-query-sql-server","title":"13. MCP Tool: Execute Database Query (SQL Server)","text":"<pre><code>@mcp.tool()\ndef execute_database_query(\n    query: str = Field(\n        description=\"T-SQL query to execute on SQL Server database\"\n    ),\n    max_rows: int = Field(\n        default=1000, \n        description=\"Maximum rows to return (default: 1000, set to 0 for unlimited)\"\n    )\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Execute T-SQL queries on SQL Server database tables.\n\n    **Use for database tables only.** \n    For CSV/Parquet files, use execute_polars_sql().\n\n    **CRITICAL:** Use schema.table format (e.g., mcp.employees), NOT 'self'.\n\n    Example: execute_database_query(\n        query=\"SELECT COUNT(*) FROM mcp.vw_ADP_Employee_Daily_Snapshot \n               WHERE Position_Status = 'Active'\"\n    )\n\n    Default limit: 1000 rows. Set max_rows=0 for unlimited.\n    \"\"\"\n    business_function = get_current_business_function()\n    logger.info(\n        f\"execute_database_query called for \"\n        f\"business_function: {business_function}\"\n    )\n    logger.info(f\"Query: {query}\")\n    logger.debug(f\"Max rows: {max_rows}\")\n\n    if not is_database_configured():\n        logger.error(\"Database not configured\")\n        return [{\"error\": \"No database connection configured.\"}]\n\n    # Validation: Check for common mistakes\n    query_lower = query.lower()\n\n    # Check if query uses 'self' (wrong tool)\n    if \" self\" in query_lower or \"from self\" in query_lower or \\\n       \"join self\" in query_lower:\n        error_msg = (\n            \"WRONG TOOL: This query uses 'self' as table name. \"\n            \"Use execute_polars_sql() for file-based queries.\"\n        )\n        logger.error(error_msg)\n        return [{\n            \"error\": error_msg,\n            \"hint\": \"Database queries must use schema.table format \"\n                    \"(e.g., mcp.employees), not 'self'\",\n            \"your_query\": query\n        }]\n\n    # Check if query has schema.table format\n    if \"mcp.\" not in query_lower and \"dbo.\" not in query_lower:\n        error_msg = (\n            \"INVALID QUERY: Database queries must use schema.table format.\"\n        )\n        logger.error(error_msg)\n        return [{\n            \"error\": error_msg,\n            \"hint\": \"Use: SELECT ... FROM mcp.table_name or dbo.table_name\",\n            \"your_query\": query\n        }]\n\n    # Validate business_function in table name\n    if f\"{business_function}_\" not in query_lower:\n        logger.warning(\n            f\"Query does not appear to reference business_function \"\n            f\"'{business_function}' tables\"\n        )\n\n    try:\n        conn = get_db_connection()\n        cursor = conn.cursor()\n\n        # Add TOP clause if not present and max_rows is set\n        modified_query = query\n        if max_rows &gt; 0:\n            query_upper = query.strip().upper()\n            if not query_upper.startswith(\"SELECT TOP\") and \\\n               query_upper.startswith(\"SELECT\"):\n                modified_query = query.strip()\n                # Insert TOP clause after SELECT\n                select_pos = modified_query.upper().find(\"SELECT\")\n                modified_query = (\n                    modified_query[:select_pos + 6] + \n                    f\" TOP {max_rows}\" + \n                    modified_query[select_pos + 6:]\n                )\n                logger.debug(f\"Added TOP {max_rows} clause to query\")\n\n        logger.debug(f\"Executing modified query: {modified_query}\")\n        cursor.execute(modified_query)\n\n        # Get column names\n        columns = [column[0] for column in cursor.description]\n        logger.debug(f\"Result columns ({len(columns)}): {columns}\")\n\n        # Fetch results\n        results = []\n        row_count = 0\n        for row in cursor.fetchall():\n            row_count += 1\n            # Convert values to JSON-serializable types\n            row_dict = {}\n            for col_name, value in zip(columns, row):\n                # Handle datetime and other types\n                if hasattr(value, 'isoformat'):\n                    row_dict[col_name] = value.isoformat()\n                elif value is None:\n                    row_dict[col_name] = None\n                else:\n                    row_dict[col_name] = value\n            results.append(row_dict)\n\n        logger.info(f\"Query executed successfully, returned {row_count} rows\")\n        if results:\n            logger.debug(f\"First row: {results[0]}\")\n\n        return results\n    except Exception as e:\n        error_msg = f\"Query execution error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return [{\"error\": error_msg, \"your_query\": query}]\n</code></pre> <p>Key features: - Automatic row limiting: Adds TOP clause to prevent massive result sets - JSON serialization: Converts datetime and other types properly - Tool validation: Catches wrong tool usage (file vs database queries) - Security: Validates business function table naming</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#14-main-entry-point-and-server-startup","title":"14. Main Entry Point and Server Startup","text":"<pre><code>import uvicorn\n\ndef main():\n    \"\"\"Main entry point for MCP server\"\"\"\n    logger.info(\"=\" * 80)\n    logger.info(\"MAIN: Starting single MCP Server on port 8000\")\n    logger.info(\"=\" * 80)\n\n    try:\n        # Create the FastMCP HTTP app\n        app = mcp.http_app()\n        logger.info(\"FastMCP HTTP app created successfully\")\n\n        # Add the business function extraction middleware\n        logger.info(\"Adding BusinessFunctionExtractionMiddleware to app\")\n        app.add_middleware(BusinessFunctionExtractionMiddleware)\n        logger.info(\"Middleware added successfully\")\n\n        logger.info(\"Server configuration:\")\n        logger.info(\"  - Host: 0.0.0.0\")\n        logger.info(\"  - Port: 8000\")\n        logger.info(\"  - Functions supported: IT, HR, FIN\")\n        logger.info(\"  - Functions extraction: Via Query String (mcp?function=XXX)\")\n        logger.info(\"=\" * 80)\n\n        # Run the server - this blocks indefinitely\n        logger.info(\"Starting Uvicorn server...\")\n        config = uvicorn.Config(\n            app,\n            host=\"0.0.0.0\",\n            port=8000,\n            log_level=\"info\",\n            access_log=True,\n            loop=\"asyncio\",\n            timeout_keep_alive=75\n        )\n        server = uvicorn.Server(config)\n        server.run()\n\n    except Exception as e:\n        logger.critical(f\"Failed to start server: {str(e)}\", exc_info=True)\n        raise\n    finally:\n        logger.info(\"=\" * 80)\n        logger.info(\"MAIN: MCP Server shutdown complete\")\n        logger.info(\"=\" * 80)\n\nif __name__ == \"__main__\":\n    logger.info(\"=\" * 80)\n    logger.info(\"Script execution started - Single Process Mode\")\n    logger.info(\"=\" * 80)\n    try:\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"\\n\" + \"=\" * 80)\n        logger.info(\"MAIN: Keyboard interrupt received, shutting down...\")\n        logger.info(\"=\" * 80)\n    except Exception as e:\n        logger.critical(f\"Fatal error in main execution: {str(e)}\", exc_info=True)\n        raise\n</code></pre> <p>Server configuration: - Port 8000: Standard HTTP port for MCP servers - Host 0.0.0.0: Listen on all network interfaces - Async event loop: Supports concurrent requests - Keep-alive timeout: 75 seconds for long-running queries - Access logging: All requests logged for auditing</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#deployment-azure-container-instance-with-tls-offloading","title":"Deployment: Azure Container Instance with TLS Offloading","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#container-build-and-deployment","title":"Container Build and Deployment","text":"<p>1. Dockerfile: <pre><code>FROM mcr.microsoft.com/devcontainers/python:3\n\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Install only necessary packages for MCP Analyst\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y \\\n    build-essential \\\n    python3-dev \\\n    git \\\n    curl \\\n    cifs-utils &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Copy and install Python dependencies\nCOPY requirements.txt .\nRUN python -m pip install --upgrade pip &amp;&amp; \\\n    python -m pip install -r requirements.txt\n\n# Add Microsoft GPG key\nRUN curl -fsSL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg\n\n# Add Microsoft SQL Server repository\nRUN curl https://packages.microsoft.com/config/debian/12/prod.list | tee /etc/apt/sources.list.d/mssql-release.list\n\n# Install Microsoft SQL Server related packages\nRUN apt-get update &amp;&amp; \\\n    ACCEPT_EULA=Y apt-get install -y \\\n    msodbcsql18 \\\n    mssql-tools18\n\n# Add MS SQL Server tools to PATH\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' &gt;&gt; ~/.bashrc \n\n# Clean up\nRUN apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n\n#Argument passed in via Command line \nARG GITHUB_PAT\n\n# Clone the private repository using the build argument\nRUN git clone https://$GITHUB_PAT@github.com/GDEnergyproducts/GDEP-MCP-ANALYST.git /app\n\nWORKDIR /app\n\nRUN chmod +x scripts/container/mountstorage.sh\n\n# Create a startup script that runs mcp server\nRUN echo '#!/bin/bash\\n\\\nset -e\\n\\\necho \"Starting container initialization...\"\\n\\\necho \"Starting analyst.py...\"\\n\\\npython /app/src/analyst.py\\n\\\n' &gt; /app/start.sh &amp;&amp; chmod +x /app/start.sh\n\nCMD [\"/app/start.sh\"]\n</code></pre></p> <p>2. requirements.txt: <pre><code># Core MCP dependencies\npolars&gt;=0.20.0\npydantic&gt;=2.0.0\n\n# Azure dependencies (from your existing setup)\nazure-identity\nazure-keyvault-secrets\n\n# Data processing\npandas\npyarrow&gt;=14.0.0\npyodbc&gt;=5.3.0\n\n# Development/debugging tools\npytest\nipython\n\n# Need this !!\nfastmcp\n</code></pre></p>"},{"location":"ai-claude-mcp-analytic-server-part3/#tls-offloading-with-fortigate","title":"TLS Offloading with FortiGate","text":"<p>Why TLS offloading at the edge? - SSL/TLS termination: FortiGate handles all encryption/decryption - Certificate management: Centralized SSL cert renewal - Performance: Offload crypto operations from container - Security: Web Application Firewall (WAF) protection - Inspection: Deep packet inspection for threats</p> <p>FortiGate configuration (simplified): <pre><code>config firewall vip\n    edit \"mcp-analyst-vip\"\n        set extip 203.0.113.10\n        set mappedip 10.0.1.100  # Container internal IP\n        set extintf \"wan1\"\n        set portforward enable\n        set extport 443\n        set mappedport 8000\n        set ssl-certificate \"wildcard-cert\"\n    next\nend\n\nconfig firewall policy\n    edit 1\n        set name \"Allow-MCP-HTTPS\"\n        set srcintf \"wan1\"\n        set dstintf \"internal\"\n        set srcaddr \"all\"\n        set dstaddr \"mcp-analyst-vip\"\n        set action accept\n        set schedule \"always\"\n        set service \"HTTPS\"\n        set utm-status enable\n        set ssl-ssh-profile \"certificate-inspection\"\n        set av-profile \"default\"\n        set webfilter-profile \"default\"\n        set ips-sensor \"default\"\n    next\nend\n</code></pre></p> <p>Result: - External URL: <code>https://mcp.gdenergyproducts.com</code> (port 443) - Internal traffic: HTTP to container (port 8000) - TLS handled by FortiGate with corporate SSL certificate</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#claude-desktop-configuration","title":"Claude Desktop Configuration","text":"<p>To connect Claude Desktop to your MCP server:</p> <ol> <li>Open Claude Desktop and go to File \u2192 Settings.</li> <li>Select Connector, then click Add Connector.</li> <li>Enter the following URL (replace <code>function=it</code> as needed):      <pre><code>https://mcp.gdenergyproducts.com/mcp?function=it\n</code></pre></li> <li>Click Connect and go through the OAuth authentication process.</li> <li>If Claude Desktop does not restart automatically, restart it manually.</li> <li>After restart, go back to Configure and allow access as appropriate. If not, you will be prompted for access when each tool is used.</li> </ol> <p>4. Start asking questions! - \"How many Windows devices do we have?\" - \"Show me employees terminated in the last 90 days\" - \"What's our Office 365 license utilization?\"</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#security-best-practices","title":"Security Best Practices","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#1-authentication-and-authorization","title":"1. Authentication and Authorization","text":"<ul> <li>\u2705 OAuth 2.0 (Azure Entra ID) only: No API keys or basic auth</li> <li>\u2705 Per-user context: Track who queries what</li> <li>\u2705 Business function isolation: Users can't cross boundaries</li> <li>\u2705 Audit logging: Every tool call logged with user identity</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part3/#2-data-protection","title":"2. Data Protection","text":"<ul> <li>\u2705 TLS in transit: All external traffic encrypted</li> <li>\u2705 No data exfiltration: Results returned only to authenticated user</li> <li>\u2705 Path validation: Prevents directory traversal</li> <li>\u2705 Query validation: Catches SQL injection attempts</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part3/#3-secrets-management","title":"3. Secrets Management","text":"<ul> <li>\u2705 Azure Key Vault: All secrets stored securely</li> <li>\u2705 No hardcoded credentials: Injected at runtime</li> <li>\u2705 Least privilege: Service principals with minimal permissions</li> <li>\u2705 Secret rotation: Automated via Key Vault</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part3/#4-network-security","title":"4. Network Security","text":"<ul> <li>\u2705 TLS offloading: FortiGate terminates SSL/TLS</li> <li>\u2705 WAF protection: Web Application Firewall blocks attacks</li> <li>\u2705 Internal-only database: No public internet exposure</li> <li>\u2705 VNet integration: Container in private virtual network</li> </ul>"},{"location":"ai-claude-mcp-analytic-server-part3/#performance-and-scalability","title":"Performance and Scalability","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#performance-characteristics","title":"Performance Characteristics","text":"<p>File queries (Polars): - 1 million rows: ~1-2 seconds - 10 million rows: ~5-10 seconds - Concurrent users: 20-50 simultaneous queries</p> <p>Database queries (SQL Server): - Simple SELECT: &lt;100ms - Complex joins: 1-5 seconds - Limited by database performance, not MCP server</p> <p>Memory usage: - Base: ~200MB - Per concurrent query: +50-500MB (depends on data size) - Recommended: 4GB RAM for 10+ concurrent users</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#scaling-strategies","title":"Scaling Strategies","text":"<p>Vertical scaling (single container): - Increase CPU/RAM allocation - Good for up to 50 concurrent users</p> <p>Horizontal scaling (multiple containers): - Deploy behind Azure Load Balancer - Session affinity not required (stateless) - Scale to 100+ users</p> <p>Database optimization: - Create indexed views for common queries - Partition large tables by business function - Use columnstore indexes for analytics</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<p>1. Application logs (<code>/mnt/azure/logs/gdepmcp.log</code>): - Tool call frequency and duration - Authentication failures - Query errors - User activity patterns</p> <p>2. Container metrics (Azure Monitor): - CPU utilization - Memory usage - Network throughput - Container restart count</p> <p>3. Database metrics (SQL Server DMVs): - Long-running queries - Connection pool usage - Lock contention - Index usage</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Issue: OAuth authentication fails Solution:  - Verify <code>accessTokenAcceptedVersion: 2</code> in manifest - Check redirect URIs include Claude callback URL - Ensure <code>access_as_user</code> scope is granted admin consent</p> <p>Issue: \"Access denied\" errors for files Solution: - Verify business function in URL matches data folder - Check file permissions in Azure File Share - Validate path in <code>get_schema()</code> or <code>execute_polars_sql()</code></p> <p>Issue: Database queries timeout Solution: - Check network connectivity to SQL Server - Verify firewall allows container IP - Optimize query with indexes - Increase <code>max_rows</code> limit or add WHERE clause</p> <p>Issue: Container crashes with OOM (Out of Memory) Solution: - Increase container memory allocation - Add pagination to large queries - Limit <code>infer_schema_length</code> in Polars - Review concurrent user load</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#lessons-learned","title":"Lessons Learned","text":""},{"location":"ai-claude-mcp-analytic-server-part3/#what-worked-well","title":"What Worked Well","text":"<ol> <li>OAuth 2.0 (Azure Entra ID) integration: Seamless enterprise SSO</li> <li>Multi-tenant architecture: Single server for all business functions</li> <li>Polars performance: Handles millions of rows effortlessly</li> <li>Data catalog: LLM generates correct queries consistently</li> <li>TLS offloading: Simplified container deployment</li> <li>Key Vault: Secure, centralized secrets management</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#challenges-and-solutions","title":"Challenges and Solutions","text":"<ol> <li>OAuth version gotcha: Manually edit manifest to v2.0</li> <li> <p>Solution: Document clearly in setup guide</p> </li> <li> <p>Claude Desktop callback: Not in standard OAuth flow</p> </li> <li> <p>Solution: Add <code>claude.ai/api/mcp/auth_callback</code> to redirect URIs</p> </li> <li> <p>SAP delimiter detection: Semicolon vs comma</p> </li> <li> <p>Solution: Detect <code>sap_</code> prefix in filename</p> </li> <li> <p>Connection pooling: Database connections timing out</p> </li> <li> <p>Solution: Implement health checks and reconnection logic</p> </li> <li> <p>Log file growth: Filled disk in production</p> </li> <li>Solution: Use <code>RotatingFileHandler</code> with size limits</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Row-level security: Filter data based on user's Azure groups</li> <li>Query caching: Redis cache for repeated queries</li> <li>Rate limiting: Prevent abuse by individual users</li> <li>Advanced analytics: Support for Python/R code execution</li> <li>Mobile app: Native iOS/Android client with OAuth</li> <li>Real-time data: WebSocket streaming for live dashboards</li> </ol>"},{"location":"ai-claude-mcp-analytic-server-part3/#conclusion","title":"Conclusion","text":"<p>The evolution from stdio-based to HTTPS-based MCP server represents a quantum leap in enterprise readiness:</p> <ul> <li>From single-user to multi-tenant: One server, many users, isolated data</li> <li>From no auth to OAuth 2.0 (Azure Entra ID): Enterprise-grade security and SSO</li> <li>From local to cloud: Scalable, containerized deployment</li> <li>From manual to automated: Zero per-user setup, instant access</li> </ul> <p>This architecture enables true self-service analytics for business users while maintaining the security, governance, and scalability requirements of enterprise IT. By combining Azure Entra ID, containerization, TLS offloading, and multi-tenant data isolation, you can deliver LLM-powered analytics that business users love and IT teams trust.</p> <p>The result: Business leaders can ask any question, across any dataset, with natural language\u2014securely, instantly, and without waiting for IT.</p>"},{"location":"ai-claude-mcp-analytic-server-part3/#references-and-credits","title":"References and Credits","text":"<ul> <li>FastMCP Documentation \u2013 MCP server framework with OAuth support</li> <li>Polars DataFrame Library \u2013 High-performance data processing</li> <li>Claude Desktop \u2013 LLM client with custom HTTPS connectors</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/","title":"Cleaning Up Obsolete FSLogix Profiles in Azure","text":"<p>Obsolete FSLogix profile containers can consume significant storage and increase costs in Azure environments. This article explains how to identify and delete outdated profiles using a Bash script, helping you save space and reduce expenses. The approach is multi-step: first, list and analyze profiles, then safely delete those that are no longer needed. This process is ideal for automation and can be run in a container for portability and security.</p>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#why-clean-up-obsolete-profiles","title":"Why Clean Up Obsolete Profiles?","text":"<ul> <li>Cost Savings: Old FSLogix profile containers can accumulate and consume large amounts of Azure Files storage, leading to unnecessary costs.</li> <li>Performance: Removing unused profiles can improve performance and reduce clutter.</li> <li>Compliance: Regular cleanup helps maintain a tidy, compliant environment.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#multi-step-approach","title":"Multi-Step Approach","text":"<ol> <li>List and Analyze Profiles: Identify which profiles are old and candidates for deletion.</li> <li>Delete Obsolete Profiles: Remove only those that are confirmed to be outdated.</li> </ol> <p>This article covers the first step\u2014identifying obsolete profiles. (You can extend the script to perform deletions after review.)</p>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#example-script-identify-obsolete-fslogix-profiles","title":"Example Script: Identify Obsolete FSLogix Profiles","text":"<p>Below is a Bash script that lists FSLogix profile containers in an Azure Files share, checks their last modified date, and logs those that haven't been updated in a specified number of days. All sensitive values are masked for security.</p> <pre><code>#!/bin/bash\n\n# Define the output log files\ndeleteLogFile=\"step1-verbose.log\"\nsizeLogFile=\"step1-filesize.log\"\ndeleteFile=\"step1-input4step2.txt\"\n\n# Variables (replace with your own values)\nresourceGroupName=\"&lt;your-resource-group&gt;\"\nstorageAccountName=\"&lt;your-storage-account&gt;\"\nfileShareName=\"&lt;your-file-share&gt;\"\ndaysThreshold=30\nmaxProfiles=500  # Limit the number of profiles to process\n\n# Determine the script directory\nscriptDir=\"$(dirname \\\"$(realpath \\\"$0\\\")\\\")\"\ndeleteFile=\"$scriptDir/$deleteFile\"\nsizeFile=\"$scriptDir/$sizeLogFile\"\ndeleteLogFile=\"$scriptDir/$deleteLogFile\"\n\n# Delete existing files if they exist\nif [ -f \"$deleteFile\" ]; then\n    echo \"Deleting existing delete file: $deleteFile\"\n    rm \"$deleteFile\"\nfi\n\nif [ -f \"$deleteLogFile\" ]; then\n    echo \"Deleting existing delete log file: $deleteLogFile\"\n    rm \"$deleteLogFile\"\nfi\n\nif [ -f \"$sizeFile\" ]; then\n    echo \"Deleting existing size log file: $sizeFile\"\n    rm \"$sizeFile\"\nfi\n\n# Redirect all echo output to the delete log file\nexec &gt; \"$deleteLogFile\" 2&gt;&amp;1\n\necho \"Starting script...\"\n\necho \"Variables set: resourceGroupName=$resourceGroupName, storageAccountName=$storageAccountName, fileShareName=$fileShareName, daysThreshold=$daysThreshold, deleteFile=$deleteFile, sizeFile=$sizeFile\"\n\n# Get the storage account key\necho \"Fetching storage account key...\"\nstorageAccountKey=$(az storage account keys list --resource-group $resourceGroupName --account-name $storageAccountName --query '[0].value' --output tsv)\nif [ $? -ne 0 ]; then\n    echo \"Failed to fetch storage account key.\"\n    exit 1\nfi\necho \"Storage account key fetched successfully.\"\n\n# Get the current date in seconds since epoch and human-readable format\ncurrentDate=$(date +%s)\nhumanReadableCurrentDate=$(date -d @$currentDate +\"%Y-%m-%d %H:%M:%S\")\necho \"Current date (epoch): $currentDate\"\necho \"Current date (human-readable): $humanReadableCurrentDate\"\n\n# Calculate the threshold date (30 days ago)\nthresholdDate=$((currentDate - daysThreshold * 24 * 60 * 60))\nhumanReadableThresholdDate=$(date -d @$thresholdDate +\"%Y-%m-%d %H:%M:%S\")\necho \"Threshold date (epoch): $thresholdDate\"\necho \"Threshold date (human-readable): $humanReadableThresholdDate\"\n\n# Initialize arrays to store profile directories, sizes, last modified dates, and delete candidates\noldProfiles=()\nprofileSizes=()\ndeleteCandidates=()\n\n# List all directories in the file share\necho \"Listing profile directories...\"\nprofileDirs=$(az storage file list --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --output tsv --query '[].name')\nif [ $? -ne 0 ]; then\n    echo \"Failed to list profile directories.\"\n    exit 1\nfi\n\n# Loop through each profile directory\nprofileCount=0\nfor profileDir in $profileDirs; do\n    if [ $profileCount -ge $maxProfiles ]; then\n        echo \"Processed $maxProfiles profiles. Exiting loop.\"\n        break\n    fi\n\n    echo \"Processing profile directory: $profileDir\"\n\n    # List files in the profile directory\n    files=$(az storage file list --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --path $profileDir --output tsv --query '[].name')\n    if [ $? -ne 0 ]; then\n        echo \"Failed to list files in $profileDir.\"\n        continue\n    fi\n\n    for file in $files; do\n        if [[ $file == *.vhdx ]]; then\n            echo \"Processing file: $file\"\n\n            # Get the properties of the .vhdx file\n            fileProperties=$(az storage file show --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --path $profileDir/$file --query '{properties: properties}' --output json)\n            if [ $? -ne 0 ]; then\n                echo \"Failed to get properties for $file.\"\n                continue\n            fi\n\n            # Get file size\n            fileSize=$(echo \"$fileProperties\" | jq -r '.properties.contentLength')\n            if [ $? -ne 0 ]; then\n                echo \"Failed to get file size for $file.\"\n                continue\n            fi\n            echo \"File size: $fileSize bytes\"\n\n            # Convert file size from bytes to GB\n            fileSizeGB=$(echo \"scale=2; $fileSize / (1024 * 1024 * 1024)\" | bc)\n            echo \"File size: $fileSizeGB GB\"\n\n            # Add profile directory and size to the arrays\n            profileSizes+=(\"$fileSizeGB $profileDir\")\n\n            # Get the last modified date of the file\n            lastModified=$(echo \"$fileProperties\" | jq -r '.properties.lastModified')\n            lastModifiedDate=$(date -d \"$lastModified\" +%s)\n            if [ $? -ne 0 ]; then\n                echo \"Failed to convert last modified date.\"\n                continue\n            fi\n            echo \"Last modified date (epoch): $lastModifiedDate\"\n\n            # Check if the last modified date is older than the threshold\n            if [[ $lastModifiedDate -lt $thresholdDate ]]; then\n                echo \"Profile $profileDir has not been updated in the last $daysThreshold days. Last modified: $lastModified\"\n\n                # Convert last modified date to human-readable format\n                humanReadableLastModifiedDate=$(date -d \"$lastModified\" +\"%Y-%m-%d %H:%M:%S\")\n\n                # Add profile directory to the delete candidates array\n                deleteCandidates+=(\"$profileDir - Last Modified: $humanReadableLastModifiedDate\")\n            fi\n        fi\n    done\n    profileCount=$((profileCount + 1))\ndone\n\n# Sort and write the profile sizes to the size log file\nif [ ${#profileSizes[@]} -ne 0 ]; then\n    echo \"Writing profile sizes to $sizeFile...\"\n    {\n        echo \"Profile directories and their .vhdx file sizes (in GB):\"\n        echo \"-------------------------------------\"\n        for i in \"${profileSizes[@]}\"; do\n            echo \"$i\"\n        done | sort -nr -k1 &gt; \"$sizeFile\"\n    }\n    echo \"Done writing to $sizeFile.\"\nelse\n    echo \"No profiles found with .vhdx files.\"\nfi\n\n# Write the outdated profiles to the delete file\nif [ ${#deleteCandidates[@]} -ne 0 ]; then\n    echo \"Writing outdated profiles to $deleteFile...\"\n    {\n        echo \"Profiles that haven't been updated in the last $daysThreshold days:\"\n        echo \"Threshold Date (human-readable): $humanReadableThresholdDate\"\n        echo \"Current Date (human-readable): $humanReadableCurrentDate\"\n        echo \"-------------------------------------\"\n        for i in \"${deleteCandidates[@]}\"; do\n            echo \"$i\"\n        done\n    } &gt; \"$deleteFile\"\n    echo \"Done writing to $deleteFile.\"\nelse\n    echo \"No profiles found that are outdated.\"\nfi\n\necho \"Script completed.\"\n</code></pre>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Setup and Variable Initialization:</li> <li>The script sets up log file names and variables for the resource group, storage account, and file share (all masked for security).</li> <li> <p>It determines the script directory and ensures log files are fresh for each run.</p> </li> <li> <p>Fetch Storage Account Key:</p> </li> <li> <p>Uses the Azure CLI to retrieve the storage account key for authentication.</p> </li> <li> <p>Date Calculations:</p> </li> <li> <p>Gets the current date and calculates the threshold date (e.g., 30 days ago) to identify old profiles.</p> </li> <li> <p>List Profile Directories:</p> </li> <li> <p>Uses <code>az storage file list</code> to enumerate all profile directories in the file share.</p> </li> <li> <p>Analyze Each Profile:</p> </li> <li>For each profile directory, lists files and looks for <code>.vhdx</code> files (FSLogix containers).</li> <li>Retrieves file size and last modified date for each <code>.vhdx</code> file.</li> <li> <p>If the file hasn't been updated in the threshold period, adds it to the delete candidates list.</p> </li> <li> <p>Log Results:</p> </li> <li>Writes a sorted list of profile sizes to a log file for review.</li> <li> <p>Writes a list of outdated profiles to a separate file for potential deletion.</p> </li> <li> <p>Output:</p> </li> <li>All output is logged for auditing and review before any deletion is performed.</li> </ol>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#benefits-of-running-in-a-container","title":"Benefits of Running in a Container","text":"<ul> <li>Isolation: Keeps dependencies and credentials isolated from your local environment.</li> <li>Portability: Easily run the script in any environment with Docker or container support.</li> <li>Security: Credentials and logs are contained within the container.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#next-steps-deleting-obsolete-profiles","title":"Next Steps: Deleting Obsolete Profiles","text":"<ul> <li>Review the generated log files to confirm which profiles are safe to delete.</li> <li>You can extend this script to delete profiles by using <code>az storage file delete</code> for each candidate.</li> <li>Always back up or confirm with stakeholders before deleting user data.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#usage-example","title":"Usage Example","text":"<p>This script is typically run as part of a maintenance workflow, either manually or on a schedule. For example, you might: - Run the script in a container or Azure Cloud Shell. - Review the output files (<code>step1-filesize.log</code> and <code>step1-input4step2.txt</code>). - Use a follow-up script to delete the confirmed obsolete profiles.</p> <p>By regularly cleaning up obsolete FSLogix profiles, you can save on Azure storage costs and keep your environment healthy and efficient.</p>"},{"location":"avd-custon-image-compute-gallery-part1/","title":"Part 1: Building and Publishing an Custom Image to Azure Compute Gallery","text":"<p>This article provides a comprehensive, step-by-step guide to creating a custom Azure Virtual Desktop (AVD) image using Infrastructure as Code (IaC), and publishing it to the Azure Compute Gallery. This is the foundation for deploying consistent, secure, and up-to-date AVD environments. For deploying AVD desktops from this image, see Part 2.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#overview","title":"Overview","text":"<p>The process involves: - Cleaning up old images in the Azure Compute Gallery - Determining the latest Microsoft 365 image SKU - Creating an image template using Bicep and Azure CLI - Building the image and publishing it to the Compute Gallery</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-1-clean-out-old-images-in-azure-compute-gallery","title":"Step 1: Clean Out Old Images in Azure Compute Gallery","text":"<ol> <li>Navigate to your Azure Compute Gallery (e.g., <code>GDEP_Azure_Compute_Gallery</code>).</li> <li>Identify and delete old images (sort by published date descending).</li> <li>Keep only the most recent image (from last month) for rollback purposes.</li> <li>Log in to Azure CLI as an administrator:</li> </ol> <pre><code>az login\n</code></pre> <p>Note: Use an account with sufficient permissions (e.g., Contributor or higher on the resource group). Masked example: <code>user.name@domain.com</code>.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-2-determine-the-latest-microsoft-365-image-sku","title":"Step 2: Determine the Latest Microsoft 365 Image SKU","text":"<p>To ensure your template uses the latest Microsoft 365 image, run:</p> <pre><code>az vm image list-skus --location eastus --publisher MicrosoftWindowsDesktop --offer office-365 --output table\n</code></pre> <ul> <li>This command lists all available SKUs for Microsoft 365 images in the East US region.</li> <li>Update your Bicep or parameter file to reference the latest SKU as needed.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-3-create-the-image-template-using-bicep","title":"Step 3: Create the Image Template Using Bicep","text":"<p>You can define your image template as code using a Bicep file (e.g., <code>avd.bicep</code>).</p>"},{"location":"avd-custon-image-compute-gallery-part1/#example-bicep-snippet","title":"Example Bicep Snippet","text":"<pre><code>param location string = 'eastus'\nparam imageTemplateName string = 'GDEPAVDWin11Template'\nparam galleryName string = 'GDEP_Azure_Compute_Gallery'\nparam resourceGroupName string = 'rg-gdep-peus-avd'\nparam sourceImagePublisher string = 'MicrosoftWindowsDesktop'\nparam sourceImageOffer string = 'office-365'\nparam sourceImageSku string = 'latest-sku-here'\nparam storageAccountUrl string = 'https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software/'\n\n// ...other parameters as needed...\n\nresource imageTemplate 'Microsoft.VirtualMachineImages/imageTemplates@2022-02-14' = {\n  name: imageTemplateName\n  location: location\n  properties: {\n    source: {\n      type: 'PlatformImage'\n      publisher: sourceImagePublisher\n      offer: sourceImageOffer\n      sku: sourceImageSku\n      version: 'latest'\n    }\n    customize: [\n      {\n        type: 'PowerShell'\n        name: 'InstallSoftware'\n        scriptUri: '${storageAccountUrl}install.ps1'\n      }\n      // Add more customization steps as needed\n    ]\n    distribute: [\n      {\n        type: 'SharedImage'\n        galleryImageId: '/subscriptions/&lt;sub-id&gt;/resourceGroups/${resourceGroupName}/providers/Microsoft.Compute/galleries/${galleryName}/images/GDEPAVDWin11Image'\n        runOutputName: 'GDEPAVDWin11ImageOutput'\n        artifactTags: {\n          source: 'avd-image-builder'\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#parameter-file-example-avdpjson","title":"Parameter File Example (<code>avdp.json</code>)","text":"<pre><code>{\n  \"location\": { \"value\": \"eastus\" },\n  \"imageTemplateName\": { \"value\": \"GDEPAVDWin11Template\" },\n  \"galleryName\": { \"value\": \"GDEP_Azure_Compute_Gallery\" },\n  \"resourceGroupName\": { \"value\": \"rg-gdep-peus-avd\" },\n  \"sourceImagePublisher\": { \"value\": \"MicrosoftWindowsDesktop\" },\n  \"sourceImageOffer\": { \"value\": \"office-365\" },\n  \"sourceImageSku\": { \"value\": \"latest-sku-here\" },\n  \"storageAccountUrl\": { \"value\": \"https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software/\" }\n}\n</code></pre> <p>Parameter Explanations: - <code>location</code>: Azure region for deployment. - <code>imageTemplateName</code>: Name for the image template resource. - <code>galleryName</code>: Name of the Azure Compute Gallery. - <code>resourceGroupName</code>: Resource group for the template and gallery. - <code>sourceImagePublisher</code>, <code>sourceImageOffer</code>, <code>sourceImageSku</code>: Define the base image. - <code>storageAccountUrl</code>: URL to a non-secure storage account containing install scripts and software (mask this in documentation).</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-4-deploy-the-image-template","title":"Step 4: Deploy the Image Template","text":"<p>Run the following command to deploy the image template:</p> <pre><code>az deployment group create --name GDEPAVDImage --resource-group rg-gdep-peus-avd --template-file ./compute/img/avd/avd.bicep --parameters @./compute/vm/avd/avdp.json\n</code></pre> <ul> <li>This command creates the image template in the specified resource group.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-5-build-the-image-and-publish-to-compute-gallery","title":"Step 5: Build the Image and Publish to Compute Gallery","text":"<p>After the template is created, build the image and publish it:</p> <pre><code>az image builder run --resource-group rg-gdep-peus-avd --name GDEPAVDWin11Template --no-wait\n</code></pre> <ul> <li>This command starts the image build process and publishes the resulting image to the Azure Compute Gallery.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-6-automate-software-installation-and-configuration-with-avdps1","title":"Step 6: Automate Software Installation and Configuration with avd.ps1","text":"<p>The <code>avd.ps1</code> PowerShell script is used during image creation to automate the installation and configuration of all required software and settings on the AVD image. Below is a detailed breakdown of its structure and key functions, with code examples and explanations for each part.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#key-functions-in-avdps1","title":"Key Functions in avd.ps1","text":""},{"location":"avd-custon-image-compute-gallery-part1/#1-invokestartprocess","title":"1. <code>InvokeStartProcess</code>","text":"<p>Runs a command (such as an installer) in a specified working directory with arguments, waits for completion, and logs the result.</p> <pre><code>Function InvokeStartProcess {\n    Param([string]$Command,[string]$WorkingDirectory,[string]$Arguments)\n    try {\n        $InstallProcess = Start-Process -FilePath $Command `\n                                        -WorkingDirectory $WorkingDirectory `\n                                        -ArgumentList $Arguments `\n                                        -Wait `\n                                        -Passthru\n        Write-Log \"Installed successfully  using command \", $Command, $Arguments -join\n    } catch {\n        Write-Log \"Error While running command \", $Command, $Arguments -join\n        $InstallProcess.Kill()\n    }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#2-downloadfilefromsa","title":"2. <code>DownloadfilefromSA</code>","text":"<p>Downloads a file from a given URI to a destination filename and logs the result.</p> <pre><code>Function DownloadfilefromSA {\n    Param([string]$SourceURI,[string]$DestinationFileName)\n    try {\n        Invoke-WebRequest $SourceURI -OutFile $DestinationFileName        \n        Write-Log \"Downloaded successfully file \", $DestinationFileName -join\n    } catch {\n        Write-Log \"Error While Downloading file \", $DestinationFileName -join\n        $InstallProcess.Kill()\n    }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#3-write-log","title":"3. <code>Write-Log</code>","text":"<p>Writes a message to a log file and outputs it to the console.</p> <pre><code>Function Write-Log {\n    Param($message)\n    Write-Output \"$(get-date -format 'yyyyMMdd HH:mm:ss') $message\" | Out-File -Encoding utf8 $logFile -Append\n    Write-Output $message  \n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#4-installandconfiguresoftware","title":"4. <code>InstallandConfigureSoftware</code>","text":"<p>This is the main function that orchestrates all software installations, registry changes, and configuration steps. It uses Chocolatey (<code>choco</code>) to install a wide range of software and handles additional configuration for FSLogix, SAP, and more.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#chocolatey-installs-with-code","title":"Chocolatey Installs (with code):","text":"<pre><code>choco install googlechrome -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install notepadplusplus -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install 7zip.install -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install visioviewer -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install vscode -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install git -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install powerbi -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install winscp -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install python -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install wireshark -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install sql-server-management-studio -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install putty -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\n</code></pre> <p>Explanation: - Each <code>choco install</code> command installs a specific application silently and logs the output. - Chocolatey is a package manager for Windows, making it easy to automate software installation in your image builds. - You can add or remove packages as needed for your own environment.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#additional-steps-in-installandconfiguresoftware","title":"Additional Steps in <code>InstallandConfigureSoftware</code>:","text":"<ul> <li>Configures FSLogix profile containers via registry.</li> <li>Applies local GPO and Office registry settings.</li> <li>Installs SAP, BPC Excel Add-In, and other business software.</li> <li>Creates desktop shortcuts for SAP, Notepad++, and a graceful logoff.</li> <li>Installs endpoint protection (Crowdstrike) and other utilities.</li> <li>All actions are logged for troubleshooting.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#example-downloading-and-installing-software","title":"Example: Downloading and Installing Software","text":"<pre><code>$StorageAccountDownloadURI = \"https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software4avd/\"\n$SoftwarefolderName = \"C:\\\\Software\\\\\"\n\n# Download a file\n$file2Download = \"ComputerGPO.cmd\"\n$fileURL2DownloadFrom = $StorageAccountDownloadURI + $file2Download\nInvoke-WebRequest $fileURL2DownloadFrom -OutFile $SoftwarefolderName\\$file2Download\n\n# Install Chrome using Chocolatey\nchoco install googlechrome -y --no-progress --limit-output --ignore-checksums\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#security-note","title":"Security Note","text":"<ul> <li>The storage account used for downloads should be read-only and not contain sensitive data.</li> <li>All user accounts and credentials should be masked in documentation.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#additional-notes","title":"Additional Notes","text":"<ul> <li>All scripts (e.g., <code>avd.ps1</code>) should be reviewed for security and idempotency.</li> <li>Use service principals or user accounts with Contributor or higher permissions for these operations. Mask actual usernames in documentation.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#related-articles","title":"Related Articles","text":"<ul> <li>Part 2: Deploying AVD Desktops</li> </ul>"},{"location":"avd-publish-part2/","title":"Part 2: Deploying Azure Virtual Desktop (AVD) Desktops","text":"<p>This article provides a detailed, step-by-step guide to deploying AVD VMs from a custom image in the Azure Compute Gallery, joining them to a domain, and ensuring a successful deployment. For the image creation process, see Part 1: Building and Publishing an AVD Image.</p>"},{"location":"avd-publish-part2/#overview","title":"Overview","text":"<p>The process involves: - Reviewing and updating parameter files for deployment - Deploying AVD VMs using Bicep and Azure CLI - Joining VMs to the domain using a privileged service account - Post-deployment validation and best practices</p>"},{"location":"avd-publish-part2/#step-1-review-and-update-parameter-file","title":"Step 1: Review and Update Parameter File","text":"<p>Before deploying, ensure your parameter file (e.g., <code>avdp.json</code>) is up to date with the correct image reference, VM size, network settings, and domain join information.</p>"},{"location":"avd-publish-part2/#example-parameter-file-avdpjson","title":"Example Parameter File (<code>avdp.json</code>)","text":"<pre><code>{\n  \"resource_group\": { \"value\": \"rg-gdep-peus-avd-pools\" },\n  \"nic_subnet_resourcegroup\": { \"value\": \"rg-gdep-peus-vnets\" },\n  \"nic_vnet_name\": { \"value\": \"GDEP_VNET_PROD\" },\n  \"nic_subnet_name\": { \"value\": \"Prod_AVD_FS_Subnet\" },\n  \"vm_name\": { \"value\": \"AZGDEPENG\" },\n  \"adminusername\": { \"value\": \"avdadmin\" },\n  \"hostpoolname\": { \"value\": \"GDEP_Engineering\" },\n  \"adminPassword\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"new-vm-password\"\n    }\n  },\n  \"adadminusername\": { \"value\": \"service-account@yourdomain.com\" },\n  \"adadminPassword\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"ad-join-password\"\n    }\n  },\n  \"hostpoolregistrationkey\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"avd-host-pool-reg-key\"\n    }\n  },\n  \"vmCount\": { \"value\": 1 }\n}\n</code></pre> <p>Parameter Explanations: - <code>resource_group</code>, <code>nic_subnet_resourcegroup</code>, <code>nic_vnet_name</code>, <code>nic_subnet_name</code>: Networking and resource group settings. - <code>vm_name</code>: Prefix for VM names. - <code>adminusername</code>/<code>adminPassword</code>: Local admin credentials for the VM (use Key Vault for secrets). - <code>adadminusername</code>/<code>adadminPassword</code>: Service account with delegated rights to join computers to the domain. Do not use personal admin accounts; use a dedicated service account. - <code>hostpoolname</code>, <code>hostpoolregistrationkey</code>: AVD host pool registration details. - <code>vmCount</code>: Number of VMs to deploy.</p>"},{"location":"avd-publish-part2/#step-2-full-bicep-code-for-avd-deployment","title":"Step 2: Full Bicep Code for AVD Deployment","text":"<p>Below is the full Bicep code for deploying AVD VMs, joining them to the domain, and registering them with the AVD host pool.</p> <pre><code>param location string = resourceGroup().location\nparam resource_group string\nparam nic_subnet_resourcegroup string\nparam nic_vnet_name string\nparam nic_subnet_name string\nparam vm_name string\nparam adminusername string\nparam adadminusername string\nparam hostpoolregistrationkey string\nparam hostpoolname string  \nparam vmCount int\n\n@secure()\nparam adminPassword string\n@secure()\nparam adadminPassword string\n\nvar applicationtag = 'Virtual Desktop'\nvar environmetag = 'Production'\nvar domain = 'yourdomain.com'\nvar ou2add2 = 'OU=Computers,OU=Standard,OU=Virtual Desktops,OU=Azure,OU=GD,DC=yourdomain,DC=com'\n\n@description('Create NICs for each VM')\nmodule nicgdepavd '../../../networking/nic/nic.bicep' = [for i in range(0, vmCount):{\n  name: '${deployment().name}-nic${i}'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: 'nicgdepp${location}${vm_name}${i}'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      ipConfigurations: [\n        {\n          name: 'ipconfig'\n          properties: {\n            privateIPAllocationMethod: 'Dynamic'\n            subnet: {\n              id: resourceId(\n                '${nic_subnet_resourcegroup}',\n                'Microsoft.Network/virtualNetworks/subnets',\n                '${nic_vnet_name}',\n                '${nic_subnet_name}'\n              )\n            }\n          }\n        }\n      ]\n    }\n  }\n}]\n\n@description('Create the AVD VM(s)')\nmodule vmgdepavd '../vm.bicep' = [for i in range(0, vmCount): {\n  name: '${deployment().name}-${i}'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: '${vm_name}-${i}'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      hardwareProfile: { vmSize: 'Standard_NV16as_v4' }\n      storageProfile: {\n        imageReference: {\n          id: resourceId('Microsoft.Compute/galleries/images', 'GDEP_Azure_Compute_Gallery', 'Win11AVD')\n        }\n        osDisk: {\n          createOption: 'FromImage'\n          diskSizeGB: 512\n          managedDisk: { storageAccountType: 'Premium_LRS' }\n        }\n      }\n      osProfile: {\n        computerName: '${vm_name}-${i}'\n        adminUsername: adminusername\n        adminPassword: adminPassword\n        windowsConfiguration: {\n          provisionVMAgent: true\n          enableAutomaticUpdates: true\n        }\n      }\n      networkProfile: {\n        networkInterfaces: [\n          {\n            id: nicgdepavd[i].outputs.id\n            properties: { deleteOption: 'Delete' }\n          }\n        ]\n      }\n      diagnosticsProfile: { bootDiagnostics: { enabled: true } }\n      licenseType: 'Windows_Client'\n    }\n  }\n}]\n\n@description('Add Custom Script Extension for Kofax')\nresource extkofaxcustomscript 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/CustomScriptExtension'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    publisher: 'Microsoft.Compute'\n    type: 'CustomScriptExtension'\n    typeHandlerVersion: '1.9'\n    autoUpgradeMinorVersion: true\n    settings: {\n      commandToExecute: 'powershell.exe -ExecutionPolicy Unrestricted -Command \"Invoke-WebRequest -Uri \\'https://storeusgdepsoftware.blob.core.windows.net/software4avd/avdkofax.ps1\\' -OutFile \\'C:\\\\software\\\\avdkofax.ps1\\'; Set-Location -Path \\'C:\\\\software\\'; .\\\\avdkofax.ps1\"'\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n  ]\n}]\n\n@description('Join VM(s) to the domain')\nresource extadd2adcustomscript 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/joindomain'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    autoUpgradeMinorVersion: true\n    publisher: 'Microsoft.Compute'\n    type: 'JsonADDomainExtension'\n    typeHandlerVersion: '1.3'\n    settings: {\n      name: domain\n      ouPath: ou2add2\n      user: adadminusername\n      restart: true\n      options: '3'\n    }\n    protectedSettings: {\n      password: adadminPassword\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n    extkofaxcustomscript[i]\n  ]\n}]\n\n@description('Register VM(s) with AVD Host Pool')\nresource vmExtension 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/Microsoft.PowerShell.DSC'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    autoUpgradeMinorVersion: true\n    publisher: 'Microsoft.Powershell'\n    type: 'DSC'\n    typeHandlerVersion: '2.73'\n    settings: {\n      modulesUrl: 'https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_1.0.02714.342.zip'\n      configurationFunction: 'Configuration.ps1\\\\AddSessionHost'\n      properties: {\n        hostPoolName: hostpoolname\n        registrationInfoToken: hostpoolregistrationkey\n        aadJoin: false\n        UseAgentDownloadEndpoint: true\n      }\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n    extkofaxcustomscript[i]\n    extadd2adcustomscript[i]\n  ]\n}]\n</code></pre>"},{"location":"avd-publish-part2/#step-3-deploy-the-avd-vms-using-azure-cli","title":"Step 3: Deploy the AVD VM(s) Using Azure CLI","text":"<p>Use the following command to deploy the VM(s):</p> <pre><code>az deployment group create --name gdepiacavd --resource-group rg-gdep-peus-avd-pools --template-file ./compute/vm/avd/avd.bicep --parameters @./compute/vm/avd/avdp.json\n</code></pre> <ul> <li>This command deploys the VM(s) using the custom image and parameters.</li> </ul>"},{"location":"avd-publish-part2/#step-4-domain-join-and-permissions","title":"Step 4: Domain Join and Permissions","text":"<ul> <li>The <code>adadminusername</code> must be a service account with delegated permissions to join computers to the specified OU in Active Directory.</li> <li>Do not use personal admin accounts; use a dedicated, least-privilege service account.</li> <li>The deployment will use the provided credentials to join the VM to the domain and place it in the correct OU.</li> </ul>"},{"location":"avd-publish-part2/#step-5-post-deployment-validation","title":"Step 5: Post-Deployment Validation","text":"<ul> <li>Log in to the new VM(s) as the local admin account.</li> <li>Verify log files (e.g., in <code>C:\\Software</code>) to ensure a clean build.</li> <li>Expand the disk to 512 GB if required.</li> <li>(Optional) Install additional software as needed.</li> <li>For validation desktops, use the correct host pool name and ensure only authorized users have access.</li> </ul>"},{"location":"avd-publish-part2/#security-and-best-practices","title":"Security and Best Practices","text":"<ul> <li>Always use secure methods (Key Vault references) for passwords and sensitive data.</li> <li>Review all scripts and templates for security and compliance.</li> <li>Mask all sensitive information in documentation.</li> <li>Use service accounts for domain join, not personal accounts.</li> </ul>"},{"location":"avd-publish-part2/#related-articles","title":"Related Articles","text":"<ul> <li>Part 1: Building and Publishing an Custom Image</li> </ul>"},{"location":"azure-ad-certificate/","title":"Certificate Based Authentication","text":""},{"location":"azure-ad-certificate/#certificate-based-authentication-for-azure-ad-why-and-how","title":"Certificate-Based Authentication for Azure AD: Why and How","text":""},{"location":"azure-ad-certificate/#creating-a-certificate-for-azure-ad-authentication","title":"Creating a Certificate for Azure AD Authentication","text":"<p>To use certificate-based authentication with Azure Active Directory (Azure AD), you first need to generate a certificate. Certificates provide a secure, manageable, and standards-based way to authenticate applications. A <code>.pfx</code> certificate may be required because Azure AD expects a certificate in Personal Information Exchange (PFX) format when uploading via the portal or for certain SDKs. The <code>.pfx</code> file contains both the public and private keys, protected by a password, and is suitable for import/export scenarios.</p>"},{"location":"azure-ad-certificate/#steps-to-generate-a-certificate-using-openssl","title":"Steps to Generate a Certificate Using OpenSSL","text":"<ol> <li>Generate a Private Key: <pre><code>openssl genrsa -out my-app-auth.key 2048\n</code></pre></li> <li>Create a Certificate Signing Request (CSR): <pre><code>openssl req -new -key my-app-auth.key -out my-app-auth.csr\n</code></pre></li> <li>Generate a Self-Signed Certificate: <pre><code>openssl x509 -req -days 730 -in my-app-auth.csr -signkey my-app-auth.key -out my-app-auth.crt\n</code></pre></li> <li>Export to PFX (if needed for Azure): <pre><code>openssl pkcs12 -export -out my-app-auth.pfx -inkey my-app-auth.key -in my-app-auth.crt\n</code></pre> <p>Note: The <code>.pfx</code> format is required if you want to upload the certificate via the Azure Portal or use it with some SDKs/tools. The <code>.crt</code> file is the public certificate, and the <code>.key</code> file is your private key (keep it secure!).</p> </li> </ol>"},{"location":"azure-ad-certificate/#uploading-the-certificate-to-your-entra-azure-ad-application","title":"Uploading the Certificate to Your Entra (Azure AD) Application","text":"<ol> <li>Go to the Microsoft Entra admin center and select Azure Active Directory.</li> <li>Navigate to App registrations and select your application.</li> <li>In the left menu, click Certificates &amp; secrets.</li> <li>Under Certificates, click Upload certificate.</li> <li>Select your <code>.crt</code> or <code>.pfx</code> file and upload it.</li> <li>After uploading, Azure will display the certificate thumbprint. Save this value for use in your application code.</li> </ol>"},{"location":"azure-ad-certificate/#assigning-permissions-to-the-application","title":"Assigning Permissions to the Application","text":"<p>After uploading the certificate, you must assign the necessary API permissions to your application:</p> <ol> <li>In your application's App registration page, go to API permissions.</li> <li>Click Add a permission and select the required Microsoft APIs (e.g., Microsoft Graph, Azure Service Management, etc.).</li> <li>Choose the appropriate permission type (Application or Delegated) and select the required permissions.</li> <li>Click Add permissions.</li> <li>If required, click Grant admin consent to approve the permissions for your organization.</li> </ol> <p>Note: The application will only be able to access resources for which it has been granted permissions. Make sure to review and assign only the permissions your app needs.</p>"},{"location":"azure-ad-certificate/#why-use-a-certificate-instead-of-an-application-secret","title":"Why Use a Certificate Instead of an Application Secret?","text":""},{"location":"azure-ad-certificate/#1-security","title":"1. Security","text":"<ul> <li>Application secrets are essentially passwords. They are susceptible to accidental exposure (e.g., in code repositories, logs, or configuration files).</li> <li>Certificates use asymmetric cryptography. The private key never leaves your environment, and only the public key is uploaded to Azure AD. This makes certificates much harder to compromise.</li> </ul>"},{"location":"azure-ad-certificate/#2-lifecycle-management","title":"2. Lifecycle Management","text":"<ul> <li>Secrets typically expire every 6-12 months, requiring regular rotation and updates in all dependent systems.</li> <li>Certificates can have longer lifespans (e.g., 1-2 years), and their expiration is easier to track and automate.</li> </ul>"},{"location":"azure-ad-certificate/#3-compliance-and-best-practices","title":"3. Compliance and Best Practices","text":"<ul> <li>Microsoft and most security frameworks recommend certificates for service-to-service authentication.</li> <li>Certificates support better auditing and can be managed centrally (e.g., via Azure Key Vault).</li> </ul>"},{"location":"azure-ad-certificate/#why-use-the-msal-library-and-not-a-specific-azure-sdk","title":"Why Use the MSAL Library (and Not a Specific Azure SDK)?","text":"<p>The MSAL (Microsoft Authentication Library) for Python is a lightweight, flexible library for acquiring tokens from Azure AD. It supports a wide range of authentication scenarios, including certificate-based authentication for confidential clients.</p> <ul> <li>Why MSAL?</li> <li>MSAL is the official library for handling authentication and token acquisition with Azure AD.</li> <li>It is not tied to a specific Azure service, making it ideal for generic authentication scenarios.</li> <li> <p>It supports advanced scenarios like certificate-based authentication, multi-tenant apps, and more.</p> </li> <li> <p>Why Not Use a Specific Azure SDK?</p> </li> <li>Some Azure SDKs (e.g., for Storage, Key Vault, etc.) provide their own authentication mechanisms, but they may not support all advanced scenarios or may require additional dependencies.</li> <li>Using MSAL directly gives you full control over the authentication flow and token management, and is more transparent for troubleshooting and customization.</li> </ul>"},{"location":"azure-ad-certificate/#code-example-certificate-based-authentication-in-python","title":"Code Example: Certificate-Based Authentication in Python","text":"<p>Below is the function used in this project to acquire an Azure AD access token using a certificate:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token_from_azure(client_id, authority, tenant_id, resource_scopes):\n    \"\"\"\n    Retrieves an access token from Azure Active Directory using a confidential client application.\n    This function uses certificate-based authentication to acquire an access token for the specified resource.\n    \"\"\"\n    try:\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n\n        app = ConfidentialClientApplication(\n            client_id=client_id,\n            authority=f\"{authority}{tenant_id}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n\n        result = app.acquire_token_for_client(scopes=resource_scopes)\n        if \"access_token\" in result:\n            return result[\"access_token\"]\n\n    except Exception as exception:\n        handle_global_exception(sys._getframe().f_code.co_name, exception)\n    finally:\n        pass\n</code></pre>"},{"location":"azure-ad-certificate/#key-points","title":"Key Points:","text":"<ul> <li>The private key is read from a secure file (<code>certs/*.key</code>).</li> <li>The certificate thumbprint and private key are passed to MSAL's <code>ConfidentialClientApplication</code>.</li> <li>No secrets or passwords are stored in code or configuration.</li> </ul>"},{"location":"azure-ad-certificate/#conclusion","title":"Conclusion","text":"<p>Certificate-based authentication is the recommended and most secure way to authenticate service applications with Azure AD. It reduces risk, simplifies management, and aligns with industry best practices. Migrating from secrets to certificates is straightforward and well-supported by both Azure and the MSAL Python library.</p>"},{"location":"azure-ad-certificate/#references","title":"References","text":"<ul> <li>MSAL Python Certificate Auth Sample</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> <li>OpenSSL Documentation</li> </ul>"},{"location":"azure-ad-user-devices/","title":"Retrieving Entra (Azure AD) User Device","text":""},{"location":"azure-ad-user-devices/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve a list of all devices registered in Microsoft Entra (Azure AD), including key device attributes and registered user information, using Python and the Microsoft Graph API. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"azure-ad-user-devices/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>requests</code></li> <li><code>msal</code> (for authentication, not shown here)</li> <li>An Azure AD application (service principal) with permissions to read device information</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-ad-user-devices/#constants-and-endpoints","title":"Constants and Endpoints","text":"<pre><code>AZURE_GRAPH_BETA = 'https://graph.microsoft.com/beta/'\nLIST_OF_AAD_DEVICE_ATTRIBUTES = [\n    'approximateLastSignInDateTime', 'deviceId', 'displayName', 'id',\n    'isCompliant', 'isManaged', 'manufacturer', 'model', 'operatingSystem',\n    'operatingSystemVersion', 'deviceOwnership', 'managementType'\n]\n</code></pre>"},{"location":"azure-ad-user-devices/#step-1-retrieve-device-inventory-from-entra-azure-ad","title":"Step 1: Retrieve Device Inventory from Entra (Azure AD)","text":""},{"location":"azure-ad-user-devices/#function-get_list_of_devices","title":"Function: <code>get_list_of_devices</code>","text":"<p>This function retrieves a list of devices from Entra (Azure AD), including key device attributes and registered user information, using the Microsoft Graph API.</p> <pre><code>def get_list_of_devices():\n    devices_list = []  # Initialize list to store device data\n    try:\n        LIST_OF_AAD_DEVICE_ATTRIBUTES = [\n            'approximateLastSignInDateTime', 'deviceId', 'displayName', 'id',\n            'isCompliant', 'isManaged', 'manufacturer', 'model', 'operatingSystem',\n            'operatingSystemVersion', 'deviceOwnership', 'managementType'\n        ]\n        selected_attributes = \",\".join(LIST_OF_AAD_DEVICE_ATTRIBUTES)\n        query_params = f\"$select={selected_attributes}&amp;$expand=registeredUsers\"\n        devices_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}devices?{query_params}\")\n        user_device = []\n        for device in devices_list:\n            registered_user = device.get('registeredUsers', [])\n            user_id = registered_user[0].get('id') if registered_user else None\n            user_ip = None  # Not assigned from the device dictionary, keeping as None\n            user_upn = registered_user[0].get('userPrincipalName') if registered_user else None\n            device_to_add = {\n                'approximateLastSignInDateTime': parse_iso_date(device.get('approximateLastSignInDateTime')),\n                'deviceId': device.get('deviceId'),\n                'displayName': device.get('displayName'),\n                'id': device.get('id'),\n                'isCompliant': device.get('isCompliant'),\n                'isManaged': device.get('isManaged'),\n                'manufacturer': device.get('manufacturer'),\n                'model': device.get('model'),\n                'operatingSystem': device.get('operatingSystem'),\n                'operatingSystemVersion': device.get('operatingSystemVersion'),\n                'deviceOwnership': device.get('deviceOwnership'),\n                'user_id': user_id,\n                'user_ip': user_ip,\n                'user_upn': user_upn,\n                'managementType': device.get('managementType'),\n            }\n            user_device.append(device_to_add)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return user_device\n</code></pre> <p>Explanation: - Builds a query to select device attributes and expand registered user information (<code>registeredUsers</code>). - Calls <code>execute_odata_query_get</code> to make the API request and handle pagination. - For each device, extracts key attributes and the first registered user's ID and UPN (if available). - Returns a list of device dictionaries, each including device and user information.</p>"},{"location":"azure-ad-user-devices/#supporting-function-execute_odata_query_get","title":"Supporting Function: <code>execute_odata_query_get</code>","text":"<p>This function is used to make authenticated, paginated requests to the Microsoft Graph API.</p> <pre><code>def execute_odata_query_get(urltoInvoke, token=''):\n    try:\n        localUserList = []\n        if not token:\n            acesstokenforClientapp = get_access_token_API_Access_AAD()\n        else:\n            acesstokenforClientapp = token\n        continueLooping = True\n        while continueLooping:\n            response = requests.get(\n                url=urltoInvoke,\n                headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n            )\n            if response.status_code == 429:  # Throttling response\n                retry_after = int(response.headers.get(\"Retry-After\", 5))\n                print(f\"Throttled! Retrying after {retry_after} seconds...\")\n                time.sleep(retry_after)\n                continue\n            if response.status_code == 401:  # Token expired or invalid\n                print(\"Token expired or invalid. Fetching a new one...\")\n                acesstokenforClientapp = get_access_token_API_Access_AAD()\n                response = requests.get(\n                    url=urltoInvoke,\n                    headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n                )\n                if response.status_code != 200:\n                    response.raise_for_status()\n            if response.status_code == 403:\n                raise Exception(f\"403 Forbidden: Access denied for URL {urltoInvoke}\")\n            if response.status_code != 200:\n                response.raise_for_status()\n            graph_data = response.json()\n            localUserList.extend(graph_data.get('value', []))\n            if \"@odata.nextLink\" in graph_data:\n                urltoInvoke = graph_data[\"@odata.nextLink\"]\n            else:\n                continueLooping = False\n        return localUserList\n    except Exception as e:\n        if hasattr(e, 'args') and e.args and '403 Forbidden' in str(e.args[0]):\n            raise\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre> <p>Explanation: - Handles authentication, pagination, throttling, and error handling for Microsoft Graph API requests. - Used by all higher-level functions to retrieve data from Entra (Azure AD).  For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p>"},{"location":"azure-ad-user-devices/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can programmatically retrieve a complete inventory of devices from Entra (Azure AD), including device details and registered user information, using Python and the Microsoft Graph API. This enables automated device inventory, compliance, and reporting workflows in your organization.</p> <p>For more details, see the Microsoft Graph API documentation.</p>"},{"location":"azure-ad-user/","title":"Retrieving Entra (Azure AD) Users, Group Membership, and License Assignments","text":""},{"location":"azure-ad-user/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve a list of all users from Microsoft Entra (Azure AD), including their group memberships and license assignments, using Python and the Microsoft Graph API. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"azure-ad-user/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>requests</code></li> <li><code>msal</code> (for authentication, not shown here)</li> <li>An Azure AD application (service principal) with permissions to read users, groups, and licenses</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-ad-user/#constants-and-endpoints","title":"Constants and Endpoints","text":"<pre><code>AZURE_GRAPH_BETA = 'https://graph.microsoft.com/beta/'\nLIST_OF_AAD_USER_ATTRIBUTES = [\n    'displayName', 'accountEnabled', 'userPrincipalName', 'licenseAssignmentStates',\n]\n</code></pre>"},{"location":"azure-ad-user/#step-1-retrieve-users-with-license-assignments-and-group-memberships","title":"Step 1: Retrieve Users with License Assignments and Group Memberships","text":""},{"location":"azure-ad-user/#function-get_users_licenseassignments_and_groups","title":"Function: <code>get_users_licenseassignments_and_groups</code>","text":"<p>This function retrieves a list of users from Entra (Azure AD), including their license assignment states and group memberships, using the Microsoft Graph API.</p> <pre><code>def get_users_licenseassignments_and_groups():\n    user_list = []  # Initialize list to store user data\n    try:\n        LIST_OF_AAD_USER_ATTRIBUTES = [\n            'displayName', 'accountEnabled', 'userPrincipalName', 'licenseAssignmentStates',\n        ]\n        selected_attributes = \",\".join(LIST_OF_AAD_USER_ATTRIBUTES)\n        query_params = f\"$select={selected_attributes}\"\n        query_params += \"&amp;$expand=memberOf\"\n        user_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}users?{query_params}\")\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return user_list\n</code></pre> <p>Explanation: - Builds a query to select user attributes and expand group memberships (<code>memberOf</code>). - Calls <code>execute_odata_query_get</code> to make the API request and handle pagination. - Returns a list of user dictionaries with license and group data.</p>"},{"location":"azure-ad-user/#step-2-retrieve-all-groups","title":"Step 2: Retrieve All Groups","text":""},{"location":"azure-ad-user/#function-get_list_of_groups","title":"Function: <code>get_list_of_groups</code>","text":"<p>This function retrieves all groups from Entra (Azure AD).</p> <pre><code>def get_list_of_groups():\n    group_list = []  # Initialize list to store group data\n    try:\n        group_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}groups?\")\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return group_list\n</code></pre> <p>Explanation: - Calls the Microsoft Graph <code>/groups</code> endpoint to retrieve all groups. - Uses <code>execute_odata_query_get</code> for API calls and pagination. - Returns a list of group dictionaries.</p>"},{"location":"azure-ad-user/#step-3-combine-user-license-and-group-data","title":"Step 3: Combine User, License, and Group Data","text":""},{"location":"azure-ad-user/#function-get_users_with_license_and_groups","title":"Function: <code>get_users_with_license_and_groups</code>","text":"<p>This function combines user, license, and group data into a unified list.</p> <pre><code>def get_users_with_license_and_groups():\n    final_list = []  # Final list of dictionaries\n    SKU_MAPPING = {\n        \"ee02fd1b-340e-4a4b-b355-4a514e4c8943\": \"Exchange Online Archiving\",\n        \"05e9a617-0261-4cee-bb44-138d3ef5d965\": \"365 E3\",\n        # ... (other SKU mappings) ...\n    }\n    try:\n        users = get_users_licenseassignments_and_groups()\n        groups = get_list_of_groups()\n        for user in users:\n            # Extract license assignments and group memberships\n            license_assignments = []\n            group_memberships = []\n            # Parse license assignments\n            for license in user.get('licenseAssignmentStates', []):\n                sku_id = license.get('skuId')\n                sku_name = SKU_MAPPING.get(sku_id, sku_id)\n                license_assignments.append({\n                    'sku_id': sku_id,\n                    'sku_name': sku_name,\n                    'assigned_by_group': license.get('assignedByGroup', None),\n                })\n            # Parse group memberships\n            for group in user.get('memberOf', []):\n                group_memberships.append({\n                    'group_id': group.get('id'),\n                    'display_name': group.get('displayName'),\n                })\n            final_list.append({\n                'user_principal_name': user.get('userPrincipalName'),\n                'display_name': user.get('displayName'),\n                'account_enabled': user.get('accountEnabled'),\n                'license_assignments': license_assignments,\n                'group_memberships': group_memberships,\n            })\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return final_list\n</code></pre> <p>Explanation: - Calls the previous two functions to get users and groups. - Maps license SKUs to human-readable names. - Extracts and structures license and group membership data for each user. - Returns a list of user dictionaries with all relevant information.</p>"},{"location":"azure-ad-user/#step-4-high-level-orchestration","title":"Step 4: High-Level Orchestration","text":""},{"location":"azure-ad-user/#function-get_list_of_users_with_license_and_groups","title":"Function: <code>get_list_of_users_with_license_and_groups</code>","text":"<p>This function orchestrates the retrieval and structuring of user, license, and group data.</p> <pre><code>def get_list_of_users_with_license_and_groups():\n    user_groups = []\n    user_license = []\n    user_lic_and_groups = get_users_with_license_and_groups()\n    for user in user_lic_and_groups:\n        for membership in user['group_memberships']:\n            user_groups.append({\n                'user_principal_name': user['user_principal_name'],\n                'group_id': membership['group_id'],\n                'group_display_name': membership['display_name'],\n            })\n        for licassignment in user['license_assignments']:\n            user_license.append({\n                'user_principal_name': user['user_principal_name'],\n                'sku_id': licassignment['sku_id'],\n                'sku_name': licassignment['sku_name'],\n                'assigned_by_group': licassignment['assigned_by_group'],\n            })\n    return user_groups, user_license\n</code></pre> <p>Explanation: - Calls <code>get_users_with_license_and_groups</code> to get the unified user data. - Flattens group memberships and license assignments into separate lists for easy processing or storage. - Returns two lists: one for user-group relationships, one for user-license assignments.</p>"},{"location":"azure-ad-user/#supporting-function-execute_odata_query_get","title":"Supporting Function: <code>execute_odata_query_get</code>","text":"<p>This function is used throughout to make authenticated, paginated requests to the Microsoft Graph API.</p> <pre><code>def execute_odata_query_get(urltoInvoke, token=''):\n    try:\n        localUserList = []\n        if not token:\n            acesstokenforClientapp = get_access_token_API_Access_AAD()\n        else:\n            acesstokenforClientapp = token\n        continueLooping = True\n        while continueLooping:\n            response = requests.get(\n                url=urltoInvoke,\n                headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n            )\n            if response.status_code == 429:  # Throttling response\n                retry_after = int(response.headers.get(\"Retry-After\", 5))\n                print(f\"Throttled! Retrying after {retry_after} seconds...\")\n                time.sleep(retry_after)\n                continue\n            if response.status_code == 401:  # Token expired or invalid\n                print(\"Token expired or invalid. Fetching a new one...\")\n                acesstokenforClientapp = get_access_token_API_Access_AAD()\n                response = requests.get(\n                    url=urltoInvoke,\n                    headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n                )\n                if response.status_code != 200:\n                    response.raise_for_status()\n            if response.status_code == 403:\n                raise Exception(f\"403 Forbidden: Access denied for URL {urltoInvoke}\")\n            if response.status_code != 200:\n                response.raise_for_status()\n            graph_data = response.json()\n            localUserList.extend(graph_data.get('value', []))\n            if \"@odata.nextLink\" in graph_data:\n                urltoInvoke = graph_data[\"@odata.nextLink\"]\n            else:\n                continueLooping = False\n        return localUserList\n    except Exception as e:\n        if hasattr(e, 'args') and e.args and '403 Forbidden' in str(e.args[0]):\n            raise\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre> <p>Explanation: - Handles authentication, pagination, throttling, and error handling for Microsoft Graph API requests. - Used by all higher-level functions to retrieve data from Entra (Azure AD).  For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p>"},{"location":"azure-ad-user/#conclusion","title":"Conclusion","text":"<p>By following this step-by-step approach, you can programmatically retrieve all users from Entra (Azure AD), along with their group memberships and license assignments, using Python and the Microsoft Graph API. The modular design allows for easy extension and integration into enterprise automation workflows.</p> <p>For more details, see the Microsoft Graph API documentation.</p>"},{"location":"azure-bcpdr-github-action/","title":"Azure BCP/DR with GitHub Actions: Fully Automated Disaster Recovery","text":"<p>This article demonstrates how to automate the entire Azure Business Continuity/Disaster Recovery (BCP/DR) process using GitHub Actions. If you want to understand the step-by-step process, rationale, and all code involved, first review:</p> <ul> <li>Part 1: Resource Group, Storage, and Network Foundation</li> <li>Part 2: Compute, Firewall, VPN, and Restore</li> </ul> <p>Those articles walk through each step and code block manually. Here, you will see how to run the same process end-to-end with a single click using GitHub Actions.</p>"},{"location":"azure-bcpdr-github-action/#why-automate-with-github-actions","title":"Why Automate with GitHub Actions?","text":"<ul> <li>Consistency: Every DR run is identical and repeatable.</li> <li>Speed: Deploy all infrastructure and restore VMs with minimal manual intervention.</li> <li>Auditability: All actions are logged in GitHub for compliance and troubleshooting.</li> <li>Integration: Easily tie into your existing CI/CD and approval workflows.</li> </ul>"},{"location":"azure-bcpdr-github-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Service Principal with Owner rights on the subscription (see Part 1).</li> <li>Manual creation of the DR deployment resource group (<code>dr-rg-gdep-pwus-deployment</code>) in West US.</li> <li>Valid and up-to-date VM list JSON files.</li> </ul>"},{"location":"azure-bcpdr-github-action/#overview-of-the-automated-process","title":"Overview of the Automated Process","text":"<p>The automation is split into three GitHub Actions workflows:</p> <ol> <li>BCP-DR Infrastructure: Deploys all Azure infrastructure (resource groups, storage, network, firewall, VPN, etc.)</li> <li>BCP-DR Restore VMs: Restores VMs from backup into the DR region.</li> <li>BCP-DR Attach NIC(s): Ensures restored VMs have the correct NICs and IPs.</li> </ol> <p>Each workflow can be triggered manually from the GitHub Actions tab.</p>"},{"location":"azure-bcpdr-github-action/#step-1-deploy-infrastructure-with-github-actions","title":"Step 1: Deploy Infrastructure with GitHub Actions","text":"<p>Workflow File: <code>.github/workflows/bcpdrinfrastructure.yml</code></p> <pre><code>name: 01-BCP-DR Build Infrastructure\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      # ...existing code for deploying Bicep files for all resources (see full YAML above)...\n</code></pre> <p>Explanation: - This workflow deploys all BCP/DR infrastructure using the same Bicep files as in Part 1 and Part 2. - Each step uses the <code>azure/arm-deploy@v1</code> action to deploy a specific Bicep template. - You can select the environment (Development or Production) when running the workflow.</p>"},{"location":"azure-bcpdr-github-action/#step-2-restore-vms-in-the-dr-region","title":"Step 2: Restore VMs in the DR Region","text":"<p>Workflow File: <code>.github/workflows/bcpdrvms.yml</code></p> <pre><code>name: 02-BCP-DR Restore VMs\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure-restore-vms:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      - name: Install PowerShell\n        run: |\n          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n          sudo apt-get update\n          sudo apt-get install -y powershell\n      - name: Install Azure CLI\n        run: |\n          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n      - name: Create the VM(s) based on vmlist1.json\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        run: |\n          pwsh ./scripts/bcpdr/vm/restorevms.ps1 -restorediskonly \"false\" -numberofhours2wait 5 -vmlist './scripts/bcpdr/vm/vmlist.json'\n        env:\n          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}\n</code></pre> <p>Explanation: - This workflow restores VMs from backup using the provided PowerShell script and VM list. - Installs PowerShell Core and Azure CLI as needed. - All restore operations are logged in GitHub Actions.</p>"},{"location":"azure-bcpdr-github-action/#step-3-attach-nics-to-restored-vms","title":"Step 3: Attach NICs to Restored VMs","text":"<p>Workflow File: <code>.github/workflows/bcpdrvmnics.yml</code></p> <pre><code>name: 03-BCP-DR Attach NIC(s)\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure-restore-vmnics:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      - name: Install PowerShell\n        run: |\n          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n          sudo apt-get update\n          sudo apt-get install -y powershell\n      - name: Install Azure CLI\n        run: |\n          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n      - name: Create and Attach NIC(s) to restored VM(s)\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        run: |\n          pwsh ./scripts/bcpdr/vm/attachnics.ps1 -vaultname \"rsv-prod-eus-01\" -vaultresourcegroupname \"rg-gdep-peus-backup\" -vmlist './scripts/bcpdr/vm/vmlist.json'\n        env:\n          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}\n</code></pre> <p>Explanation: - This workflow ensures that all restored VMs have the correct NICs and IP addresses, matching the original environment. - Can be run multiple times to ensure NICs are correct.</p>"},{"location":"azure-bcpdr-github-action/#how-to-run-the-workflows","title":"How to Run the Workflows","text":"<ol> <li>Go to your GitHub repository's Actions tab.</li> <li>Select the workflow you want to run (Infrastructure, Restore VMs, Attach NICs).</li> <li>Click Run workflow, select the environment, and start the workflow.</li> <li>Monitor progress and logs directly in GitHub.</li> </ol>"},{"location":"azure-bcpdr-github-action/#summary","title":"Summary","text":"<ul> <li>This approach automates the entire Azure BCP/DR process described in Part 1 and Part 2.</li> <li>All code, infrastructure, and restore steps are executed via GitHub Actions for speed, repeatability, and auditability.</li> </ul> <p>Ready to automate your DR? Trigger your first workflow in GitHub Actions!</p>"},{"location":"azure-bcpdr-part1/","title":"Azure BCP/DR with Backup &amp; Restore: Part 1 \u2013 Resource Group, Storage, and Network Foundation","text":"<p>This multi-part technical blog series walks you through a practical, cost-effective approach to Business Continuity and Disaster Recovery (BCP/DR) in Azure using backup and restore, rather than Azure Site Recovery. The scenario targets restoring all critical infrastructure from Azure East (primary) to Azure West (DR region). Each step is explained with code and rationale, so you can duplicate this in your own environment.</p> <p>Part 2: Compute, Firewall, and Final Steps</p>"},{"location":"azure-bcpdr-part1/#why-this-approach","title":"Why This Approach?","text":"<ul> <li>Cost-Effective: Backup and restore avoids the ongoing costs of Azure Site Recovery (ASR) replication.</li> <li>Simplicity: You control what is restored and when, with clear, auditable steps.</li> <li>RTO/RPO: Recovery Time Objective (RTO) and Recovery Point Objective (RPO) are acceptable for many workloads, as restore times are predictable and backups are recent.</li> <li>Flexibility: You can restore to any region, in this case from East to West US.</li> </ul>"},{"location":"azure-bcpdr-part1/#step-1-create-the-target-resource-group-manual","title":"Step 1: Create the Target Resource Group (Manual)","text":"<p>This is the only step that must be done manually in the Azure Portal or CLI. It creates the DR resource group in the target region (West US).</p> <ul> <li>Resource Group Name: <code>dr-rg-gdep-pwus-deployment</code></li> <li>Region: West US</li> <li>Tags: Infrastructure, Disaster Recovery</li> </ul> <p>Command: <pre><code>az group create --name dr-rg-gdep-pwus-deployment --location westus --tags \"Purpose=Infrastructure\" \"Type=DisasterRecovery\"\n</code></pre></p>"},{"location":"azure-bcpdr-part1/#step-2-deploy-resource-groups-via-bicep","title":"Step 2: Deploy Resource Groups via Bicep","text":"<p>This step uses a Bicep template to deploy any additional resource groups needed for the DR environment.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-rg --template-file ./rg/rg_main.bicep --resource-group dr-rg-gdep-pwus-deployment\n</code></pre></p> <p>Bicep Code: <code>rg_main.bicep</code> <pre><code>param location string = 'westus'\nvar environmetag = 'Disaster Recovery'\n\nparam resourcegroups2create array = [\n  'dr-rg-gdep-pwus-infrastructure'\n  'dr-rg-gdep-pwus-vnets'\n  'dr-rg-gdep-pwus-fortinet'\n  'dr-rg-gdep-pwus-meraki-sdwan'\n]\n\nmodule rggdepwus './rg.bicep' = [for rggroupname in resourcegroups2create: {\n  name: '${deployment().name}-${rggroupname}'\n  scope:subscription()\n  params: {\n    name: rggroupname\n    location: location\n    tags: {Application:'Infrastructure',Environment: environmetag}\n    }\n}]\n</code></pre></p> <p>Supporting Bicep: <code>rg.bicep</code> <pre><code>targetScope='subscription'\n\nparam name string \nparam location string\nparam tags object\n\nresource rggdepdrwus 'Microsoft.Resources/resourceGroups@2022-09-01' = {\n  name: name\n  location: location\n  tags: tags\n}\noutput resourceGroupName string = rggdepdrwus.name\n</code></pre></p> <p>Explanation: - <code>rg_main.bicep</code> loops through a list of DR resource group names and deploys each using the <code>rg.bicep</code> module. - <code>rg.bicep</code> creates a resource group at the subscription level with the specified name, location, and tags. - This ensures all required DR resource groups are created consistently and tagged for easy management.</p>"},{"location":"azure-bcpdr-part1/#step-3-create-staging-storage-account","title":"Step 3: Create Staging Storage Account","text":"<p>Deploy a storage account for staging backups and other DR artifacts.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-sa-staging --resource-group dr-rg-gdep-pwus-deployment --template-file ./storage/sa/sa_staging.bicep\n</code></pre></p> <p>Bicep Code: <code>sa_staging.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\n\n@description('This Storage Account is used as a Staging account to restore VM(s)')\nmodule sagdepdrwusstaging './sa.bicep' = {\n  name: '${deployment().name}-sa-gdep-pwus-staging'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'storegdeppwusstaging'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      accessTier: 'Hot'\n      allowBlobPublicAccess: false\n      allowCrossTenantReplication: false\n      allowSharedKeyAccess: true\n      defaultToOAuthAuthentication: false\n      dnsEndpointType: 'Standard'\n      encryption: {\n        keySource: 'Microsoft.Storage'\n        requireInfrastructureEncryption: false\n        services: {\n          blob: {\n            enabled: true\n            keyType: 'Account'\n          }\n          file: {\n            enabled: true\n            keyType: 'Account'\n          }\n        }\n      }\n      largeFileSharesState: 'Enabled'\n      minimumTlsVersion: 'TLS1_2'\n      networkAcls: {\n        bypass: 'AzureServices'\n        defaultAction: 'Allow'\n        ipRules: []\n        virtualNetworkRules: []\n      }\n      publicNetworkAccess: 'Enabled'\n      supportsHttpsTrafficOnly: true\n    }\n  }\n}\n</code></pre></p> <p>Supporting Bicep: <code>sa.bicep</code> <pre><code>param location string\nparam name string\nparam tags object\nparam properties object\n\nresource sagdepdrwus 'Microsoft.Storage/storageAccounts@2023-04-01' = {\n  name: name\n  location: location\n  tags: tags\n  kind: 'StorageV2'\n  sku: {\n    name: 'Standard_LRS'\n  }\n  properties: properties\n}\n\nresource sagdepdrwusblobservices 'Microsoft.Storage/storageAccounts/blobServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n    deleteRetentionPolicy: {\n      allowPermanentDelete: false\n      enabled: false\n    }\n  }\n}\n\nresource sagdepdrwusfileservices 'Microsoft.Storage/storageAccounts/fileServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n    protocolSettings: {\n      smb: {}\n    }\n  }\n}\n\nresource sagdepdrwusqueueservices 'Microsoft.Storage/storageAccounts/queueServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n  }\n}\n\nresource sagdepdrwustableservices 'Microsoft.Storage/storageAccounts/tableServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - <code>sa_staging.bicep</code> deploys a storage account for DR staging in the infrastructure resource group, with secure settings and encryption. - The <code>sa.bicep</code> module provisions the storage account and all required services (blob, file, queue, table). - This storage account is used for storing backup files, scripts, and logs during the DR restore process.</p>"},{"location":"azure-bcpdr-part1/#step-4-create-network-security-groups-nsgs","title":"Step 4: Create Network Security Groups (NSGs)","text":"<p>Deploy NSGs to secure your DR environment.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-nsg --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/nsg/nsg_main.bicep\n</code></pre></p> <p>Bicep Code: <code>nsg_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\n\n@description('Used By Most of our Subnets')\nmodule nsggdepdrwusdefault './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-default'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-default'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {}\n  }\n}\n@description('Used By Meraki Public Subnet')\nmodule nsggdepdrwusmeraki './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-meraki'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-meraki'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      securityRules: [\n        {\n          name: 'AllowAnyCustom443Inbound'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            description: 'Per Abhishek this may be required when (if) we enable Cisco Anyconnect'\n            protocol: '*'\n            sourcePortRange: '*'\n            destinationPortRange: '*'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 100\n            direction: 'Inbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n        {\n          name: 'AllowAnyCustom443InboundUDP'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            protocol: 'UDP'\n            sourcePortRange: '443'\n            destinationPortRange: '443'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 110\n            direction: 'Inbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n        {\n          name: 'AllowAny'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            protocol: '*'\n            sourcePortRange: '*'\n            destinationPortRange: '*'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 120\n            direction: 'Outbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n      ]\n    }\n  }\n}\n@description('Used By Fortinet Firewall')\nmodule nsggdepdrwusfortinet './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-fortinet'\n  scope: resourceGroup(fortinet_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-fortinet'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      securityRules: [\n        {\n          name: 'AllowAllOutbound'\n          properties: {\n            access: 'Allow'\n            description: 'Allow all out'\n            destinationAddressPrefix: '*'\n            destinationAddressPrefixes: []\n            destinationPortRange: '*'\n            destinationPortRanges: []\n            direction: 'Outbound'\n            priority: 105\n            protocol: '*'\n            sourceAddressPrefix: '*'\n            sourceAddressPrefixes: []\n            sourcePortRange: '*'\n            sourcePortRanges: []\n          }\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n        }\n        {\n          name: 'AllowAllInbound'\n          properties: {\n            access: 'Allow'\n            destinationAddressPrefix: '*'\n            destinationAddressPrefixes: []\n            destinationPortRange: '*'\n            destinationPortRanges: []\n            direction: 'Inbound'\n            priority: 110\n            protocol: '*'\n            sourceAddressPrefix: '*'\n            sourceAddressPrefixes: []\n            sourcePortRange: '*'\n            sourcePortRanges: []\n          }\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n        }\n      ]\n    }\n  }\n}\n</code></pre></p> <p>Supporting Bicep: <code>nsg.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource nsggdepdrwus 'Microsoft.Network/networkSecurityGroups@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\n</code></pre></p> <p>Explanation: - <code>nsg_main.bicep</code> deploys three NSGs: default, Meraki, and Fortinet, each with appropriate rules for their subnet roles. - The <code>nsg.bicep</code> module provisions the NSG with the specified rules and tags. - NSGs are critical for controlling traffic flow and securing your DR network.</p>"},{"location":"azure-bcpdr-part1/#step-5-create-route-tables","title":"Step 5: Create Route Tables","text":"<p>Deploy User Defined Route (UDR) tables for custom routing.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-rt --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/udr/udr_main.bicep\n</code></pre></p> <p>Bicep Code: <code>udr_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\n\n//var infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\nvar meraki_sdwan_rg_name = 'dr-rg-gdep-pwus-meraki-sdwan'\n\n@description('Used to route traffic intended to go to On Premise network from Azure Hub')\nmodule udrgdepdrwushub2onprem './udr.bicep' = {\n  name: '${deployment().name}-route-gdep-pwus-azurehub-onprem'\n  scope: resourceGroup(fortinet_rg_name)\n  params: {\n    name: 'route-gdep-pwus-azurehub-onprem'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      disableBgpRoutePropagation: false\n      routes: [ ... ]\n    }\n  }\n}\n// ...additional modules for all required UDRs (see full code in repo)...\n</code></pre></p> <p>Supporting Bicep: <code>udr.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource rtgdepdrwus 'Microsoft.Network/routeTables@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\n</code></pre></p> <p>Explanation: - <code>udr_main.bicep</code> deploys all required route tables for the DR environment, including routes for on-premises, spokes, and SDWAN. - The <code>udr.bicep</code> module provisions each route table with the specified routes and settings. - UDRs are essential for custom traffic flow and integration with firewalls and VPNs.</p>"},{"location":"azure-bcpdr-part1/#step-6-create-virtual-networks-and-subnets","title":"Step 6: Create Virtual Networks and Subnets","text":"<p>Deploy VNETs, subnets, and associate them with NSGs and UDRs.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-vnet --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/vnet/vnet_main.bicep\n</code></pre></p> <p>Bicep Code: <code>vnet_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\n\n/*\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\nvar meraki_sdwan_rg_name = 'dr-rg-gdep-pwus-meraki-sdwan'\n*/\nvar vnet_rg_name = 'dr-rg-gdep-pwus-vnets'\n\n@description('Hub Virtual Network with NVA namely Fortinet Firewall')\nmodule vnetgdepdrwusfortinet './vnet.bicep' = {\n  name: '${deployment().name}-vnet-gdep-pwus-fortinet'\n  scope: resourceGroup(vnet_rg_name)\n  params: {\n    name: 'vnet-gdep-pwus-fortinet'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      addressSpace: {\n        addressPrefixes: [ ... ]\n      }\n      dhcpOptions: {\n        dnsServers: [ ... ]\n      }\n      enableDdosProtection: false\n      subnets: [ ... ]\n    }\n  }\n}\n// ...additional modules for management and production spokes, and VNET peering (see full code in repo)...\n</code></pre></p> <p>Supporting Bicep: <code>vnet.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource vnetgdepdrwus 'Microsoft.Network/virtualNetworks@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\noutput virtualnetworkname string = vnetgdepdrwus.name\n</code></pre></p> <p>Explanation: - <code>vnet_main.bicep</code> provisions all required virtual networks and subnets for the DR environment, including hub, management, and production spokes. - Subnets are associated with NSGs and UDRs as needed, and VNET peering is configured for connectivity. - The <code>vnet.bicep</code> module provisions each VNET with the specified address spaces, subnets, and settings.</p> <p>Continue to Part 2: Compute, Firewall, and Final Steps</p>"},{"location":"azure-bcpdr-part2/","title":"Azure BCP/DR with Backup &amp; Restore: Part 2 \u2013 Compute, Firewall, VPN, and Restore","text":"<p>Back to Part 1: Resource Group, Storage, and Network Foundation</p> <p>This part continues the step-by-step BCP/DR process, focusing on compute, firewall, VPN, and restoring VMs. All steps are automated using Bicep and PowerShell, but executed manually for full control and auditability.</p>"},{"location":"azure-bcpdr-part2/#step-7-deploy-fortinet-firewall","title":"Step 7: Deploy Fortinet Firewall","text":"<p>Deploy the Fortinet firewall solution, including load balancers, NVAs, and all required public IPs and NICs.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-nva --resource-group dr-rg-gdep-pwus-deployment --template-file ./compute/vm/nva/nva.bicep --parameters ./compute/vm/nva/nva.json\n</code></pre></p> <p>Bicep Code: <code>nva.bicep</code> <pre><code>param location string = resourceGroup().location\nparam resource_group string\nparam adminusername string\n@secure()\nparam adminPassword string\nparam fortinetoffer string //Will need to check what is availabe at the time of DR (fortinet_fortigate-vm_v5)\nparam fortinetsku string //Will need to check what is availabe at the time of DR (fortinet_fg-vm_payg_2023)\n\n// ...existing code for variables and modules (see repo for full details)...\n// This Bicep deploys:\n// - Public IPs for Fortinet\n// - External and internal load balancers\n// - 8 NICs for Fortinet VMs\n// - 2 Fortinet NVA VMs with all required extensions\n</code></pre></p> <p>Parameter File: <code>nva.json</code> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"resource_group\": {\n      \"value\": \"dr-rg-gdep-pwus-fortinet\"\n    },\n    \"adminusername\": {\n      \"value\": \"nvafwadmin\"\n    },\n    \"fortinetoffer\": {\n      \"value\": \"fortinet_fortigate-vm_v5\"\n    },\n    \"fortinetsku\": {\n      \"value\": \"fortinet_fg-vm_payg_2023\"\n    },\n    \"adminPassword\": {\n      \"reference\": {\n        \"keyVault\": {\n          \"id\": \"/subscriptions/df8d9f29-f5c5-4e48-a004-21ea3b8a4834/resourceGroups/rg-gdep-peus-applications/providers/Microsoft.KeyVault/vaults/kv-gdep-peus-iac\"\n        },\n        \"secretName\": \"new-vm-password\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - The Bicep file provisions Fortinet NVAs, load balancers, public IPs, NICs, and all required networking for the DR firewall solution. - The parameter file provides values for resource group, admin credentials, and Fortinet offer/SKU.</p>"},{"location":"azure-bcpdr-part2/#step-8-fortinet-firewall-manual-steps","title":"Step 8: Fortinet Firewall Manual Steps","text":"<ul> <li>Obtain or create a backup of the NVA configuration (from SFTP or by running <code>backup.py</code> in the scripts folder).</li> <li>Update alias/host names as needed for the DR region.</li> <li>Import configuration via the Fortinet web interface for each NVA in West US.</li> <li>Confirm settings via Serial Console.</li> <li>Ensure Point-to-Site VPN is up before proceeding.</li> </ul>"},{"location":"azure-bcpdr-part2/#step-9-deploy-point-to-site-vpn","title":"Step 9: Deploy Point-to-Site VPN","text":"<p>Deploy the P2S VPN gateway and configuration.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-p2s --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/p2s/p2s.bicep --parameters ./networking/p2s/p2sp.json\n</code></pre></p> <p>Bicep Code: <code>p2s.bicep</code> <pre><code>/*\nLets create Point to Site VPN for users to connect into US West\nCreate PIP, VPN Gateway, for PIP we have no dependency but \nfor VPN Gateway we do.  As such notice \n*/\nparam location string = resourceGroup().location\nparam resource_group string\n\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar AADTenant = '${environment().authentication.loginEndpoint}${subscription().tenantId}'\n\nmodule pipgdepdrfw '../../networking/pip/pip.bicep' = {\n  name: '${deployment().name}-pipp2svpn'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: 'pip-gdep-pwus-p2svpn'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      publicIPAllocationMethod: 'Static'\n      publicIPAddressVersion: 'IPv4'\n    }\n  }\n}\n//Create VPN Gateway\nmodule vngwpoint2site './p2sngw.bicep' = {\n  name: '${deployment().name}-vngwpoint2site'\n  scope: resourceGroup('dr-rg-gdep-pwus-vnets')\n  params: {\n    name: 'vngw_gdep_pwus'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      enablePrivateIpAddress: false\n      ipConfigurations: [ ... ]\n      sku: {\n        name: 'VpnGw2'\n        tier: 'VpnGw2'\n      }\n      gatewayType: 'Vpn'\n      vpnType: 'RouteBased'\n      enableBgp: false\n      activeActive: false\n      vpnClientConfiguration: {\n        vpnClientAddressPool: {\n          addressPrefixes: ['10.27.48.0/22']\n        }\n        vpnClientProtocols: ['OpenVPN']\n        vpnAuthenticationTypes: ['AAD']\n        aadTenant: AADTenant\n        aadAudience: '41b23e61-6c1e-4545-b367-cd054e0ed4b4'\n        aadIssuer: '${'https://sts.windows.net/'}${subscription().tenantId}${'/'}'\n      }\n      customRoutes: {\n        addressPrefixes: [ ... ]\n      }\n      vpnGatewayGeneration: 'Generation2'\n    }\n  }\n}\n</code></pre></p> <p>Parameter File: <code>p2sp.json</code> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"resource_group\": {\n      \"value\": \"dr-rg-gdep-pwus-fortinet\"\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - The Bicep file provisions the VPN gateway, public IP, and all required settings for secure remote access to the DR environment.</p>"},{"location":"azure-bcpdr-part2/#step-10-restore-virtual-machines-or-disks","title":"Step 10: Restore Virtual Machines or Disks","text":"<p>Restore VMs using PowerShell Core. All VMs to be restored should be listed in <code>vmlist.json</code>.</p> <p>Command: <pre><code>pwsh ./scripts/bcpdr/vm/restorevms.ps1 -restorediskonly \"false\" -numberofhours2wait 5 -vmlist './scripts/bcpdr/vm/vmlist.json'\n</code></pre></p> <p>PowerShell Script: <code>restorevms.ps1</code> <pre><code># Boolean variable to indicate whether to restore disks only\n# If it is True then VM will also be created so be careful \nparam (\n    [string] $restorediskonly = \"false\",\n    [int] $numberofhours2wait = 2,\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json'\n)\n\n# ...existing code for restore logic (see repo for full details)...\n# This script restores VMs or disks from backup, as defined in the JSON file.\n# Set -restorediskonly to false to create VMs directly.\n# Adjust -numberofhours2wait as needed for your restore window.\n</code></pre></p> <p>Explanation: - The script restores VMs or disks from backup, as defined in the JSON file. - Set <code>-restorediskonly</code> to <code>false</code> to create VMs directly. - Adjust <code>-numberofhours2wait</code> as needed for your restore window.</p>"},{"location":"azure-bcpdr-part2/#step-11-attach-nics-to-restored-vms","title":"Step 11: Attach NICs to Restored VMs","text":"<p>Attach NICs to the restored VMs using PowerShell Core. This can be run multiple times safely.</p> <p>Command: <pre><code>pwsh ./scripts/bcpdr/vm/attachnics.ps1 -vaultname \"&lt;your-recovery-vault&gt;\" -vaultresourcegroupname \"&lt;your-backup-rg&gt;\" -vmlist './scripts/bcpdr/vm/vmlist.json'\n</code></pre></p> <p>PowerShell Script: <code>attachnics.ps1</code> <pre><code>param (\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json',\n    [string] $vaultname = 'rsv-prod-eus-01',\n    [string] $vaultresourcegroupname = 'rg-gdep-peus-backup'\n)\n\n# ...existing code for NIC attachment logic (see repo for full details)...\n# This script attaches network interfaces to the restored VMs.\n# Replace &lt;your-recovery-vault&gt; and &lt;your-backup-rg&gt; with your actual values.\n</code></pre></p> <p>Explanation: - The script attaches network interfaces to the restored VMs. - Replace <code>&lt;your-recovery-vault&gt;</code> and <code>&lt;your-backup-rg&gt;</code> with your actual values.</p>"},{"location":"azure-bcpdr-part2/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you need to re-run any step, you can safely delete previous deployments and start again.</li> <li>All steps are idempotent and can be repeated as needed.</li> </ul>"},{"location":"azure-bcpdr-part2/#summary-why-backup-restore","title":"Summary: Why Backup &amp; Restore?","text":"<ul> <li>Cost: No ongoing replication costs as with Azure Site Recovery (ASR).</li> <li>Control: You decide what to restore and when.</li> <li>RTO/RPO: For many workloads, restore times and backup frequency are sufficient.</li> <li>Flexibility: Restore to any region (here, from East to West US).</li> </ul> <p>When to Use This Approach: - When RTO/RPO requirements are not sub-minute. - When cost is a concern. - When you want full control over the DR process.</p> <p>When to Use Azure Site Recovery: - When you need near-instant failover and minimal data loss. - When you want fully automated DR with minimal manual steps.</p> <p>Back to Part 1</p>"},{"location":"azure-billing/","title":"Download Azure Bill","text":""},{"location":"azure-billing/#programmatically-downloading-and-storing-azure-billing-data","title":"Programmatically Downloading and Storing Azure Billing Data:","text":""},{"location":"azure-billing/#introduction","title":"Introduction","text":"<p>Automating the retrieval and storage of Azure billing data is essential for organizations seeking cost transparency and operational efficiency. This guide details a robust, production-grade approach to programmatically obtaining Azure billing data using Python, authenticating securely with certificates, and efficiently storing the results in a SQL Server database. </p>"},{"location":"azure-billing/#1-secure-authentication-acquiring-an-azure-access-token-with-certificates","title":"1. Secure Authentication: Acquiring an Azure Access Token with Certificates","text":"<p>The first step is to authenticate with Azure Active Directory (Azure AD) using certificate-based authentication. This is more secure than using client secrets and is recommended for automation and service-to-service scenarios. For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p> <p>Python Example:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token(client_id, authority, tenant_id, resource_scopes, cert_thumbprint, cert_key_path):\n    \"\"\"\n    Acquire an Azure AD access token using certificate-based authentication.\n    \"\"\"\n    with open(cert_key_path, \"r\") as key_file:\n        private_key = key_file.read()\n    app = ConfidentialClientApplication(\n        client_id=client_id,\n        authority=f\"{authority}{tenant_id}\",\n        client_credential={\n            \"thumbprint\": cert_thumbprint,\n            \"private_key\": private_key,\n        },\n    )\n    result = app.acquire_token_for_client(scopes=resource_scopes)\n    if \"access_token\" not in result:\n        raise Exception(f\"Token acquisition failed: {result}\")\n    return result[\"access_token\"]\n</code></pre> <ul> <li>Why certificates? They are more secure, support longer lifecycles, and are recommended for automation.</li> <li>MSAL Library: The Microsoft Authentication Library (MSAL) is used for token acquisition, providing flexibility and support for advanced scenarios.</li> </ul>"},{"location":"azure-billing/#2-generating-the-azure-cost-report-via-rest-api","title":"2. Generating the Azure Cost Report via REST API","text":"<p>Once authenticated, you can use the Azure Cost Management API to request a cost details report for your subscription. This involves making a POST request to the appropriate endpoint and polling until the report is ready.</p> <p>Python Example:</p> <pre><code>import requests\nimport time\nimport json\n\ndef generate_azure_cost_report(subscription_id, access_token, start_date, end_date, api_version=\"2022-05-01\"):\n    url = f\"https://management.azure.com/subscriptions/{subscription_id}/providers/Microsoft.CostManagement/generateCostDetailsReport?api-version={api_version}\"\n    payload = json.dumps({\"metric\": \"ActualCost\", \"timePeriod\": {\"start\": start_date, \"end\": end_date}})\n    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}\n    response = requests.post(url, headers=headers, data=payload)\n    # Poll until the report is ready\n    while response.status_code == 202:\n        location_url = response.headers.get('Location')\n        retry_after = int(response.headers.get('Retry-After', 30))\n        time.sleep(retry_after)\n        response = requests.get(url=location_url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to generate cost report: {response.status_code} - {response.text}\")\n    return response.json()\n</code></pre> <ul> <li>Polling: The API may return a 202 status, indicating the report is being generated. Poll the <code>Location</code> header until a 200 response is received.</li> <li>Error Handling: Always check for non-200 responses and handle errors appropriately.</li> </ul>"},{"location":"azure-billing/#3-downloading-the-cost-report-data","title":"3. Downloading the Cost Report Data","text":"<p>The response from the cost report API includes a manifest with one or more blob URLs. Download these blobs to obtain the actual cost data, typically in CSV format.</p> <p>Python Example:</p> <pre><code>import urllib3\n\ndef download_cost_report_blobs(manifest, output_path):\n    http = urllib3.PoolManager()\n    for blob in manifest['blobs']:\n        blob_url = blob['blobLink']\n        with open(output_path, 'wb') as out_file:\n            blob_response = http.request('GET', blob_url, preload_content=False)\n            out_file.write(blob_response.data)\n</code></pre> <ul> <li>Blob Download: Use a robust HTTP client (e.g., <code>urllib3</code>) to download the report data.</li> <li>Output: Save the CSV file to a secure, accessible location for further processing.</li> </ul>"},{"location":"azure-billing/#4-loading-the-cost-data-into-sql-server-efficiently","title":"4. Loading the Cost Data into SQL Server Efficiently","text":"<p>After downloading the cost report, the next step is to load the data into a SQL Server table. For large datasets, use a fast, batch insert method to optimize performance.</p> <p>Python Example:</p> <pre><code>import pyodbc\nimport csv\n\ndef load_csv_to_sql_server(csv_path, connection_string, table_name):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    with open(csv_path, 'r', encoding='utf-8-sig') as csvfile:\n        reader = csv.reader(csvfile)\n        columns = next(reader)  # Header row\n        insert_query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['?' for _ in columns])})\"\n        data = list(reader)\n        cursor.fast_executemany = True\n        cursor.executemany(insert_query, data)\n        conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Fast Insert: The <code>fast_executemany</code> flag in <code>pyodbc</code> enables high-performance bulk inserts.</li> <li>Schema Alignment: Ensure the CSV columns match the SQL table schema.</li> </ul>"},{"location":"azure-billing/#5-orchestrating-the-end-to-end-process","title":"5. Orchestrating the End-to-End Process","text":"<p>A typical workflow to automate Azure billing data retrieval and storage:</p> <pre><code>def fetch_and_update_azure_billing_data():\n    # Step 1: Get access token\n    access_token = get_access_token(\n        client_id=..., authority=..., tenant_id=..., resource_scopes=..., cert_thumbprint=..., cert_key_path=...\n    )\n    # Step 2: Generate cost report\n    report = generate_azure_cost_report(\n        subscription_id=..., access_token=access_token, start_date=..., end_date=...\n    )\n    # Step 3: Download report blob(s)\n    download_cost_report_blobs(report['manifest'], output_path=\"azure_billing.csv\")\n    # Step 4: Load into SQL Server\n    load_csv_to_sql_server(\n        csv_path=\"azure_billing.csv\", connection_string=..., table_name=\"AzureBilling\"\n    )\n</code></pre>"},{"location":"azure-billing/#6-additional-considerations","title":"6. Additional Considerations","text":"<ul> <li>Permissions: The Azure AD application must have the required API permissions (e.g., Cost Management Reader) and access to the subscription.</li> <li>Certificate Security: Store private keys securely and never commit them to source control.</li> <li>Error Handling: Implement robust error handling and logging for production use.</li> <li>Scheduling: Use a scheduler (e.g., cron, Azure Automation) to run the process regularly.</li> </ul>"},{"location":"azure-billing/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can securely and efficiently automate the retrieval and storage of Azure billing data using Python. This enables advanced reporting, cost analysis, and integration with enterprise data platforms.</p>"},{"location":"azure-billing/#references","title":"References","text":"<ul> <li>Azure Cost Management REST API</li> <li>MSAL Python Library</li> <li>pyodbc Documentation</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> </ul>"},{"location":"azure-container-schedule/","title":"Orchestrating Scheduled Jobs in a Python Container Using a Scheduler Script","text":"<p>This article demonstrates how to build a robust job scheduler in a Python-based container environment. The scheduler coordinates the execution of various Python scripts at specific times or intervals, ensuring that business processes run reliably and in the correct timezone. The approach is suitable for any containerized environment, such as those running in Azure, AWS, or on-premises.</p>"},{"location":"azure-container-schedule/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Key Concepts</li> <li>Core Scheduler Script<ul> <li>Scheduling Jobs</li> <li>Running Jobs in the Background</li> <li>Timezone Aware Scheduling</li> </ul> </li> <li>Container Startup with Supervisor</li> <li>Conclusion</li> </ol>"},{"location":"azure-container-schedule/#overview","title":"Overview","text":"<p>In many automation and integration scenarios, you need to run a set of scripts or jobs on a schedule\u2014some daily, some every few minutes, and some only on certain days. When running in a container, you want the scheduler to start automatically and keep running, launching jobs as needed. This article explains how to implement such a scheduler in Python, using the <code>schedule</code> library, and how to ensure it starts with your container using Supervisor.</p>"},{"location":"azure-container-schedule/#key-concepts","title":"Key Concepts","text":"<ul> <li>Job Scheduling: Use the <code>schedule</code> library to define when each job should run (e.g., every day at a certain time, every N minutes).</li> <li>Background Execution: Use Python's <code>threading</code> module to run jobs asynchronously, so the scheduler loop is never blocked.</li> <li>Timezone Awareness: Use the <code>pytz</code> library to ensure jobs run at the correct local time, even if the container's system time is UTC.</li> <li>Error Handling: Use try/except blocks and logging to capture and report errors without stopping the scheduler.</li> <li>Container Startup: Use Supervisor to ensure the scheduler script starts automatically when the container is created and restarts if it fails.</li> </ul>"},{"location":"azure-container-schedule/#core-scheduler-script","title":"Core Scheduler Script","text":"<p>Below is a simplified and generic version of a Python scheduler script suitable for containerized environments. All company-specific details have been removed.</p>"},{"location":"azure-container-schedule/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import schedule\nimport time\nimport subprocess\nimport threading\nimport sys\nimport pytz\nfrom datetime import datetime, timedelta\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"scheduler\")\n</code></pre>"},{"location":"azure-container-schedule/#helper-functions","title":"Helper Functions","text":""},{"location":"azure-container-schedule/#timezone-aware-scheduling","title":"Timezone Aware Scheduling","text":"<pre><code>def get_next_utc_for_local(hour, minute=0, timezone_str='America/Chicago'):\n    \"\"\"Calculate the next occurrence of the specified hour in the given timezone and convert to UTC.\"\"\"\n    tz = pytz.timezone(timezone_str)\n    now = datetime.now(tz)\n    target_time = now.replace(hour=hour, minute=minute, second=0, microsecond=0)\n    if now &gt;= target_time:\n        target_time += timedelta(days=1)\n    return target_time.astimezone(pytz.utc).strftime('%H:%M')\n</code></pre>"},{"location":"azure-container-schedule/#running-jobs-in-the-background","title":"Running Jobs in the Background","text":"<pre><code>def run_in_background(target_function):\n    \"\"\"Runs the given function asynchronously in a background thread.\"\"\"\n    thread = threading.Thread(target=target_function, daemon=True)\n    thread.start()\n</code></pre>"},{"location":"azure-container-schedule/#executing-python-scripts","title":"Executing Python Scripts","text":"<pre><code>def execute_script(script_path, *args):\n    \"\"\"Helper function to execute a Python script with optional arguments.\"\"\"\n    subprocess.run(['python', script_path, *args])\n</code></pre>"},{"location":"azure-container-schedule/#defining-job-functions","title":"Defining Job Functions","text":"<p>Each job function can call a different Python script or perform a specific task. For example:</p> <pre><code>def job_example():\n    logger.info(\"Running example job...\")\n    execute_script('path/to/your_script.py', 'optional_arg')\n</code></pre>"},{"location":"azure-container-schedule/#scheduling-jobs","title":"Scheduling Jobs","text":"<p>You can schedule jobs at specific times or intervals. Here are some examples:</p> <pre><code>if __name__ == \"__main__\":\n    try:\n        # Schedule daily jobs at specific local times (converted to UTC)\n        schedule.every().day.at(get_next_utc_for_local(7)).do(lambda: run_in_background(job_example)).tag('job_example', 'daily_task')\n        schedule.every().day.at(get_next_utc_for_local(12, 30)).do(lambda: run_in_background(job_example)).tag('job_example', 'daily_task')\n\n        # Schedule jobs at regular intervals\n        schedule.every(15).minutes.do(lambda: run_in_background(job_example)).tag('job_example', 'frequent_task')\n\n        # Main scheduler loop\n        while True:\n            try:\n                schedule.run_pending()\n            except Exception as e:\n                logger.error(f\"Scheduler Error: {e}\")\n            time.sleep(1)\n    except Exception as e:\n        logger.error(f\"Fatal Scheduler Error: {e}\")\n</code></pre>"},{"location":"azure-container-schedule/#explanation","title":"Explanation","text":"<ul> <li>get_next_utc_for_local: Ensures jobs run at the correct local time, regardless of the container's timezone.</li> <li>run_in_background: Prevents long-running jobs from blocking the scheduler loop.</li> <li>execute_script: Launches other Python scripts as subprocesses, optionally with command-line arguments.</li> <li>schedule.every().day.at(...): Schedules jobs at specific times.</li> <li>schedule.every(N).minutes.do(...): Schedules jobs at regular intervals.</li> <li>Infinite Loop: The script runs forever, checking for jobs to run every second.</li> </ul>"},{"location":"azure-container-schedule/#container-startup-with-supervisor","title":"Container Startup with Supervisor","text":"<p>To ensure your scheduler script starts automatically when the container is created and restarts if it fails, use Supervisor. Add a <code>supervisord.conf</code> file to your container image with the following content:</p> <pre><code>[program:scheduler]\ncommand=python /app/src/gdepscheduler.py\nautostart=true\nautorestart=true\n</code></pre> <ul> <li>[program:scheduler]: Defines a program called \"scheduler\".</li> <li>command: The command to run your scheduler script.</li> <li>autostart=true: Start the program automatically when Supervisor starts.</li> <li>autorestart=true: Restart the program if it exits unexpectedly.</li> </ul> <p>Supervisor will keep your scheduler running, even if it crashes or the container restarts.</p>"},{"location":"azure-container-schedule/#conclusion","title":"Conclusion","text":"<p>By combining the <code>schedule</code> library, background threading, timezone handling, and Supervisor, you can build a reliable, container-friendly job scheduler in Python. This approach ensures your automation and integration jobs run on time, every time, with minimal manual intervention.</p> <p>Adapt the code and configuration to your own scripts and business requirements. This pattern is suitable for any containerized environment where scheduled automation is needed.</p>"},{"location":"azure-query-recovery-services-vault/","title":"Querying Azure Recovery Services Vault (RSV)","text":""},{"location":"azure-query-recovery-services-vault/#introduction","title":"Introduction","text":"<p>Azure Recovery Services Vault (RSV) is a core component for managing and monitoring backups of virtual machines and other resources in Azure. Automating the retrieval and analysis of backup data can help with compliance, reporting, and operational efficiency. This article demonstrates how to:</p> <ul> <li>Connect to Azure using the Python SDK and a service principal</li> <li>Query a Recovery Services Vault for backup details</li> <li>Process and store backup information for further analysis</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"azure-query-recovery-services-vault/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with Recovery Services Vault(s) and VM backups</li> <li>Service principal with appropriate permissions (Backup Reader, etc.)</li> <li>Python 3.8+ and the following packages (install with <code>pip install ...</code>):</li> <li><code>azure-identity</code> (for authentication)</li> <li><code>azure-mgmt-recoveryservicesbackup</code> (for querying backup data)</li> <li><code>azure-mgmt-resource</code> (for resource management, optional)</li> <li><code>requests</code> (for any direct REST API calls, optional)</li> <li><code>python-dateutil</code> (for date parsing, optional)</li> <li><code>csv</code> (standard library, for CSV export)</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-query-recovery-services-vault/#step-1-authenticate-to-azure-with-a-service-principal","title":"Step 1: Authenticate to Azure with a Service Principal","text":"<p>Use the Azure Identity library to authenticate securely:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_client_secret_credential(tenant_id, client_id, client_secret):\n    \"\"\"Obtain a ClientSecretCredential for Azure authentication.\"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n\n**Explanation:**\n- Use a service principal for secure, automated access.\n- Store secrets securely (e.g., Azure Key Vault).\n\n---\n\n## Step 2: Connect to the Recovery Services Backup Client\n\n\n```python\nfrom azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\n\ndef get_backup_client(credential, subscription_id):\n    \"\"\"Create a RecoveryServicesBackupClient for backup operations.\"\"\"\n    return RecoveryServicesBackupClient(credential, subscription_id)\n# Example usage:\n# backup_client = get_backup_client(credential, '&lt;your-subscription-id&gt;')\n</code></pre> <p>Explanation: - The <code>RecoveryServicesBackupClient</code> allows you to query backup items, jobs, and policies.</p>"},{"location":"azure-query-recovery-services-vault/#step-3-query-backup-items-in-a-recovery-services-vault","title":"Step 3: Query Backup Items in a Recovery Services Vault","text":"<pre><code>def list_vm_backups(backup_client, resource_group, vault_name):\n    \"\"\"List all backup items (e.g., Azure VMs) in the specified vault.\"\"\"\n    items = backup_client.backup_protected_items.list(\n        vault_name=vault_name,\n        resource_group_name=resource_group,\n        filter=\"backupManagementType eq 'AzureIaasVM'\"\n    )\n    backup_info = []\n    for item in items:\n        backup_info.append({\n            'vm_name': item.properties.friendly_name,\n            'protection_status': item.properties.protection_status,\n            'last_backup_time': item.properties.last_backup_time,\n            'health_status': item.properties.health_status,\n            'resource_id': item.id\n        })\n    return backup_info\n</code></pre> <p>Explanation: - Lists all VM backup items in the specified Recovery Services Vault. - Extracts key properties for reporting or further processing.</p>"},{"location":"azure-query-recovery-services-vault/#step-4-store-or-report-on-backup-data","title":"Step 4: Store or Report on Backup Data","text":"<p>You can save the backup information to a CSV file or database for further analysis:</p> <pre><code>def save_backup_info_to_csv(backup_info, filename):\n    \"\"\"Save backup information to a CSV file.\"\"\"\n    with open(filename, 'w', newline='') as csvfile:\n        fieldnames = ['vm_name', 'protection_status', 'last_backup_time', 'health_status', 'resource_id']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for row in backup_info:\n            writer.writerow(row)\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    # Retrieve credentials securely\n    tenant_id = '&lt;your-tenant-id&gt;'\n    client_id = '&lt;your-client-id&gt;'\n    client_secret = '&lt;your-client-secret&gt;'\n    subscription_id = '&lt;your-subscription-id&gt;'\n    resource_group = '&lt;your-resource-group&gt;'\n    vault_name = '&lt;your-vault-name&gt;'\n\n    credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n    backup_client = get_backup_client(credential, subscription_id)\n    backup_info = list_vm_backups(backup_client, resource_group, vault_name)\n    save_backup_info_to_csv(backup_info, 'azure_vm_backups.csv')\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#advanced-updating-backup-resource-details-programmatically","title":"Advanced: Updating Backup Resource Details Programmatically","text":"<p>In some enterprise scenarios, you may want to enrich or update your backup resource inventory with additional details from Azure Recovery Services Vault (RSV). The following function demonstrates how to programmatically update a list of backup resources with the latest backup status and metadata for each VM.</p> <pre><code>from azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\nfrom azure.mgmt.recoveryservicesbackup.activestamp.models import AzureIaaSComputeVMProtectedItem\nimport datetime\n\ndef update_backup_resources(resource_group, vault_name, backup_resources):\n    \"\"\"\n    Update backup resource details by fetching relevant VM backup information from Recovery Services Vault (RSV).\n\n    :param resource_group: Name of the Azure resource group\n    :param vault_name: Name of the Recovery Services Vault\n    :param backup_resources: List of backup resource dictionaries to update\n    \"\"\"\n    try:\n        # Obtain Azure credentials (replace with your secure credential retrieval)\n        credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n        backup_client = RecoveryServicesBackupClient(credential, subscription_id)\n\n        # Dictionary to store VM backups from the vault\n        azure_vm_backups = {}\n\n        # Fetch backup-protected items from the Recovery Services Vault\n        rsv_backup_items = backup_client.backup_protected_items.list(vault_name, resource_group)\n        azure_vm_backups.update({\n            item.properties.virtual_machine_id.lower(): item\n            for item in rsv_backup_items\n            if isinstance(item.properties, AzureIaaSComputeVMProtectedItem)\n        })\n\n        # Get today's date in YYYY-MM-DD format\n        today_date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n\n        def format_date(value):\n            return value.strftime(\"%Y-%m-%d\") if isinstance(value, datetime.datetime) else today_date\n\n        # Iterate through the list of backup resources and update details\n        for resource in backup_resources:\n            normalized_vm_id = resource['resource_id'].lower()\n            vm_backup = azure_vm_backups.get(normalized_vm_id)\n            resource.update({\n                \"friendly_name\": getattr(vm_backup.properties, 'friendly_name', '') if vm_backup else '',\n                \"policy_name\": getattr(vm_backup.properties, 'policy_name', '') if vm_backup else '',\n                \"last_backup_status\": getattr(vm_backup.properties, 'last_backup_status', '') if vm_backup else '',\n                \"last_backup_time\": format_date(getattr(vm_backup.properties, 'last_backup_time', None)) if vm_backup else today_date,\n                \"last_recovery_point\": format_date(getattr(vm_backup.properties, 'last_recovery_point', None)) if vm_backup else today_date,\n                \"protection_state\": getattr(vm_backup.properties, 'protection_state', '') if vm_backup else '',\n                \"protection_status\": getattr(vm_backup.properties, 'protection_status', '') if vm_backup else '',\n                \"container_name\": getattr(vm_backup.properties, 'container_name', '') if vm_backup else '',\n            })\n    except Exception as e:\n        print(f\"Error updating backup resources: {e}\")\n</code></pre> <p>Explanation: - This function takes a list of backup resources (e.g., VMs) and updates each with the latest backup metadata from Azure RSV. - It normalizes VM IDs for matching, fetches backup items from the vault, and updates each resource dictionary in-place. - Error handling is included for robustness; in production, use secure credential management and structured logging.</p>"},{"location":"azure-query-recovery-services-vault/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the retrieval and reporting of VM backup data from Azure Recovery Services Vaults. This enables better compliance, reporting, and operational insight into your backup posture.</p>"},{"location":"azure-resources/","title":"Download Azure Resource","text":""},{"location":"azure-resources/#programmatically-downloading-azure-resource-inventory-and-tag-management","title":"Programmatically Downloading Azure Resource Inventory and Tag Management","text":""},{"location":"azure-resources/#introduction","title":"Introduction","text":"<p>Maintaining an up-to-date inventory of Azure resources and their associated tags is critical for governance, cost management, and compliance. This article provides a detailed, production-grade approach to programmatically fetching Azure resource metadata and synchronizing it with a SQL Server database using Python. </p>"},{"location":"azure-resources/#1-authentication-secure-access-to-azure-apis","title":"1. Authentication: Secure Access to Azure APIs","text":"<p>Before accessing Azure resources, authenticate using a secure method. The function below demonstrates using the Azure Identity SDK's <code>ClientSecretCredential</code> for authentication. This is a common approach for automation scenarios, but for higher security, certificate-based authentication is recommended (see the article Certificate Based Authorization for Azure AD.)</p>"},{"location":"azure-resources/#deep-dive-get_azure_credential-function","title":"Deep Dive: <code>get_azure_credential</code> Function","text":"<p>The <code>get_azure_credential</code> function leverages the <code>azure-identity</code> Python SDK, which provides a unified way to authenticate to Azure services. Here, we use the <code>ClientSecretCredential</code> class, which is suitable for service principals (app registrations) with a client secret.</p> <p>Python Example:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_azure_credential(tenant_id, client_id, client_secret):\n    \"\"\"\n    Returns a credential object for authenticating with Azure SDKs.\n    Uses the azure-identity library's ClientSecretCredential.\n    \"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n</code></pre> <ul> <li>azure-identity SDK: This is the official Microsoft library for Azure authentication in Python. It supports multiple credential types, including secrets, certificates, managed identity, and interactive login.</li> <li>ClientSecretCredential: This class is used for service-to-service authentication using a client ID and secret. It is widely supported by Azure SDKs, including resource management, storage, and more.</li> <li>When to use: Use this for automation where a client secret is securely stored (e.g., in Azure Key Vault or environment variables). For higher security, use <code>CertificateCredential</code> instead.</li> </ul>"},{"location":"azure-resources/#2-fetching-azure-resource-inventory","title":"2. Fetching Azure Resource Inventory","text":"<p>Use the Azure SDK to enumerate all resources in a subscription. Extract key metadata such as resource ID, name, location, type, and tags.</p> <p>Python Example:</p> <pre><code>def fetch_azure_resources(credential, subscription_id):\n    client = ResourceManagementClient(credential, subscription_id)\n    resource_list = []\n    for item in client.resources.list():\n        type_parts = str(item.type).split('/')\n        type1, type2, type3, type4, type5 = (type_parts + [''] * 5)[:5]\n        resource_group_list = str(item.id).split('/')\n        resource_data = {\n            \"id\": str(item.id).replace(f'/subscriptions/{subscription_id}/', ''),\n            \"location\": item.location,\n            \"name\": item.name,\n            \"tags\": item.tags,\n            \"resourceGroup\": resource_group_list[4] if len(resource_group_list) &gt;= 4 else '',\n            \"type1\": type1,\n            \"type2\": type2,\n            \"type3\": type3,\n            \"type4\": type4,\n            \"type5\": type5,\n        }\n        resource_list.append(resource_data)\n    return resource_list\n</code></pre> <ul> <li>Resource Types: The code splits the resource type string to extract up to five type levels for flexible reporting.</li> <li>Tags: Tags are included for governance and cost allocation.</li> </ul>"},{"location":"azure-resources/#3-synchronizing-with-sql-server-fast-bulk-operations","title":"3. Synchronizing with SQL Server: Fast Bulk Operations","text":"<p>Efficiently update the SQL Server inventory table by marking all resources as inactive, then bulk updating existing resources and inserting new ones. This ensures the database reflects the current Azure state.</p> <p>Note: The <code>Dim_Resources</code> table is not a full load (truncate-and-reload) table. Instead, it is designed to retain records of resources that may have been deleted from Azure. By marking resources as inactive rather than removing them, you can track the lifecycle of resources, including those that have been deleted, for audit, compliance, and historical analysis purposes.</p> <p>Python Example:</p> <pre><code>import pyodbc\n\ndef sync_resources_to_sql(resource_list, connection_string):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    existing_resource_ids = {str(row[0]).lower() for row in cursor.execute(\"SELECT ResourceID FROM Dim_Resources\").fetchall()}\n    updateresources = [\n        [r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True, r['id']]\n        for r in resource_list if str(r['id']).lower() in existing_resource_ids\n    ]\n    newresources = [\n        [r['id'], r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True]\n        for r in resource_list if str(r['id']).lower() not in existing_resource_ids\n    ]\n    cursor.execute('UPDATE Dim_Resources SET Active = 0')\n    if updateresources:\n        query = '''UPDATE Dim_Resources SET Location=?, Name=?, ResourceGroup=?, Type1=?, Type2=?, Type3=?, Type4=?, Type5=?, Active=? WHERE ResourceId=?'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, updateresources)\n    if newresources:\n        query = '''INSERT INTO Dim_Resources (ResourceID, Location, Name, ResourceGroup, Type1, Type2, Type3, Type4, Type5, Active) VALUES (?,?,?,?,?,?,?,?,?,?)'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, newresources)\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Bulk Operations: Use <code>fast_executemany</code> for high-performance updates and inserts.</li> <li>Active Flag: Mark all resources as inactive before updating, then set active for current resources.</li> </ul>"},{"location":"azure-resources/#4-end-to-end-orchestration","title":"4. End-to-End Orchestration","text":"<p>A typical workflow for resource inventory management:</p> <pre><code>def fetch_and_store_resources():\n    credential = get_azure_credential(tenant_id=..., client_id=..., client_secret=...)\n    resource_list = fetch_azure_resources(credential, subscription_id=...)\n    sync_resources_to_sql(resource_list, connection_string=...)\n</code></pre>"},{"location":"azure-resources/#5-best-practices-and-considerations","title":"5. Best Practices and Considerations","text":"<ul> <li>Security: Use certificate-based authentication for automation when possible. Store credentials securely.</li> <li>Performance: Use bulk operations for large datasets.</li> <li>Data Quality: Regularly update the inventory to reflect the current Azure state.</li> <li>Scheduling: Automate the process with a scheduler (e.g., cron, Azure Automation).</li> <li>Auditing: Keep logs of changes and exceptions for compliance.</li> </ul>"},{"location":"azure-resources/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can automate the discovery and inventory of Azure resources, ensuring your SQL Server database remains a reliable source of truth for governance and reporting.</p>"},{"location":"azure-resources/#references","title":"References","text":"<ul> <li>Azure Resource Management Python SDK</li> <li>azure-identity Python SDK</li> <li>pyodbc Documentation</li> <li>Azure Tagging Best Practices</li> </ul>"},{"location":"azure-restore-from-backup-part1/","title":"Azure VM Restore from Backup \u2013 Part 1: Automated Restore with PowerShell","text":"<p>This article is Part 1 of a two-part series on automating Azure VM restore from Recovery Services Vault (RSV) backups, enabling Business Continuity/Disaster Recovery (BCP/DR) across regions. Here, we focus on the main restore process using the <code>restorevms.ps1</code> script. In Part 2, we cover NIC re-creation and IP assignment for a true like-for-like DR.</p>"},{"location":"azure-restore-from-backup-part1/#overview","title":"Overview","text":"<ul> <li>Goal: Restore VMs from backup in one Azure region (e.g., East US) to another (e.g., West US) for BCP/DR.</li> <li>Approach: Use PowerShell and Azure CLI to automate finding the latest restore point and restoring the VM, including all required network and resource group settings.</li> <li>Why: This approach ensures your DR VM is as close as possible to the original, with minimal manual effort.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#the-script-restorevmsps1","title":"The Script: <code>restorevms.ps1</code>","text":"<p>Below is the full script used to automate the restore process:</p> <pre><code># Boolean variable to indicate whether to restore disks only\n# If it is True then VM will also be created so be careful \nparam (\n    [string] $restorediskonly = \"false\",\n    [int] $numberofhours2wait = 2,\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json'\n)\n\n########################################################\n# Install Azure CLI manually if not installed already\n# curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n########################################################\nfunction Get-Latest-Recovery-Point {\n    param (\n        [string]$vmname,    \n        [string]$vmcontainername,\n        [string]$vaultname,\n        [string]$vaultresourcegroupname\n    )\n\n    $latest_recovery_point = az backup recoverypoint list `\n        --container-name $vmcontainername `\n        --backup-management-type AzureIaasVM `\n        --item-name $vmname `\n        --resource-group $vaultresourcegroupname `\n        --vault-name $vaultname `\n        --query \"[0]\" `\n        --use-secondary-region `\n        --output json `\n    | ConvertFrom-Json\n    | Select-Object -ExpandProperty name\n\n    return $latest_recovery_point\n}\nfunction Restore-Disks {\n    param (\n        [string]$vmname,    \n        [string]$vmcontainername,\n        [string]$vaultname,\n        [string]$vaulresourcegrouptname,\n        [string]$storageaccountname,\n        [string]$recoverypointname,\n        [string]$targetresourcegroupname,\n        [string]$targetvmname = '',\n        [string]$targetvnetname = '',\n        [string]$targetvnetresourcegroup = '',\n        [string]$targetsubnetname = '',\n        [bool]$restoretodisk = $false\n    )\n    if ($restoretodisk) {\n        $restore_operation = az backup restore restore-disks `\n            --resource-group $vaulresourcegrouptname `\n            --vault-name $vaultname `\n            --restore-mode AlternateLocation `\n            --container-name $vmcontainername `\n            --item-name $vmname `\n            --storage-account $storageaccountname `\n            --rp-name $recoverypointname `\n            --target-resource-group $targetresourcegroupname `\n            --restore-to-staging-storage-account $true `\n            --use-secondary-region\n    }\n    else {\n        $restore_operation = az backup restore restore-disks `\n            --resource-group $vaulresourcegrouptname `\n            --vault-name $vaultname `\n            --container-name $vmcontainername `\n            --item-name $vmname `\n            --storage-account $storageaccountname `\n            --rp-name $recoverypointname `\n            --target-resource-group $targetresourcegroupname `\n            --target-vm-name $targetvmname `\n            --target-vnet-name $targetvnetname `\n            --target-vnet-resource-group $targetvnetresourcegroup `\n            --target-subnet-name $targetsubnetname `\n            --use-secondary-region \n    }    \n\n    return $restore_operation\n}\nfunction Invoke-AzureCLICommand {\n    param (\n        [string]$command,\n        [string]$message = \"\"\n    )\n\n    if (-not [string]::IsNullOrEmpty($message)) {\n        Write-CustomLog (\"About to run message {0}\" -f $message)\n        Write-CustomLog (\"About to run command {0}\" -f $command)\n    }\n    $executionResult = Invoke-Expression $command\n    return $executionResult\n}\n# logging function which should work on both local dev container and Git Hub action\nfunction Write-CustomLog {\n    param (\n        [string]$message\n    )\n    Write-Host \"::notice::$message\"\n}\n\n$booleanRestorediskonly = $false\n\nswitch ($restorediskonly.ToLower()) {\n    \"true\" {\n        $restorediskonly = [bool]$true\n        $booleanRestorediskonly = $true\n    }\n    \"false\" {\n        $restorediskonly = [bool]$false\n        $booleanRestorediskonly = $false\n    }\n    default {\n        throw \"Invalid value for restorediskonly: $restorediskonly. Must be 'true' or 'false'.\"\n    }\n}\n\n# Log values with type information\nWrite-CustomLog (\"BooleanRestorediskonly variable value: $booleanRestorediskonly (Type: $($booleanRestorediskonly.GetType().Name))\")\nWrite-CustomLog (\"Numberofhours2wait variable value: $numberofhours2wait (Type: $($numberofhours2wait.GetType().Name))\")\n\n# Use the boolean variable in further logic\nif ($booleanRestorediskonly) {\n    Write-CustomLog \"Restoring disk only.\"\n} else {\n    Write-CustomLog \"Performing full restore.\"\n}\n\n############################################################\n# Variables, note the staging SA we created in earler steps\n############################################################\n$gdep_rsv_vault_name = \"rsv-prod-eus-01\"\n$gdep_rsv_vault_resource_group = \"rg-gdep-peus-backup\"\n\n$gdep_staging_storage_account = az storage account show --name storegdeppwusstaging --query 'id' --output tsv\nWrite-CustomLog (\"Staging storage account is {0}\" -f $gdep_staging_storage_account)\n\n&lt;#\n    gdep_virtual_machine                = Specifies the name of the backed-up VM item to restore.\n    gdep_virtual_machine_containername  = Specifies the name of the container within the vault that holds the backed-up VM\n    gdep_target_resource_group_name     = Specifies the resource group where the restored VM will be created.\n    gdep_target_vm_name                 = Specifies the name to assign to the newly restored VM\n    gdep_target_vnet_name               = Specifies the name of the virtual network (VNet) to which the restored VM will be connected\n    gdep_target_vnet_resource_group     = Specifies the resource group name of the VNet where the restored VM will be connected\n    gdep_target_subnet_name             = Specifies the name of the subnet within the target VNet where the restored VM will be placed\n\n    vaultname = Specifies the name of the Recovery Services vault from which the backup will be restored\n    vaulresourcegrouptname = Specifies the resource group name where the vault is located. \n    storageaccountname = Specifies the name of the storage account where the VM disks will be restored. \n    recoverypointname = Specifies the name of the recovery point to restore from.\n\n    gdep_virtual_machine_nic_name = Name of the NIC resource\n    gdep_virtual_machine_nic_ip = Final IP desired for this NIC\n    gdep_virtual_machine_nic_subnet_resource_group = \n    gdep_virtual_machine_nic_vnet_name = \n    gdep_virtual_machine_nic_snet_name = \n#&gt;\n\n$vm_list = Get-Content -Path $vmlist | ConvertFrom-Json\n# Start of script\n\n# Lets add restore Job related properties to keep track of each job \nforeach ($gdepvm in $vm_list) {\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_restore_job_name\" -Value \"\"\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_restore_job_status\" -Value \"Not Started\"\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_nic_operation_completed\" -Value $false\n}\n\nforeach ($gdepvm in $vm_list) {\n    try {\n        # Extract parameters from the hashtable\n        $gdep_virtual_machine = $gdepvm.gdep_virtual_machine\n        $gdep_virtual_machine_container_name = $gdepvm.gdep_virtual_machine_containername\n        $gdep_target_resource_group_name = $gdepvm.gdep_target_resource_group_name\n        $gdep_target_vm_name = $gdepvm.gdep_target_vm_name\n        $gdep_target_vnet_name = $gdepvm.gdep_target_vnet_name\n        $gdep_target_vnet_resource_group = $gdepvm.gdep_target_vnet_resource_group\n        $gdep_target_subnet_name = $gdepvm.gdep_target_subnet_name\n\n        Write-CustomLog (\"About to obtain recovery point for VM {0}\" -f $gdep_virtual_machine)\n\n        $latest_recovery_point = Get-Latest-Recovery-Point `\n            -vmname $gdep_virtual_machine `\n            -vmcontainername $gdep_virtual_machine_container_name `\n            -vaultname $gdep_rsv_vault_name `\n            -vaultresourcegroupname $gdep_rsv_vault_resource_group\n\n        if ($booleanRestorediskonly) {\n            Write-CustomLog (\"Sould not have come into this loop {0}\" -f $gdep_virtual_machine)\n            if (-not [string]::IsNullOrEmpty($latest_recovery_point)) {\n                $restore_operation = Restore-Disks `\n                    -vmname $gdep_virtual_machine `\n                    -vmcontainername $gdep_virtual_machine_container_name `\n                    -vaultname $gdep_rsv_vault_name `\n                    -vaulresourcegrouptname $gdep_rsv_vault_resource_group `\n                    -storageaccountname $gdep_staging_storage_account `\n                    -recoverypointname $latest_recovery_point `\n                    -targetresourcegroupname $gdep_target_resource_group_name `\n                    -restoretodisk $true\n                # ($restore_operation | ConvertFrom-Json).properties.activityId\n                $gdepvm.gdep_restore_job_name = ($restore_operation | ConvertFrom-Json).name\n                Write-CustomLog (\"Working on Restoring Disk for job name {0}\" -f $gdepvm.gdep_restore_job_name)\n            }\n            else {\n                Write-CustomLog (\"No disk(s) recovery point found for VM specfified {0}\" -f $gdep_virtual_machine)\n            }\n        }\n        else {\n            if (-not [string]::IsNullOrEmpty($latest_recovery_point)) {\n                Write-CustomLog (\"This is the correct condition for VM {0}\" -f $gdep_virtual_machine)\n                $restore_operation = Restore-Disks `\n                    -vmname $gdep_virtual_machine `\n                    -vmcontainername $gdep_virtual_machine_container_name `\n                    -vaultname $gdep_rsv_vault_name `\n                    -vaulresourcegrouptname $gdep_rsv_vault_resource_group `\n                    -storageaccountname $gdep_staging_storage_account `\n                    -recoverypointname $latest_recovery_point `\n                    -targetresourcegroupname $gdep_target_resource_group_name `\n                    -targetvmname $gdep_target_vm_name `\n                    -targetvnetname $gdep_target_vnet_name `\n                    -targetvnetresourcegroup $gdep_target_vnet_resource_group `\n                    -targetsubnetname $gdep_target_subnet_name `\n                    -restoretodisk $false\n                $gdepvm.gdep_restore_job_name = ($restore_operation | ConvertFrom-Json).name\n                Write-CustomLog (\"Working on Restoring Virtual Machine for job name {0}\" -f $gdepvm.gdep_restore_job_name)\n            }\n            else {\n                Write-CustomLog (\"No virtual machine recovery point found for VM {0}\" -f $gdep_virtual_machine)\n            }\n        }\n    }\n    catch {\n        Write-Error \"An error occurred: $_\"\n    }\n}\n</code></pre>"},{"location":"azure-restore-from-backup-part1/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"azure-restore-from-backup-part1/#1-parameters-and-setup","title":"1. Parameters and Setup","text":"<ul> <li><code>restorediskonly</code>: If <code>true</code>, only disks are restored; if <code>false</code>, a full VM is created.</li> <li><code>numberofhours2wait</code>: How long to wait for restore jobs (not used directly in this script, but can be used for polling/wait logic).</li> <li><code>vmlist</code>: Path to the JSON file listing VMs to restore (see example below).</li> </ul>"},{"location":"azure-restore-from-backup-part1/#2-helper-functions","title":"2. Helper Functions","text":"<ul> <li>Get-Latest-Recovery-Point: Finds the most recent backup recovery point for a VM in the secondary region.</li> <li>Restore-Disks: Runs the Azure CLI command to restore either disks or a full VM, depending on parameters.</li> <li>Invoke-AzureCLICommand: Utility to run arbitrary Azure CLI commands and log them.</li> <li>Write-CustomLog: Standardized logging for both local and CI environments.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#3-main-logic","title":"3. Main Logic","text":"<ul> <li>Loads the VM list from the provided JSON file.</li> <li>For each VM, finds the latest recovery point and triggers a restore (either disk or full VM).</li> <li>Tracks job names and statuses for each VM.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#4-vm-list-json-example","title":"4. VM List JSON Example","text":"<p>The script expects a JSON file like this:</p> <pre><code>[\n  {\n    \"gdep_virtual_machine\": \"MDCAVDPRDAE02\",\n    \"gdep_virtual_machine_containername\": \"MDCAVDPRDAE02\",\n    \"gdep_target_resource_group_name\": \"dr-rg-gdep-pwus-infrastructure\",\n    \"gdep_target_vm_name\": \"DRMDCAVDPRDAE02\",\n    \"gdep_target_vnet_name\": \"vnet-gdep-pwus-management\",\n    \"gdep_target_vnet_resource_group\": \"dr-rg-gdep-pwus-vnets\",\n    \"gdep_target_subnet_name\": \"snet-gdep-pwus-management-restore\"\n    // ...other NIC properties...\n  }\n]\n</code></pre>"},{"location":"azure-restore-from-backup-part1/#summary","title":"Summary","text":"<ul> <li>This script automates the restore of VMs from Azure RSV backups to a new region.</li> <li>It ensures you always restore from the latest available backup.</li> <li>For full DR, combine with the NIC re-creation process in Part 2.</li> </ul> <p>Continue to Part 2: NIC Re-Creation and IP Assignment</p>"},{"location":"azure-restore-from-backup-part2/","title":"Azure VM Restore from Backup \u2013 Part 2: NIC Re-Creation and IP Assignment","text":"<p>This article is Part 2 of the series on automating Azure VM restore from Recovery Services Vault (RSV) backups for BCP/DR. Here, we focus on the process of re-creating and attaching NICs to restored VMs, ensuring each VM receives the correct IP address and network configuration\u2014just as it was in the source region.</p> <p>If you haven't already, see Part 1 for the main restore process.</p>"},{"location":"azure-restore-from-backup-part2/#overview","title":"Overview","text":"<ul> <li>Goal: After restoring VMs from backup, ensure each VM has a NIC with the same IP and subnet as in the original region.</li> <li>Approach: Use PowerShell and Azure CLI to automate NIC creation, assignment, and validation.</li> <li>Why: Azure restores VMs with default NICs; for true DR, you must re-create and attach NICs with the desired configuration.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#the-script-attachnicsps1","title":"The Script: <code>attachnics.ps1</code>","text":"<p>Below is the full script used to automate the NIC re-creation and assignment process:</p> <pre><code>param (\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json',\n    [string] $vaultname = 'rsv-prod-eus-01',\n    [string] $vaultresourcegroupname = 'rg-gdep-peus-backup'\n)\n\nfunction Write-CustomLog {\n    param (\n        [string]$message\n    )\n    Write-Host \"::notice::$message\"\n}\n\nfunction Get-Last-Restore-JobID {\n    param (\n        [string]$vaultname,\n        [string]$vaultresourcegroupname,\n        [string]$vmname,\n        [string]$timeRangeStart\n    )\n\n    $vmname = $vmname.ToLower()\n    $jobid = ''\n\n    $backupJobsJson = az backup job list `\n        --resource-group $vaultresourcegroupname `\n        --vault-name $vaultname `\n        --query \"[?properties.endTime &gt;= '$timeRangeStart' `\n                &amp;&amp; properties.operation == 'CrossRegionRestore'  `\n                &amp;&amp; properties.jobType == 'AzureIaaSVMJob' `\n                &amp;&amp; properties.status == 'Completed' `\n                ] | sort_by(@, &amp;properties.endTime) | reverse(@)\" `\n        --output json\n    $backupJobsJson = $backupJobsJson | ConvertFrom-Json\n\n    foreach ($job in $backupJobsJson) {\n        if ($job.properties.entityFriendlyName.ToLower() -like \"*$vmName*\") {\n            $vmNameRestored = $job.properties.entityFriendlyName\n            Write-CustomLog \"Cross Region Restore for VM has completed : $vmNameRestored\"\n            $jobid = $job.id\n            break\n        }\n    }\n    return $jobid\n}\n\nfunction Invoke-AzureCLICommand {\n    param (\n        [string]$command,\n        [string]$message = \"\"\n    )\n\n    if (-not [string]::IsNullOrEmpty($message)) {\n        Write-CustomLog (\"About to run message {0}\" -f $message)\n        Write-CustomLog (\"About to run command {0}\" -f $command)\n    }\n    $executionResult = Invoke-Expression $command\n    return $executionResult\n}\n\n$vm_list = Get-Content -Path $vmlist | ConvertFrom-Json\n$deploymentresourceGroupName = \"dr-rg-gdep-pwus-deployment\"\n$templateFilePath = \"./networking/nic/nic4drvms.bicep\"\n$vaultname = 'rsv-prod-eus-01'\n$vaultresourcegroupname = 'rg-gdep-peus-backup'\n$data2gobacktoo = (Get-Date).AddDays(-2).ToString(\"yyyy-MM-ddTHH:mm:ssZ\")\n\nforeach ($gdepvm in $vm_list) {\n    try {\n\n        $nicName = $gdepvm.gdep_virtual_machine_nic_name\n        $deploymentGroupName = \"gdepdr-\" + $nicName\n        $nicResouceGroup = $gdepvm.gdep_target_resource_group_name\n        $nicIpAddress = $gdepvm.gdep_virtual_machine_nic_ip\n        $nicSubnetResourceGroup = $gdepvm.gdep_virtual_machine_nic_subnet_resource_group\n        $nicVnetName = $gdepvm.gdep_virtual_machine_nic_vnet_name\n        $nicSubnetName = $gdepvm.gdep_virtual_machine_nic_snet_name\n        $vmName = $gdepvm.gdep_target_vm_name\n        $vmresourceGroupName = $gdepvm.gdep_target_resource_group_name\n\n        Write-CustomLog (\"Retrieving last completed cross region restore status for VM {0} since {1}\" -f $gdepvm.gdep_virtual_machine,$data2gobacktoo)\n        $gdep_restore_job_id = Get-Last-Restore-JobID `\n            -vaultname $vaultname `\n            -vaultresourcegroupname $vaultresourcegroupname `\n            -vmname $gdepvm.gdep_virtual_machine `\n            -timeRangeStart $data2gobacktoo\n\n        if (-not [string]::IsNullOrEmpty($gdep_restore_job_id)) {\n            #This means that VM has been restored \n            Write-CustomLog (\"Restore job {0} has finished for VM {1}\" -f $gdep_restore_job_id, $gdepvm.gdep_virtual_machine)\n            #Check if we have already created and attached the new NIC for this or not\n            $nicExists = az network nic show --resource-group $nicResouceGroup --name $nicName --query id --output tsv\n            if ($nicExists) {\n                Write-CustomLog (\"NIC {0} has already been created and attached to VM {1} nothing to do\" -f $gdepvm.gdep_virtual_machine_nic_name, $gdepvm.gdep_target_vm_name)\n            }\n            else {\n                Write-CustomLog (\"About to create new NIC namely {0}\" -f $gdepvm.gdep_virtual_machine_nic_name)\n                $command = \"az deployment group create --name $deploymentGroupName --resource-group $deploymentresourceGroupName --template-file $templateFilePath --parameters nic_name=$nicName nic_resource_group=$nicResouceGroup nic_ipaddress=$nicIpAddress nic_subnet_resourcegroup=$nicSubnetResourceGroup nic_vnet_name=$nicVnetName nic_subnet_name=$nicSubnetName\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"About to Create new NIC for VM : $vmName\"\n                Write-CustomLog \"Provisioning State: $((($executionResult | ConvertFrom-Json).properties.provisioningState))\"\n                #Setup New NIC Variables we are going to need later\n                $nicid2add = (az network nic show -g $nicResouceGroup -n $nicName | ConvertFrom-Json).id\n                #Dealocate the VM first as it would be up and running \n                Write-CustomLog (\"About to deallocate VM namely {0}\" -f $vmName)\n                $command = \"az vm deallocate --name $vmName --resource-group $vmresourceGroupName --no-wait\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"About to deallocate restored VM: $vmName\"\n\n                # Verify that VM is deallocated\n                $vmStatus = $null\n                while ($vmStatus -ne \"VM deallocated\") {\n                    $vmStatus = az vm get-instance-view --name $gdepvm.gdep_target_vm_name --resource-group $gdepvm.gdep_target_resource_group_name\n                    $vmObject = $vmStatus | ConvertFrom-Json\n                    $nicid2remove = $vmObject.networkProfile.networkInterfaces[0].id\n                    $nicname2remove = az network nic show --ids $nicid2remove --query \"name\" -o tsv\n\n                    $vmInstanceView = ($vmStatus | ConvertFrom-Json).instanceView  \n                    $powerState = ($vmInstanceView.statuses | Where-Object { $_.code -eq \"PowerState/deallocated\" }).displayStatus\n                    if ($powerState -eq \"VM deallocated\") {\n                        Write-CustomLog (\"VM namely {0} is deallocated \" -f $vmName)\n                        $vmStatus = \"VM deallocated\"\n                    }\n                    else {\n                        Write-CustomLog (\"VM namely {0} is still not deallocated sleeping for 15 seconds\" -f $vmName)\n                        Start-Sleep -Seconds 15\n                    }      \n                }\n\n                # First we need to associate the new NIC to this VM and Make it primary\n                $command = \"az vm nic add -g $vmresourceGroupName --vm-name $vmName --nics $nicid2add --primary-nic $nicid2add\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Adding NIC $nicid2add to $vmName\"\n                # Now lets remove and delete NIC\n                $command = \"az vm nic remove -g $vmresourceGroupName --vm-name $vmName --nics $nicid2remove\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Removing NIC $nicid2remove from $vmName\"\n                $command = \"az network nic delete -g $nicResouceGroup -n $nicname2remove\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Deleting NIC $nicname2remove\"\n\n                # Check if the NIC name is 'nic-dr-azgdepprt01' so we can add to backend pool of printer lb created earlier\n                if ($nicIpAddress -eq \"10.27.17.39\") {\n                    $backendAddressPoolName = \"bep-lbi-gdep-pwus-print\"\n                    $printerilbname = \"lbi-gdep-pwus-print-001\"\n                    Write-CustomLog (\"Adding NIC $nicid2add to backend address pool $backendAddressPoolName\")\n                    $command = \"az network nic ip-config address-pool add --address-pool $backendAddressPoolName --ip-config-name ipconfig --nic-name $nicName --resource-group $nicResouceGroup --lb-name $printerilbname\"\n                    $executionResult = Invoke-AzureCLICommand -command $command -message \"Adding NIC $nicid2add to backend address pool $backendAddressPoolName\"\n                }\n            }\n        }\n        else {\n            Write-CustomLog (\"No completed restore job found for VM {0}\" -f $gdepvm.gdep_virtual_machine)\n        }\n    }\n    catch {\n        Write-Error \"An error occurred: $_\"\n    }\n}\n</code></pre>"},{"location":"azure-restore-from-backup-part2/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"azure-restore-from-backup-part2/#1-parameters-and-setup","title":"1. Parameters and Setup","text":"<ul> <li><code>vmlist</code>: Path to the JSON file listing VMs and their desired NIC/network configuration.</li> <li><code>vaultname</code>, <code>vaultresourcegroupname</code>: Used for querying restore jobs.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#2-helper-functions","title":"2. Helper Functions","text":"<ul> <li>Write-CustomLog: Standardized logging for local and CI environments.</li> <li>Get-Last-Restore-JobID: Finds the most recent completed cross-region restore job for a VM.</li> <li>Invoke-AzureCLICommand: Utility to run and log Azure CLI commands.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#3-main-logic","title":"3. Main Logic","text":"<ul> <li>Loads the VM list from the provided JSON file.</li> <li>For each VM:</li> <li>Checks if a NIC with the desired name already exists and is attached. If so, skips.</li> <li>If not, creates a new NIC with the correct IP, subnet, and VNet.</li> <li>Deallocates the VM, attaches the new NIC as primary, removes and deletes the old NIC.</li> <li>Special handling for printer NICs to add to the correct backend pool.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#4-vm-list-json-example","title":"4. VM List JSON Example","text":"<p>The script expects a JSON file like this:</p> <pre><code>[\n  {\n    \"gdep_virtual_machine\": \"MDCAVDPRDAE02\",\n    \"gdep_virtual_machine_nic_name\": \"nic-dr-mdcavdprdae02\",\n    \"gdep_virtual_machine_nic_ip\": \"10.27.11.5\",\n    \"gdep_virtual_machine_nic_subnet_resource_group\": \"dr-rg-gdep-pwus-vnets\",\n    \"gdep_virtual_machine_nic_vnet_name\": \"vnet-gdep-pwus-management\",\n    \"gdep_virtual_machine_nic_snet_name\": \"snet-gdep-pwus-management\"\n    // ...other properties...\n  }\n]\n</code></pre>"},{"location":"azure-restore-from-backup-part2/#summary","title":"Summary","text":"<ul> <li>This script automates the process of re-creating and attaching NICs to restored VMs in a DR region.</li> <li>Ensures each VM receives the correct IP address and network configuration for a true like-for-like DR.</li> <li>Use in conjunction with Part 1 for a complete BCP/DR restore workflow.</li> </ul> <p>You now have a fully automated, script-driven process for restoring Azure VMs and their network configuration across regions!</p>"},{"location":"deprovision-user/","title":"Automating User Deprovisioning in Microsoft 365: OneDrive and Mailbox Reassignment","text":"<p>When an employee leaves an organization, it's critical to deprovision their accounts securely and efficiently, while ensuring business continuity. This article demonstrates a Python-based approach to automate user deprovisioning in Microsoft 365, focusing on:</p> <ul> <li>Disabling accounts in Active Directory (AD/ADDS)</li> <li>Disabling accounts in Entra ID (Azure AD)</li> <li>Assigning the user's OneDrive and mailbox access to their manager</li> </ul> <p>We will walk through the following functions: - <code>terminate_employees_adds</code> - <code>terminate_employees_entra</code> - <code>grant_access_to_users_mailbox</code></p>"},{"location":"deprovision-user/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Required libraries:</li> <li><code>requests</code></li> <li><code>msal</code> (Microsoft Authentication Library)</li> <li><code>subprocess</code> (standard library)</li> <li><code>PnP.PowerShell</code> and <code>ExchangeOnlineManagement</code> PowerShell modules installed on a system with PowerShell Core</li> <li>Service principal with appropriate permissions in Microsoft 365</li> <li>Certificate-based authentication for automation</li> </ul>"},{"location":"deprovision-user/#1-disabling-users-in-active-directory-adadds","title":"1. Disabling Users in Active Directory (AD/ADDS)","text":"<p>The function <code>terminate_employees_adds</code> connects to AD and disables user accounts. This is typically done using the <code>ldap3</code> library, but the code can be adapted for other libraries or direct PowerShell calls.</p> <pre><code>def terminate_employees_adds(terminated_employees):\n    try:\n        conn = get_adds_Connection()\n        if not conn:\n            return\n        for employee in terminated_employees:\n            # Example: Disable the user account\n            user_dn = employee['distinguishedName']\n            conn.modify(user_dn, {'userAccountControl': [(ldap3.MODIFY_REPLACE, [514])]})\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n    finally:\n        if conn:\n            conn.unbind()\n</code></pre> <ul> <li>Explanation:</li> <li>Connects to AD using a service account.</li> <li>Iterates through the list of terminated employees.</li> <li>Sets the <code>userAccountControl</code> attribute to <code>514</code> (disabled).</li> </ul>"},{"location":"deprovision-user/#2-disabling-users-in-entra-id-azure-ad","title":"2. Disabling Users in Entra ID (Azure AD)","text":"<p>The function <code>terminate_employees_entra</code> disables the user in Entra ID and then delegates OneDrive and mailbox access to the user's manager.</p> <pre><code>def terminate_employees_entra(terminated_employees, existing_employees):\n    try:\n        existing_employee_dict = {str(employee['hr_PositionID']).lower(): employee for employee in existing_employees}\n        for terminated_employee in terminated_employees:\n            terminated_user_reportsto_id = terminated_employee['hr_ReportsToPositionID']\n            if str(terminated_user_reportsto_id).lower() in existing_employee_dict:\n                manager = existing_employee_dict[str(terminated_user_reportsto_id).lower()]\n                manager_email = manager['entra_mail']\n\n            entra_id = terminated_employee.get('entra_id')\n            terminated_user_email = terminated_employee.get('entra_userPrincipalName')\n            if entra_id:\n                entra_onedrive_url = grant_access_to_users_onedrive(\n                                        terminated_user_email, \n                                        manager_email)\n                grant_access_to_users_mailbox(\n                                        terminated_user_email, \n                                        manager_email)  \n                terminated_employee['entra_onedrive_url'] = entra_onedrive_url\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return False\n</code></pre> <ul> <li>Explanation:</li> <li>Looks up the manager for each terminated employee.</li> <li>Calls helper functions to delegate OneDrive and mailbox access to the manager.</li> </ul>"},{"location":"deprovision-user/#3-assigning-onedrive-access-to-the-manager","title":"3. Assigning OneDrive Access to the Manager","text":"<p>The function <code>grant_access_to_users_onedrive</code> uses Microsoft Graph API to get the user's OneDrive URL, then uses PowerShell to assign the manager as the site owner.</p> <pre><code>def grant_access_to_users_onedrive(terminated_user_email, manager_email):\n    try:\n        access_token = get_access_token_API_Access_AAD()\n        url = f\"https://graph.microsoft.com/v1.0/users/{terminated_user_email}/drive\"\n        response = requests.get(url, headers={'Authorization': f'Bearer {access_token}'})\n        response.raise_for_status()\n        graph_data = response.json()\n        user_onedrive_url = graph_data['webUrl']\n\n        command_to_run = f'''\n        Import-Module PnP.PowerShell\n        Connect-PnPOnline -Url &lt;SharePointRootUrl&gt; -ClientId &lt;AppId&gt; -Tenant &lt;TenantName&gt; -CertificatePath &lt;CertPath&gt;\n        $OneDriveSiteUrl = \"{user_onedrive_url.replace('/Documents', '')}\"\n        $SiteCollAdmin = \"{manager_email}\"\n        Set-PnPTenantSite -Url $OneDriveSiteUrl -Owners $SiteCollAdmin\n        '''\n        command_result = execute_powershell_command(command_to_run)\n        if command_result.returncode != 0:\n            return ''\n        return user_onedrive_url\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return ''\n</code></pre> <ul> <li>Explanation:</li> <li>Retrieves the user's OneDrive URL via Microsoft Graph.</li> <li>Uses PnP PowerShell to assign the manager as the site owner.</li> <li>Requires certificate-based authentication for automation.</li> </ul>"},{"location":"deprovision-user/#4-assigning-mailbox-access-to-the-manager","title":"4. Assigning Mailbox Access to the Manager","text":"<p>The function <code>grant_access_to_users_mailbox</code> uses PowerShell to grant the manager full access to the user's mailbox.</p> <pre><code>def grant_access_to_users_mailbox(terminated_user_email, manager_email):\n    try:\n        command_to_run = f'''\n        Import-Module ExchangeOnlineManagement\n        Connect-ExchangeOnline -CertificateFilePath &lt;CertPath&gt; -AppID &lt;AppId&gt; -Organization &lt;TenantName&gt;\n        Add-MailboxPermission -Identity {terminated_user_email} -User {manager_email} -AccessRights FullAccess -InheritanceType All\n        '''\n        command_result = execute_powershell_command(command_to_run)\n        if command_result.returncode != 0:\n            return ''\n        return command_result\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre> <ul> <li>Explanation:</li> <li>Connects to Exchange Online using certificate-based authentication.</li> <li>Grants the manager full access to the terminated user's mailbox.</li> </ul>"},{"location":"deprovision-user/#5-methodology-and-best-practices","title":"5. Methodology and Best Practices","text":"<ul> <li>Automation: All steps are automated to reduce manual errors and speed up offboarding.</li> <li>Security: Uses certificate-based authentication for all automated PowerShell and Graph API calls.</li> <li>Separation of Duties: The terminated user's data is not deleted immediately, but access is reassigned to their manager for business continuity.</li> <li>Error Handling: All functions use a global exception handler to log and notify on errors.</li> </ul>"},{"location":"deprovision-user/#6-required-libraries-and-tools","title":"6. Required Libraries and Tools","text":"<ul> <li><code>requests</code> for REST API calls</li> <li><code>msal</code> for Microsoft authentication</li> <li><code>ldap3</code> for AD/LDAP operations</li> <li><code>PnP.PowerShell</code> and <code>ExchangeOnlineManagement</code> PowerShell modules</li> <li>Service principal with delegated permissions and certificate</li> </ul>"},{"location":"deprovision-user/#7-conclusion","title":"7. Conclusion","text":"<p>Automating user deprovisioning in Microsoft 365 ensures compliance, security, and business continuity. By delegating OneDrive and mailbox access to managers, organizations can maintain access to critical data while protecting sensitive information.</p> <p>Note: - Replace placeholders like <code>&lt;SharePointRootUrl&gt;</code>, <code>&lt;AppId&gt;</code>, <code>&lt;TenantName&gt;</code>, and <code>&lt;CertPath&gt;</code> with your actual values. - Always test automation in a non-production environment before rolling out to production.</p>"},{"location":"devops-build-container/","title":"Building the Container Image: Dockerfile Explained","text":"<p>This article explains how to build the container image for your Azure Container Instance, focusing on the <code>Dockerfile</code> and <code>requirements.txt</code>. This is the foundation for running your application in the cloud. For deploying and running the image using Bicep and GitHub Actions, see the related articles linked below.</p>"},{"location":"devops-build-container/#overview","title":"Overview","text":"<p>The container image is built using a <code>Dockerfile</code> and a <code>requirements.txt</code> file. The Dockerfile defines the environment, dependencies, and setup steps, while <code>requirements.txt</code> lists the Python packages needed by your application.</p>"},{"location":"devops-build-container/#dockerfile-walkthrough","title":"Dockerfile Walkthrough","text":"<p>The main Dockerfile is located at <code>compute/ci/interface/container/Dockerfile</code>.</p>"},{"location":"devops-build-container/#key-sections","title":"Key Sections","text":"<pre><code># Use a specific version of the base image\nFROM mcr.microsoft.com/devcontainers/python:3\n\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Install necessary packages\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y \\\n    supervisor \\\n    unzip \\\n    build-essential \\\n    python3-dev \\\n    git \\\n    curl \\\n    gnupg \\\n    snmp \n\n# Add Microsoft GPG key and SQL Server repository\nRUN curl -fsSL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg\nRUN curl https://packages.microsoft.com/config/debian/12/prod.list | tee /etc/apt/sources.list.d/mssql-release.list\n\n# Install Microsoft SQL Server related packages\nRUN apt-get update &amp;&amp; \\\n    ACCEPT_EULA=Y apt-get install -y \\\n    msodbcsql18 \\\n    mssql-tools18 \n\n# Install PowerShell Core and modules\nRUN curl -fsSL https://packages.microsoft.com/config/debian/12/packages-microsoft-prod.deb -o packages-microsoft-prod.deb &amp;&amp; \\\ndpkg -i packages-microsoft-prod.deb &amp;&amp; \\\nrm packages-microsoft-prod.deb &amp;&amp; \\\napt-get update &amp;&amp; \\\napt-get install -y powershell\n\nRUN pwsh -Command Install-Module PnP.PowerShell -Force -AllowClobber -SkipPublisherCheck\nRUN pwsh -Command Install-Module ExchangeOnlineManagement -Force -AllowClobber -SkipPublisherCheck\nRUN pwsh -Command Install-Module MicrosoftTeams -Force -AllowClobber -SkipPublisherCheck\n\n# Clean up\nRUN apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Add MS SQL Server tools to PATH\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' &gt;&gt; ~/.bashrc\n\n# Supervisor configuration\nRUN mkdir -p /etc/supervisor/conf.d\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# Copy and install Python dependencies\nCOPY requirements.txt .\nRUN python -m pip install -r requirements.txt\n\n# Clone the private repository using a build argument\nARG GITHUB_PAT\nRUN git clone https://$GITHUB_PAT@github.com/GDEnergyproducts/GDEP-INTFC.git /app\n\nWORKDIR /app\n\nRUN chmod +x scripts/container/setuppyrfc.sh\nRUN chmod +x scripts/container/mountstorage.sh\nRUN ./scripts/container/setuppyrfc.sh\n\n# Run tests\nRUN pytest\n\n# Start supervisor\nCMD [\"/usr/bin/supervisord\"]\n</code></pre> <p>Explanation: - Base Image: Uses a Python dev container as the base. - System Packages: Installs tools for Python, SQL Server, PowerShell, and more. - PowerShell Modules: Installs modules for Azure and Microsoft 365 automation. - Python Dependencies: Installs all Python packages listed in <code>requirements.txt</code>. - Source Code: Clones the application code from a private GitHub repository using a build argument for authentication. - Setup Scripts: Makes and runs setup scripts executable. - Testing: Runs <code>pytest</code> to ensure the build is valid. - Supervisor: Uses Supervisor to manage processes in the container.</p>"},{"location":"devops-build-container/#requirementstxt","title":"requirements.txt","text":"<p>This file lists all Python dependencies needed by your application. It is copied into the image and installed with pip. Keeping dependencies in this file makes builds reproducible and easy to update.</p>"},{"location":"devops-build-container/#building-the-image-locally","title":"Building the Image Locally","text":"<p>To build the image locally, run:</p> <pre><code>docker build --build-arg GITHUB_PAT=your_token_here -t myacr.azurecr.io/myimage:latest compute/ci/interface/container\n</code></pre> <p>Replace <code>your_token_here</code> with a valid GitHub personal access token.</p>"},{"location":"devops-build-container/#related-articles","title":"Related Articles","text":"<ul> <li>How to Deploy an Azure Container Instance Using Bicep (IaC)</li> <li>How to Run Your Azure Container Instance Bicep Deployment</li> </ul>"},{"location":"devops-deploy-container-github-action/","title":"How to Run Your Azure Container Instance Bicep Deployment","text":"<p>This article explains how to execute your Bicep-based Azure Container Instance deployment, both via GitHub Actions and from the command line.</p>"},{"location":"devops-deploy-container-github-action/#running-with-github-actions","title":"Running with GitHub Actions","text":"<p>The workflow file <code>.github/workflows/ciinterface.yml</code> automates the build, push, and deployment process.</p>"},{"location":"devops-deploy-container-github-action/#key-steps-in-the-workflow","title":"Key Steps in the Workflow","text":"<pre><code>name: Interface Container Instance\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n      - name: Azure key vault - Get Secret\n        uses: Azure/get-keyvault-secrets@v1\n        with:\n          keyvault: ${{ vars.AZURE_KEY_VAULT_NAME }}\n          secrets: patgdepintfcrepo\n        id: getAZKVpatgdepiacrepo\n\n      - name: Build Docker image\n        run: |\n          docker build --build-arg GITHUB_PAT=\"${{ steps.getAZKVpatgdepiacrepo.outputs.patgdepintfcrepo }}\" \\\n                       -t ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }}/${{ vars.IMAGE_NAME }}:latest \\\n                       compute/ci/interface/container\n\n      - name: Login to Azure Container Registry\n        run: |\n          echo $ACR_PASSWORD | docker login ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }} -u $ACR_USERNAME --password-stdin\n        env:\n          ACR_USERNAME: ${{ vars.ACR_USERNAME }}\n          ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}\n\n      - name: Push Docker image to Azure Container Registry\n        run: docker push ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }}/${{ vars.IMAGE_NAME }}:latest\n\n      - name: Deploy the bicep file\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        uses: azure/arm-deploy@v1\n        with:\n          scope: 'resourcegroup'\n          subscriptionId: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          resourceGroupName: rg-gdep-peus-2delete\n          template: ./compute/ci/interface/interface.bicep\n          parameters: ./compute/ci/interface/interface-dev.parameters.json\n</code></pre> <p>Explanation: - Build and Push: Docker image is built and pushed to Azure Container Registry. - Secrets: Pulled securely from Azure Key Vault. - Deploy: The Bicep template is deployed using the specified parameters file.</p>"},{"location":"devops-deploy-container-github-action/#running-from-the-command-line","title":"Running from the Command Line","text":"<p>You can also deploy the Bicep template directly using Azure CLI:</p> <pre><code>az login\naz account set --subscription \"&lt;your-subscription-id&gt;\"\naz deployment group create \\\n  --resource-group &lt;your-resource-group&gt; \\\n  --template-file ./compute/ci/interface/interface.bicep \\\n  --parameters @./compute/ci/interface/interface-dev.parameters.json\n</code></pre> <p>Tips: - Make sure you have the Azure CLI and Bicep CLI installed. - Use Key Vault references in your parameters file for secrets.</p>"},{"location":"devops-deploy-container-github-action/#related-articles","title":"Related Articles","text":"<ul> <li>How to Build Your Container Image</li> <li>How to Deploy an Azure Container Instance Using Bicep (IaC)</li> </ul>"},{"location":"devops-deploy-container-iac/","title":"Deploying Azure Container Instances with BICEP (IaC)","text":"<p>This article demonstrates how to use Infrastructure as Code (IaC) with BICEP to deploy an Azure Container Instance (ACI). We'll walk through the main BICEP template, explain each section, and highlight best practices such as using Azure Key Vault for secrets management.</p>"},{"location":"devops-deploy-container-iac/#introduction","title":"Introduction","text":"<p>Azure Container Instances provide a fast and simple way to run containers in Azure, without managing virtual machines. Using BICEP, you can define your container infrastructure as code, making deployments repeatable and version-controlled.</p>"},{"location":"devops-deploy-container-iac/#bicep-template-overview","title":"BICEP Template Overview","text":"<p>The main BICEP file is <code>compute/ci/interface/interface.bicep</code>. It defines all the parameters, secrets, and resources needed to deploy an ACI.</p>"},{"location":"devops-deploy-container-iac/#parameters","title":"Parameters","text":"<pre><code>param image_name string \nparam image_tag string \nparam container_registry_name string \nparam aci_name string\nparam subnet_name string\nparam tags object\nparam subnet_id string\nparam volume_name string \n\n@secure()\nparam azure_file_share_name string\n@secure()\nparam azure_file_share_user_name string\n@secure()\nparam application_interface_clientid string\n@secure()\nparam application_interface_clientsecret string\n@secure()\nparam application_interface_tenantid string\n@secure()\nparam container_registry_password string\n@secure()\nparam azure_file_share_password string\n@secure()\nparam ukg_encrypt_passphrase string\nparam location string = resourceGroup().location\nparam containerRegistryServer string = '${container_registry_name}.azurecr.io' \n</code></pre> <p>Explanation: - Parameters allow you to customize the deployment. - <code>@secure()</code> marks secrets so they are not logged or exposed. - Many parameters are injected from Azure Key Vault for security.</p>"},{"location":"devops-deploy-container-iac/#variables","title":"Variables","text":"<pre><code>var container_registry_username  = container_registry_name\n</code></pre> <p>Explanation: - Sets the registry username to the registry name for convenience.</p>"},{"location":"devops-deploy-container-iac/#resource-definition","title":"Resource Definition","text":"<pre><code>resource aci 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  name: aci_name\n  location: location\n  tags: tags\n\n  properties: {\n    containers: [\n      {\n        name: aci_name\n        properties: {\n          image: '${containerRegistryServer}/${image_name}:${image_tag}'\n          ports: [\n            {\n              port: 80\n              protocol: 'TCP'\n            }\n          ]\n          resources: {\n            limits: {\n              cpu: 4\n              memoryInGB: json('16')\n            }\n            requests: {\n              cpu: 4\n              memoryInGB: json('16')\n            }\n          }\n          environmentVariables:[\n            {\n              name:'application_interface_clientid'\n              secureValue:application_interface_clientid\n            }\n            {\n              name:'application_interface_clientsecret'\n              secureValue:application_interface_clientsecret\n            }\n            {\n              name:'application_interface_tenantid'\n              secureValue:application_interface_tenantid\n            }\n            {\n              name:'azure_file_share_name'\n              secureValue:azure_file_share_name\n            }\n            {\n              name:'azure_file_share_user_name'\n              secureValue:azure_file_share_user_name\n            }\n            {\n              name:'azure_file_share_password'\n              secureValue:azure_file_share_password\n            }\n            {\n              name:'ukg_encrypt_passphrase'\n              secureValue:ukg_encrypt_passphrase\n            }\n          ]\n          volumeMounts: [\n            {\n              name: volume_name\n              mountPath: '/mnt/azure'\n              readOnly: false\n            }\n          ]\n        }\n      }\n    ]\n    osType: 'Linux'\n    imageRegistryCredentials: [\n      {\n        server: containerRegistryServer\n        username: container_registry_username\n        password: container_registry_password\n      }\n    ]\n    volumes: [\n      {\n        name: volume_name \n        azureFile: {\n          shareName: azure_file_share_name\n          storageAccountName: azure_file_share_user_name\n          storageAccountKey: azure_file_share_password\n        }\n      }\n    ]\n    priority: 'Regular'\n    restartPolicy: 'Never'\n    sku: 'Standard'\n    subnetIds: [\n      {\n        id: subnet_id\n        name: subnet_name\n      }\n    ]\n    dnsConfig: {\n      nameServers: [\n        '10.27.11.4'\n        '10.27.11.5'\n        '168.63.129.16'  \n      ]\n    }\n  }\n}\n</code></pre> <p>Explanation: - Container Definition: Specifies the image, ports, resources, environment variables, and volume mounts. - Resources: Both <code>limits</code> and <code>requests</code> are set to 4 CPUs and 16GB RAM. - Environment Variables: Secure values are injected from Key Vault. - Volumes: Azure File Share is mounted for persistent storage. - Image Registry Credentials: Pulled securely from parameters. - Networking: Subnet and DNS configuration are specified.</p>"},{"location":"devops-deploy-container-iac/#parameters-file","title":"Parameters File","text":"<p>The <code>interface-dev.parameters.json</code> file provides all parameter values for deployment. This keeps secrets and environment-specific values out of the main template.</p> <p>Advantages: - Separation of Concerns: Code and configuration are separated. - Reusability: Use different parameter files for different environments.</p>"},{"location":"devops-deploy-container-iac/#why-use-azure-key-vault","title":"Why Use Azure Key Vault?","text":"<ul> <li>Security: Secrets like passwords and client secrets are never stored in source control.</li> <li>Centralized Management: Rotate secrets in one place without updating code.</li> <li>Integration: BICEP and Azure Resource Manager can reference Key Vault secrets directly.</li> </ul>"},{"location":"devops-deploy-container-iac/#related-articles","title":"Related Articles","text":"<ul> <li>How to Build Your Container Image</li> <li>How to Run Your Azure Container Instance Bicep Deployment</li> </ul>"},{"location":"email-o365-proofpoint-part1/","title":"\ud83d\udce7 Email Security Implementation (O365 and Proof Point Essentials) Series","text":""},{"location":"email-o365-proofpoint-part1/#part-1-understanding-spf-dkim-and-dmarc-fundamentals","title":"Part 1: Understanding SPF, DKIM, and DMARC Fundamentals","text":""},{"location":"email-o365-proofpoint-part1/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC (Current)</li> <li>Part 2: DNS Configuration and Setup</li> <li>Part 3: Office 365 Connector Configuration</li> <li>Part 4: Proofpoint Integration Setup</li> <li>Part 5: Testing and Troubleshooting</li> </ul>"},{"location":"email-o365-proofpoint-part1/#what-youll-learn","title":"\ud83c\udfaf What You'll Learn","text":"<p>By the end of this series, you'll have a complete understanding of how to implement enterprise-grade email security using SPF, DKIM, DMARC, and integrate Office 365 with Proofpoint Essentials for comprehensive email protection.</p>"},{"location":"email-o365-proofpoint-part1/#understanding-email-authentication-protocols","title":"\ud83d\udd10 Understanding Email Authentication Protocols","text":"<p>Email security is critical in today's threat landscape. Without proper authentication, your domain can be spoofed, your users can receive malicious emails, and your organization's reputation can be damaged. Let's dive into the three pillars of email authentication.</p>"},{"location":"email-o365-proofpoint-part1/#spf-sender-policy-framework","title":"SPF (Sender Policy Framework)","text":""},{"location":"email-o365-proofpoint-part1/#what-is-spf","title":"What is SPF?","text":"<p>SPF is like a guest list for your email domain. It tells receiving email servers which IP addresses and mail servers are authorized to send emails on behalf of your domain.</p>"},{"location":"email-o365-proofpoint-part1/#why-spf-matters","title":"Why SPF Matters:","text":"<ul> <li>\u2705 Prevents Domain Spoofing: Stops bad actors from sending emails that appear to come from your domain</li> <li>\u2705 Improves Deliverability: Legitimate emails are more likely to reach the inbox</li> <li>\u2705 Reduces Spam: Helps email providers identify and block fraudulent emails</li> </ul>"},{"location":"email-o365-proofpoint-part1/#how-spf-works","title":"How SPF Works:","text":"<ol> <li>You publish an SPF record in your DNS</li> <li>When someone receives an email claiming to be from your domain, their email server checks your SPF record</li> <li>If the sending server's IP is in your SPF record \u2192 \u2705 PASS</li> <li>If the sending server's IP is NOT in your SPF record \u2192 \u274c FAIL</li> </ol>"},{"location":"email-o365-proofpoint-part1/#dkim-domainkeys-identified-mail","title":"DKIM (DomainKeys Identified Mail)","text":""},{"location":"email-o365-proofpoint-part1/#what-is-dkim","title":"What is DKIM?","text":"<p>DKIM is like a digital signature for your emails. It uses cryptographic signatures to verify that an email actually came from your domain and hasn't been tampered with during transit.</p>"},{"location":"email-o365-proofpoint-part1/#why-dkim-matters","title":"Why DKIM Matters:","text":"<ul> <li>\u2705 Email Integrity: Ensures emails haven't been modified in transit</li> <li>\u2705 Authentication: Proves the email really came from your domain</li> <li>\u2705 Trust Building: Increases recipient confidence in your emails</li> </ul>"},{"location":"email-o365-proofpoint-part1/#how-dkim-works","title":"How DKIM Works:","text":"<ol> <li>Your email server signs outgoing emails with a private key</li> <li>You publish the corresponding public key in your DNS</li> <li>Receiving servers use the public key to verify the signature</li> <li>If signature matches \u2192 \u2705 PASS | If not \u2192 \u274c FAIL</li> </ol>"},{"location":"email-o365-proofpoint-part1/#dmarc-domain-based-message-authentication-reporting-conformance","title":"DMARC (Domain-based Message Authentication, Reporting &amp; Conformance)","text":""},{"location":"email-o365-proofpoint-part1/#what-is-dmarc","title":"What is DMARC?","text":"<p>DMARC is the policy enforcer that tells receiving email servers what to do when SPF or DKIM checks fail. It also provides valuable reporting on email authentication results.</p>"},{"location":"email-o365-proofpoint-part1/#why-dmarc-matters","title":"Why DMARC Matters:","text":"<ul> <li>\u2705 Policy Enforcement: Actively blocks fraudulent emails</li> <li>\u2705 Visibility: Provides reports on who's sending emails using your domain</li> <li>\u2705 Gradual Implementation: Allows you to monitor before enforcing</li> </ul>"},{"location":"email-o365-proofpoint-part1/#dmarc-policies","title":"DMARC Policies:","text":"<ul> <li><code>p=none</code>: Monitor only (recommended for initial setup)</li> <li><code>p=quarantine</code>: Send suspicious emails to spam folder</li> <li><code>p=reject</code>: Completely block suspicious emails</li> </ul>"},{"location":"email-o365-proofpoint-part1/#implementation-strategy-overview","title":"\ud83c\udfd7\ufe0f Implementation Strategy Overview","text":""},{"location":"email-o365-proofpoint-part1/#the-logical-implementation-order","title":"The Logical Implementation Order","text":"<pre><code>graph TD\n    A[Phase 1: DNS Foundation] --&gt; B[Phase 2: Service Integration]\n    B --&gt; C[Phase 3: Advanced Features]\n    C --&gt; D[Phase 4: Monitoring &amp; Hardening]\n\n    A --&gt; A1[Setup SPF with ~all]\n    A --&gt; A2[Configure DKIM keys]\n    A --&gt; A3[Implement DMARC p=none]\n\n    B --&gt; B1[Configure Proofpoint]\n    B --&gt; B2[Setup O365 Connectors]\n    B --&gt; B3[Create Transport Rules]\n\n    C --&gt; C1[Enable Security Features]\n    C --&gt; C2[Configure Warning Tags]\n    C --&gt; C3[Setup User Access]\n\n    D --&gt; D1[Monitor DMARC Reports]\n    D --&gt; D2[Harden Policies]\n    D --&gt; D3[Switch to -all]\n</code></pre>"},{"location":"email-o365-proofpoint-part1/#why-this-order-matters","title":"Why This Order Matters","text":"<ol> <li>Start Soft, Finish Hard: Begin with monitoring (<code>~all</code>, <code>p=none</code>) before enforcing hard policies</li> <li>Foundation First: DNS records must be in place before services can use them</li> <li>Test Everything: Each phase includes testing before moving to the next</li> <li>Minimize Disruption: Gradual implementation reduces the risk of blocking legitimate emails</li> </ol>"},{"location":"email-o365-proofpoint-part1/#important-considerations-before-starting","title":"\u26a0\ufe0f Important Considerations Before Starting","text":""},{"location":"email-o365-proofpoint-part1/#planning-phase","title":"Planning Phase","text":"<ul> <li>\ud83d\udccb Document Current Setup: Know your existing mail flow</li> <li>\ud83d\udd52 Plan Maintenance Window: Some changes may briefly affect mail delivery</li> <li>\ud83d\udd04 Prepare Rollback Plan: Know how to quickly revert changes if needed</li> <li>\ud83d\udc65 Stakeholder Communication: Inform users about potential temporary issues</li> </ul>"},{"location":"email-o365-proofpoint-part1/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>\u274c Don't start with hard policies (<code>-all</code>, <code>p=reject</code>)</li> <li>\u274c Don't skip testing phases</li> <li>\u274c Don't implement everything at once</li> <li>\u274c Don't ignore DMARC reports</li> </ul>"},{"location":"email-o365-proofpoint-part1/#whats-next","title":"\ud83c\udfaf What's Next","text":"<p>In Part 2, we'll dive into the actual DNS configuration, including: - Setting up SPF records with proper syntax - Handling SPF character limits with multiple records - Configuring DKIM keys from multiple sources - Implementing DMARC with proper reporting</p>"},{"location":"email-o365-proofpoint-part1/#series-navigation_1","title":"\ud83d\udcd6 Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC (Current)</li> <li>Part 2: DNS Configuration and Setup \u2192</li> </ul> <p>This series covers enterprise-level email security implementation. Always test in a non-production environment first and have a rollback plan ready.</p>"},{"location":"email-o365-proofpoint-part2/","title":"\ud83d\udce7 Email Security Implementation (O365 and Proof Point Essentials) Series","text":""},{"location":"email-o365-proofpoint-part2/#part-2-dns-configuration-and-setup","title":"Part 2: DNS Configuration and Setup","text":""},{"location":"email-o365-proofpoint-part2/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC</li> <li>Part 2: DNS Configuration and Setup (Current)</li> <li>Part 3: Office 365 Connector Configuration</li> <li>Part 4: Proofpoint Integration Setup</li> <li>Part 5: Testing and Troubleshooting</li> </ul>"},{"location":"email-o365-proofpoint-part2/#what-well-configure","title":"\ud83c\udfaf What We'll Configure","text":"<p>In this part, we'll set up all the DNS records needed for our email security implementation: - SPF records (including handling character limits) - DKIM keys from multiple sources - DMARC policy with reporting - MX record for mail routing</p>"},{"location":"email-o365-proofpoint-part2/#dns-records-overview","title":"\ud83d\udccb DNS Records Overview","text":"<p>Here's what we'll be implementing in your DNS (using Cloudflare as an example):</p> Record Type Name Purpose MX yourdomain.com Route mail through Proofpoint TXT yourdomain.com Main SPF record TXT _spf1.yourdomain.com Additional SPF IPs (Part 1) TXT _spf2.yourdomain.com Additional SPF IPs (Part 2) TXT _dmarc.yourdomain.com DMARC policy CNAME hs1-19543953._domainkey Office 365 DKIM key CNAME hs2-19543953._domainkey Office 365 DKIM key TXT selector-1678913997._domainkey Proofpoint DKIM key"},{"location":"email-o365-proofpoint-part2/#mx-record-configuration","title":"\ud83d\udce7 MX Record Configuration","text":""},{"location":"email-o365-proofpoint-part2/#step-1-update-mx-record","title":"Step 1: Update MX Record","text":"<p>The MX record tells the internet where to deliver emails for your domain. We'll route everything through Proofpoint first.</p> <pre><code>Type: MX\nName: yourdomain.com\nValue: mx2-us1.ppe-hosted.com\nPriority: 10\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#why-this-matters","title":"Why This Matters:","text":"<ul> <li>All inbound emails will go to Proofpoint first for filtering</li> <li>Proofpoint will then forward clean emails to Office 365</li> <li>This prevents bad actors from bypassing your security by sending directly to O365</li> </ul>"},{"location":"email-o365-proofpoint-part2/#spf-record-configuration","title":"\ud83d\udee1\ufe0f SPF Record Configuration","text":""},{"location":"email-o365-proofpoint-part2/#the-challenge-character-limits","title":"The Challenge: Character Limits","text":"<p>SPF records have a 255-character limit, but we need to include many IP addresses and services. The solution is to split our SPF record across multiple DNS entries.</p>"},{"location":"email-o365-proofpoint-part2/#step-2-main-spf-record","title":"Step 2: Main SPF Record","text":"<pre><code>Type: TXT\nName: yourdomain.com\nValue: \"v=spf1 a:dispatch-us.ppe-hosted.com ip4:20.81.4.12 include:spf.protection.outlook.com include:19543953.spf07.hubspotemail.net include:_spf1.yourdomain.com include:_spf2.yourdomain.com include:sendgrid.net ~all\"\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#breaking-down-this-record","title":"Breaking Down This Record:","text":"<ul> <li><code>v=spf1</code>: SPF version identifier</li> <li><code>a:dispatch-us.ppe-hosted.com</code>: Allow Proofpoint's dispatch server</li> <li><code>ip4:XX.XX.XX.XX</code>: Specific IP address (likely internal system)</li> <li><code>include:spf.protection.outlook.com</code>: Allow Office 365 to send</li> <li><code>include:19543953.spf07.hubspotemail.net</code>: Allow HubSpot to send</li> <li><code>include:_spf1.yourdomain.com</code>: Reference to our first IP list</li> <li><code>include:_spf2.yourdomain.com</code>: Reference to our second IP list</li> <li><code>include:sendgrid.net</code>: Allow SendGrid to send</li> <li><code>~all</code>: Soft fail for all other sources (monitoring mode)</li> </ul>"},{"location":"email-o365-proofpoint-part2/#step-3-first-ip-address-block-_spf1","title":"Step 3: First IP Address Block (_spf1)","text":"<pre><code>Type: TXT\nName: _spf1.yourdomain.com\nValue: \"v=spf1 ip4:XXX.YYY.XX.YYY ip4:XXX.XXX.XXX.XXX\"\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#step-4-second-ip-address-block-_spf2","title":"Step 4: Second IP Address Block (_spf2)","text":"<pre><code>Type: TXT\nName: _spf2.yourdomain.com\nValue: \"v=spf1 ip4:XXX.YYY.XX.YYY ip4:XXX.XXX.XXX.XXX\"\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#what-these-ips-represent","title":"What These IPs Represent:","text":"<p>These are your company's trusted locations and public IP addresses, including: - Office locations - Data centers - Remote offices - Any systems that need to send email directly</p> <p>\u26a0\ufe0f Important Note: These same IP addresses will appear in your Office 365 SMTP Relay connector configuration. This alignment is crucial for proper email flow.</p>"},{"location":"email-o365-proofpoint-part2/#dkim-configuration","title":"\ud83d\udd11 DKIM Configuration","text":"<p>DKIM requires both public and private keys. The private keys stay on your email servers, while public keys go in DNS.</p>"},{"location":"email-o365-proofpoint-part2/#step-5-office-365-dkim-keys-cnames","title":"Step 5: Office 365 DKIM Keys (CNAMEs)","text":"<p>Office 365 manages these keys for you, so we create CNAME records that point to Microsoft's infrastructure:</p> <pre><code>Type: CNAME\nName: hs1-19543953._domainkey\nValue: hs1-19543953._domainkey.yourdomain.onmicrosoft.com\n\nType: CNAME\nName: hs2-19543953._domainkey\nValue: hs2-19543953._domainkey.yourdomain.onmicrosoft.com\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#finding-your-o365-dkim-values","title":"Finding Your O365 DKIM Values:","text":"<ol> <li>Go to Microsoft 365 Admin Center</li> <li>Navigate to Setup &gt; Domains</li> <li>Select your domain</li> <li>Look for DKIM configuration section</li> <li>Copy the CNAME values provided</li> </ol>"},{"location":"email-o365-proofpoint-part2/#step-6-proofpoint-dkim-key-txt","title":"Step 6: Proofpoint DKIM Key (TXT)","text":"<p>This is a TXT record with the public key that Proofpoint generates:</p> <pre><code>Type: TXT\nName: selector-1678913997._domainkey\nValue: \"v=DKIM1; k=rsa; t=s; n=core; p=publickeygoeshere\"\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#how-to-get-your-proofpoint-dkim-key","title":"How to Get Your Proofpoint DKIM Key:","text":"<ol> <li>Log into Proofpoint Essentials admin portal</li> <li>Navigate to Email &gt; Domains</li> <li>Select your domain</li> <li>Look for DKIM configuration section</li> <li>Generate or copy the public key</li> <li>The selector name will be provided by Proofpoint</li> </ol>"},{"location":"email-o365-proofpoint-part2/#dmarc-configuration","title":"\ud83d\udcca DMARC Configuration","text":"<p>DMARC ties SPF and DKIM together and provides valuable reporting.</p>"},{"location":"email-o365-proofpoint-part2/#step-7-dmarc-policy-record","title":"Step 7: DMARC Policy Record","text":"<pre><code>Type: TXT\nName: _dmarc.yourdomain.com\nValue: \"v=DMARC1; p=quarantine; fo=1; rua=mailto:84c81b71e65344cfb4a5900d6c64d628@dmarc-reports.cloudflare.net,mailto:admin@yourdomain.com\"\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#breaking-down-this-record_1","title":"Breaking Down This Record:","text":"<ul> <li><code>v=DMARC1</code>: DMARC version</li> <li><code>p=quarantine</code>: Policy for emails that fail authentication (send to spam)</li> <li><code>fo=1</code>: Forensic reporting options (generate reports on failure)</li> <li><code>rua=mailto:...</code>: Where to send aggregate reports</li> </ul>"},{"location":"email-o365-proofpoint-part2/#dmarc-policy-evolution","title":"DMARC Policy Evolution:","text":"<p>Start with <code>p=none</code> for monitoring, then gradually move to: 1. <code>p=none</code> \u2192 Monitor and collect data (recommended start) 2. <code>p=quarantine</code> \u2192 Send suspicious emails to spam 3. <code>p=reject</code> \u2192 Completely block suspicious emails (final goal)</p>"},{"location":"email-o365-proofpoint-part2/#implementation-steps","title":"\ud83d\udee0\ufe0f Implementation Steps","text":""},{"location":"email-o365-proofpoint-part2/#phase-1-preparation","title":"Phase 1: Preparation","text":"<ol> <li>Document existing records - Take screenshots of current DNS</li> <li>Lower TTL values - Set TTL to 300 (5 minutes) for faster changes</li> <li>Plan timing - Implement during low-traffic hours</li> </ol>"},{"location":"email-o365-proofpoint-part2/#phase-2-implementation-order","title":"Phase 2: Implementation Order","text":"<ol> <li>Add the split SPF records first (_spf1 and _spf2)</li> <li>Update the main SPF record</li> <li>Add MX record</li> <li>Configure DKIM records</li> <li>Add DMARC record (start with <code>p=none</code>)</li> </ol>"},{"location":"email-o365-proofpoint-part2/#phase-3-verification","title":"Phase 3: Verification","text":"<p>After each record, verify using online tools: - SPF: <code>dig TXT yourdomain.com</code> - DKIM: <code>dig TXT selector._domainkey.yourdomain.com</code> - DMARC: <code>dig TXT _dmarc.yourdomain.com</code></p>"},{"location":"email-o365-proofpoint-part2/#dns-verification-commands","title":"\ud83d\udd0d DNS Verification Commands","text":""},{"location":"email-o365-proofpoint-part2/#using-dig-linuxmacwindows-with-wsl","title":"Using dig (Linux/Mac/Windows with WSL):","text":"<pre><code># Check SPF record\ndig TXT yourdomain.com\n\n# Check DKIM records\ndig TXT hs1-19543953._domainkey.yourdomain.com\ndig TXT selector-1678913997._domainkey.yourdomain.com\n\n# Check DMARC record\ndig TXT _dmarc.yourdomain.com\n\n# Check MX record\ndig MX yourdomain.com\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#using-nslookup-windows","title":"Using nslookup (Windows):","text":"<pre><code># Check SPF record\nnslookup -type=TXT yourdomain.com\n\n# Check DMARC record\nnslookup -type=TXT _dmarc.yourdomain.com\n</code></pre>"},{"location":"email-o365-proofpoint-part2/#common-pitfalls-and-solutions","title":"\u26a0\ufe0f Common Pitfalls and Solutions","text":""},{"location":"email-o365-proofpoint-part2/#spf-issues","title":"SPF Issues:","text":"<ul> <li>Too many DNS lookups: SPF has a 10-lookup limit</li> <li>Multiple SPF records: Only one SPF record per domain allowed</li> <li>Character limits: Use includes and split records as shown</li> </ul>"},{"location":"email-o365-proofpoint-part2/#dkim-issues","title":"DKIM Issues:","text":"<ul> <li>Selector mismatch: Ensure selector names match between DNS and service</li> <li>Key formatting: Remove spaces and line breaks from public keys</li> </ul>"},{"location":"email-o365-proofpoint-part2/#dmarc-issues","title":"DMARC Issues:","text":"<ul> <li>Invalid syntax: Use DMARC validators before publishing</li> <li>Missing prerequisites: SPF and DKIM must work before DMARC is effective</li> </ul>"},{"location":"email-o365-proofpoint-part2/#verification-checklist","title":"\u2705 Verification Checklist","text":"<p>Before moving to Part 3, ensure: - [ ] All DNS records are published and propagated - [ ] SPF record includes all necessary services and IPs - [ ] DKIM keys are properly formatted and accessible - [ ] DMARC record has correct syntax and reporting addresses - [ ] MX record points to Proofpoint - [ ] You have access to DMARC reports</p>"},{"location":"email-o365-proofpoint-part2/#whats-next","title":"\ud83c\udfaf What's Next","text":"<p>With DNS configured, we'll move to Part 3 where we'll set up the Office 365 connectors that work with these DNS records to ensure proper mail flow and security.</p>"},{"location":"email-o365-proofpoint-part2/#series-navigation_1","title":"\ud83d\udcd6 Series Navigation","text":"<ul> <li>\u2190 Part 1: Understanding SPF, DKIM, and DMARC</li> <li>Part 2: DNS Configuration and Setup (Current)</li> <li>Part 3: Office 365 Connector Configuration \u2192</li> </ul> <p>Remember to always test DNS changes in a non-production environment first and monitor email flow carefully during implementation.</p>"},{"location":"email-o365-proofpoint-part3/","title":"\ud83d\udce7 Email Security Implementation (O365 and Proof Point Essentials) Series","text":""},{"location":"email-o365-proofpoint-part3/#part-3-office-365-connector-configuration","title":"Part 3: Office 365 Connector Configuration","text":""},{"location":"email-o365-proofpoint-part3/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC</li> <li>Part 2: DNS Configuration and Setup</li> <li>Part 3: Office 365 Connector Configuration (Current)</li> <li>Part 4: Proofpoint Integration Setup</li> <li>Part 5: Testing and Troubleshooting</li> </ul>"},{"location":"email-o365-proofpoint-part3/#what-well-configure","title":"\ud83c\udfaf What We'll Configure","text":"<p>In this part, we'll set up three critical Office 365 connectors and a transport rule: 1. Proofpoint Inbound Connector - Receives filtered email from Proofpoint 2. ProofPoint Outbound Connector - Sends outbound email through Proofpoint 3. SMTP Relay Connector - Allows internal systems to send email 4. ProofPoint Spam Bypass Rule - Prevents double-scanning of emails</p>"},{"location":"email-o365-proofpoint-part3/#mail-flow-architecture","title":"\ud83d\udd04 Mail Flow Architecture","text":"<p>Understanding the mail flow is crucial before configuration:</p> <pre><code>graph TB\n    subgraph \"Internet\"\n        EXT[External Sender]\n        INT[Internal Apps/Printers]\n    end\n\n    subgraph \"Proofpoint Essentials\"\n        PP[Proofpoint Filtering]\n    end\n\n    subgraph \"Office 365\"\n        EOP[Exchange Online Protection]\n        EXO[Exchange Online]\n        USERS[User Mailboxes]\n    end\n\n    EXT --&gt; PP\n    PP --&gt; EOP\n    EOP --&gt; EXO\n    EXO --&gt; USERS\n\n    INT --&gt; EOP\n    EXO --&gt; PP\n    PP --&gt; EXT\n\n    style PP fill:#ff6b6b\n    style EOP fill:#4ecdc4\n    style EXO fill:#45b7d1\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#prerequisites","title":"\ud83d\udd11 Prerequisites","text":"<p>Before starting, ensure you have: - [ ] Global Administrator access to Office 365 - [ ] Exchange Online Administrator permissions - [ ] DNS records from Part 2 implemented and propagated - [ ] Proofpoint IP address ranges (from Proofpoint documentation)</p>"},{"location":"email-o365-proofpoint-part3/#connector-1-proofpoint-inbound","title":"\ud83d\udce5 Connector 1: Proofpoint Inbound","text":"<p>This connector accepts filtered email from Proofpoint and is the most security-critical configuration.</p>"},{"location":"email-o365-proofpoint-part3/#step-1-create-the-inbound-connector","title":"Step 1: Create the Inbound Connector","text":"<ol> <li>Navigate to Exchange Admin Center</li> <li>Go to admin.exchange.microsoft.com</li> <li> <p>Sign in with Global Admin credentials</p> </li> <li> <p>Access Mail Flow Settings</p> </li> <li>Click Mail flow in the left navigation</li> <li>Select Connectors</li> <li> <p>Click + Add a connector</p> </li> <li> <p>Configure Basic Settings <pre><code>Connection from: Partner organization\nConnection to: Office 365\nName: Proofpoint Inbound connector\nDescription: Receives filtered email from Proofpoint Essentials\n</code></pre></p> </li> </ol>"},{"location":"email-o365-proofpoint-part3/#step-2-configure-partner-organization-identity","title":"Step 2: Configure Partner Organization Identity","text":"<pre><code>How to identify your partner organization:\n\u2611\ufe0f By verifying that messages are coming from these domains: *\n</code></pre> <p>Why use asterisk (*): We want to accept email for ALL domains that might be processed by Proofpoint, not just our primary domain.</p>"},{"location":"email-o365-proofpoint-part3/#step-3-configure-security-restrictions","title":"Step 3: Configure Security Restrictions","text":""},{"location":"email-o365-proofpoint-part3/#tls-requirements","title":"TLS Requirements:","text":"<pre><code>\u2611\ufe0f Reject messages if they aren't encrypted using Transport Layer Security (TLS)\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#ip-address-restrictions","title":"IP Address Restrictions:","text":"<pre><code>\u2611\ufe0f Reject messages if they don't come from within these IP address ranges:\n67.231.149.0/24\n67.231.148.0/24  \n67.231.147.0/24\n67.231.146.0/24\n67.231.145.0/24\n67.231.144.0/24\n67.231.156.0/24\n</code></pre> <p>\u26a0\ufe0f Critical Security Note: Include ALL Proofpoint IP ranges from their documentation. The security of your email depends on this being complete and accurate.</p>"},{"location":"email-o365-proofpoint-part3/#step-4-apply-advanced-security-setting","title":"Step 4: Apply Advanced Security Setting","text":"<p>After creating the connector, run this PowerShell command to prevent bypass attacks:</p> <pre><code># Connect to Exchange Online PowerShell\nConnect-ExchangeOnline\n\n# Enable the critical security setting\nSet-InboundConnector -Identity \"Proofpoint Inbound connector\" -RestrictDomainsToIPAddresses $True\n\n# Verify the change was applied successfully\nGet-InboundConnector -Identity \"Proofpoint Inbound connector\" | Select-Object Name, RestrictDomainsToIPAddresses\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#why-this-setting-matters","title":"Why This Setting Matters:","text":"<ul> <li>Without it: Bad actors can send emails directly to your O365 endpoints, bypassing Proofpoint</li> <li>With it: Only emails from Proofpoint IP ranges are accepted, forcing all email through your security stack</li> <li>The Risk: It's easy for attackers to discover if an organization uses O365 and target the endpoints directly</li> </ul>"},{"location":"email-o365-proofpoint-part3/#connector-2-proofpoint-outbound","title":"\ud83d\udce4 Connector 2: ProofPoint Outbound","text":"<p>This connector routes all outbound email through Proofpoint for scanning and protection.</p>"},{"location":"email-o365-proofpoint-part3/#step-1-create-the-outbound-connector","title":"Step 1: Create the Outbound Connector","text":"<ol> <li>In Exchange Admin Center</li> <li>Mail flow &gt; Connectors</li> <li> <p>Click + Add a connector</p> </li> <li> <p>Configure Basic Settings <pre><code>Connection from: Office 365\nConnection to: Partner organization\nName: ProofPoint Outbound connector\nDescription: Routes outbound email through Proofpoint for filtering\n</code></pre></p> </li> </ol>"},{"location":"email-o365-proofpoint-part3/#step-2-configure-domain-scope","title":"Step 2: Configure Domain Scope","text":"<pre><code>Use of connector: Use only for email sent to these domains: *\n</code></pre> <p>Why asterisk (*): We want ALL outbound email to go through Proofpoint, regardless of destination domain.</p>"},{"location":"email-o365-proofpoint-part3/#step-3-configure-routing","title":"Step 3: Configure Routing","text":"<pre><code>Route email messages through these smart hosts:\noutbound-us1.ppe-hosted.com\n</code></pre> <p>Note: Your Proofpoint smart host may be different. Check your Proofpoint configuration for the correct outbound smart host address.</p>"},{"location":"email-o365-proofpoint-part3/#step-4-configure-security-settings","title":"Step 4: Configure Security Settings","text":"<pre><code>\u2611\ufe0f Always use Transport Layer Security (TLS)\n\u2611\ufe0f Connect only if the recipient's email server certificate is issued by a trusted certificate authority (CA)\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#connector-3-smtp-relay","title":"\ud83d\udda5\ufe0f Connector 3: SMTP Relay","text":"<p>This connector allows internal systems (ERP, printers, scanners) to send email directly to Office 365.</p>"},{"location":"email-o365-proofpoint-part3/#step-1-create-the-smtp-relay-connector","title":"Step 1: Create the SMTP Relay Connector","text":"<ol> <li>In Exchange Admin Center</li> <li>Mail flow &gt; Connectors</li> <li> <p>Click + Add a connector</p> </li> <li> <p>Configure Basic Settings <pre><code>Connection from: Your organization's email server\nConnection to: Office 365\nName: SMTP Relay\nDescription: Allows internal systems to send email directly to O365\n</code></pre></p> </li> </ol>"},{"location":"email-o365-proofpoint-part3/#step-2-configure-ip-address-authentication","title":"Step 2: Configure IP Address Authentication","text":"<pre><code>How to identify email sent from your email server:\n\u2611\ufe0f By verifying that the sending server's IP address is within these IP address ranges:\n\nXX.XX.X.XX\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#step-3-additional-requirements","title":"Step 3: Additional Requirements","text":"<pre><code>\u2611\ufe0f The sender's or recipient's email address is an accepted domain for your organization\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#critical-connection","title":"\ud83d\udd17 Critical Connection:","text":"<p>Notice these IP addresses exactly match the ones in your DNS SPF records (_spf1 and _spf2 from Part 2). This alignment is essential because:</p> <ul> <li>SPF records tell the world these IPs can send email for your domain</li> <li>SMTP Relay connector tells Office 365 to accept email from these same IPs</li> <li>Misalignment between these two configurations will cause email delivery failures</li> </ul>"},{"location":"email-o365-proofpoint-part3/#transport-rule-proofpoint-spam-bypass","title":"\ud83d\udee1\ufe0f Transport Rule: ProofPoint Spam Bypass","text":"<p>This rule prevents \"double dipping\" - where both Proofpoint and Exchange Online Protection scan the same email.</p>"},{"location":"email-o365-proofpoint-part3/#step-1-create-the-transport-rule","title":"Step 1: Create the Transport Rule","text":"<ol> <li>Navigate to Mail Flow Rules</li> <li>Mail flow &gt; Rules</li> <li> <p>Click + Add a rule</p> </li> <li> <p>Configure Basic Settings <pre><code>Name: ProofPoint Spam bypass\nDescription: Prevents double-scanning by setting SCL to -1 for Proofpoint-filtered emails\n</code></pre></p> </li> </ol>"},{"location":"email-o365-proofpoint-part3/#step-2-configure-conditions","title":"Step 2: Configure Conditions","text":"<pre><code>Apply this rule if:\n\u2611\ufe0f The sender IP address is in any of these ranges or exactly matches:\n\n148.163.128.0/19\n67.231.149.0/24\n67.231.148.0/24\n67.231.147.0/24\n67.231.146.0/24\n67.231.145.0/24\n67.231.144.0/24\n67.231.156.0/24\n67.231.155.0/24\n67.231.154.0/24\n[... include all Proofpoint IP ranges]\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#step-3-configure-actions","title":"Step 3: Configure Actions","text":"<pre><code>Do the following:\n\u2611\ufe0f Set the spam confidence level (SCL) to: -1\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#step-4-configure-rule-settings","title":"Step 4: Configure Rule Settings","text":"<pre><code>Rule mode: Enforce\nPriority: 6\nSeverity: Not specified\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#why-this-rule-is-essential","title":"Why This Rule is Essential:","text":"<ul> <li>Without it: Emails are scanned by both Proofpoint AND Exchange Online Protection</li> <li>Problem: This can cause legitimate emails (already approved by Proofpoint) to be blocked by EOP</li> <li>Solution: SCL -1 tells EOP \"this email is already clean, don't scan it again\"</li> </ul>"},{"location":"email-o365-proofpoint-part3/#configuration-verification","title":"\u2699\ufe0f Configuration Verification","text":""},{"location":"email-o365-proofpoint-part3/#step-1-verify-connectors","title":"Step 1: Verify Connectors","text":"<pre><code># Connect to Exchange Online PowerShell\nConnect-ExchangeOnline\n\n# List all connectors\nGet-InboundConnector | Select-Object Name, Enabled, ConnectorSource\nGet-OutboundConnector | Select-Object Name, Enabled, SmartHosts\n\n# Verify the critical security setting\nGet-InboundConnector -Identity \"Proofpoint Inbound connector\" | \n    Select-Object Name, RestrictDomainsToIPAddresses, TlsSettings\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#step-2-verify-transport-rule","title":"Step 2: Verify Transport Rule","text":"<pre><code># Check the transport rule\nGet-TransportRule -Identity \"ProofPoint Spam bypass\" | \n    Select-Object Name, State, Priority, SetSCL\n</code></pre>"},{"location":"email-o365-proofpoint-part3/#step-3-test-mail-flow","title":"Step 3: Test Mail Flow","text":"<p>Before proceeding, conduct these basic tests:</p> <ol> <li>Internal to Internal: Send email between internal users</li> <li>Outbound Test: Send email to external address</li> <li>Inbound Test: Have external party send email to you</li> <li>SMTP Relay Test: Send email from internal system (printer, etc.)</li> </ol>"},{"location":"email-o365-proofpoint-part3/#critical-security-considerations","title":"\ud83d\udea8 Critical Security Considerations","text":""},{"location":"email-o365-proofpoint-part3/#ip-address-management","title":"IP Address Management","text":"<ul> <li>Keep IP lists synchronized between DNS SPF records and O365 connectors</li> <li>Regular auditing - Remove IP addresses that are no longer in use</li> <li>Documentation - Maintain a record of what each IP address represents</li> </ul>"},{"location":"email-o365-proofpoint-part3/#connector-security","title":"Connector Security","text":"<ul> <li>Never disable IP restrictions on the Proofpoint Inbound connector</li> <li>Monitor failed connections - Unexpected sources may indicate attacks</li> <li>Regular review of Proofpoint IP ranges for updates</li> </ul>"},{"location":"email-o365-proofpoint-part3/#transport-rule-priority","title":"Transport Rule Priority","text":"<ul> <li>Keep spam bypass rule high priority (low number) to ensure it runs first</li> <li>Test rule effectiveness by checking message headers</li> </ul>"},{"location":"email-o365-proofpoint-part3/#monitoring-and-maintenance","title":"\ud83d\udcca Monitoring and Maintenance","text":""},{"location":"email-o365-proofpoint-part3/#weekly-checks","title":"Weekly Checks","text":"<ul> <li>[ ] Review connector status in Exchange Admin Center</li> <li>[ ] Check for any failed message delivery reports</li> <li>[ ] Monitor transport rule hit counts</li> </ul>"},{"location":"email-o365-proofpoint-part3/#monthly-checks","title":"Monthly Checks","text":"<ul> <li>[ ] Verify Proofpoint IP ranges haven't changed</li> <li>[ ] Review SMTP relay usage patterns</li> <li>[ ] Update IP address lists if infrastructure changes</li> </ul>"},{"location":"email-o365-proofpoint-part3/#quarterly-checks","title":"Quarterly Checks","text":"<ul> <li>[ ] Full end-to-end mail flow testing</li> <li>[ ] Review and update documentation</li> <li>[ ] Security audit of connector configurations</li> </ul>"},{"location":"email-o365-proofpoint-part3/#troubleshooting-common-issues","title":"\ud83d\udd27 Troubleshooting Common Issues","text":""},{"location":"email-o365-proofpoint-part3/#inbound-mail-issues","title":"Inbound Mail Issues","text":"<p>Problem: Legitimate emails being rejected <pre><code>Error: \"550 5.7.64 TenantAttribution; Relay Access Denied\"\n</code></pre> Solution:  1. Check if sender IP is in Proofpoint ranges 2. Verify <code>RestrictDomainsToIPAddresses</code> is set correctly 3. Check TLS configuration</p> <p>Problem: Emails being double-scanned <pre><code>Symptom: Clean emails marked as spam\n</code></pre> Solution:  1. Verify spam bypass rule is active and has correct IP ranges 2. Check rule priority (should be high priority/low number) 3. Review message headers for SCL values</p>"},{"location":"email-o365-proofpoint-part3/#outbound-mail-issues","title":"Outbound Mail Issues","text":"<p>Problem: Outbound emails not being delivered <pre><code>Error: \"451 4.4.0 DNS query failed\"\n</code></pre> Solution:  1. Verify outbound smart host address 2. Check DNS resolution of smart host 3. Confirm Proofpoint outbound service is active</p>"},{"location":"email-o365-proofpoint-part3/#smtp-relay-issues","title":"SMTP Relay Issues","text":"<p>Problem: Internal systems can't send email <pre><code>Error: \"550 5.7.60 SMTP; Client does not have permissions to send as this sender\"\n</code></pre> Solution:  1. Verify sender IP is in SMTP relay connector 2. Check if sender domain is accepted domain 3. Confirm connector is enabled</p>"},{"location":"email-o365-proofpoint-part3/#configuration-checklist","title":"\ud83d\udccb Configuration Checklist","text":"<p>Before moving to Part 4, ensure:</p>"},{"location":"email-o365-proofpoint-part3/#connectors-created","title":"Connectors Created:","text":"<ul> <li>[ ] Proofpoint Inbound connector configured with IP restrictions</li> <li>[ ] Proofpoint Outbound connector routing to correct smart host</li> <li>[ ] SMTP Relay connector with all internal IP addresses</li> <li>[ ] All connectors are enabled and operational</li> </ul>"},{"location":"email-o365-proofpoint-part3/#security-settings-applied","title":"Security Settings Applied:","text":"<ul> <li>[ ] <code>RestrictDomainsToIPAddresses</code> set to <code>$True</code> on inbound connector</li> <li>[ ] TLS required on all appropriate connectors</li> <li>[ ] IP address ranges complete and accurate</li> </ul>"},{"location":"email-o365-proofpoint-part3/#transport-rules","title":"Transport Rules:","text":"<ul> <li>[ ] ProofPoint Spam bypass rule created and enabled</li> <li>[ ] Rule includes all Proofpoint IP ranges</li> <li>[ ] Rule priority set appropriately (recommend 1-10)</li> <li>[ ] SCL set to -1 for Proofpoint sources</li> </ul>"},{"location":"email-o365-proofpoint-part3/#testing-completed","title":"Testing Completed:","text":"<ul> <li>[ ] Internal-to-internal email flow tested</li> <li>[ ] Inbound email from external sources tested</li> <li>[ ] Outbound email to external destinations tested</li> <li>[ ] SMTP relay from internal systems tested</li> </ul>"},{"location":"email-o365-proofpoint-part3/#common-mistakes-to-avoid","title":"\u26a0\ufe0f Common Mistakes to Avoid","text":"<ol> <li>Incomplete IP Ranges: Not including all Proofpoint IP addresses</li> <li>Missing Security Setting: Forgetting to set <code>RestrictDomainsToIPAddresses</code></li> <li>Wrong Smart Host: Using incorrect outbound smart host address</li> <li>IP Address Mismatch: Having different IPs in SPF records vs. connectors</li> <li>Rule Priority Issues: Setting spam bypass rule too low priority</li> <li>Testing Shortcuts: Not testing all mail flow scenarios</li> </ol>"},{"location":"email-o365-proofpoint-part3/#whats-next","title":"\ud83c\udfaf What's Next","text":"<p>With Office 365 connectors configured, we'll move to Part 4 where we'll set up the Proofpoint side of the integration, including: - Domain relay configuration - DKIM key generation and management - Security feature enablement - Email warning tag configuration</p> <p>The connectors we've just configured will work hand-in-hand with the Proofpoint settings to create a seamless, secure email flow.</p>"},{"location":"email-o365-proofpoint-part3/#series-navigation_1","title":"\ud83d\udcd6 Series Navigation","text":"<ul> <li>\u2190 Part 2: DNS Configuration and Setup</li> <li>Part 3: Office 365 Connector Configuration (Current)</li> <li>Part 4: Proofpoint Integration Setup \u2192</li> </ul> <p>Always test connector changes during maintenance windows and monitor mail flow carefully for the first 24-48 hours after implementation.</p>"},{"location":"email-o365-proofpoint-part4/","title":"\ud83d\udce7 Email Security Implementation (O365 and Proof Point Essentials) Series","text":""},{"location":"email-o365-proofpoint-part4/#part-4-proofpoint-integration-setup","title":"Part 4: Proofpoint Integration Setup","text":""},{"location":"email-o365-proofpoint-part4/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC</li> <li>Part 2: DNS Configuration and Setup</li> <li>Part 3: Office 365 Connector Configuration</li> <li>Part 4: Proofpoint Integration Setup (Current)</li> <li>Part 5: Testing and Troubleshooting</li> </ul>"},{"location":"email-o365-proofpoint-part4/#what-well-configure","title":"\ud83c\udfaf What We'll Configure","text":"<p>In this part, we'll configure the Proofpoint Essentials side of the integration: 1. Domain Relay Configuration - Set up mail routing to Office 365 2. Security Features - Enable comprehensive email protection 3. Email Warning Tags - Configure user notification system 4. DKIM Key Management - Generate and manage authentication keys 5. User Provisioning and SSO - Enable user access (overview)</p>"},{"location":"email-o365-proofpoint-part4/#prerequisites","title":"\ud83d\udd27 Prerequisites","text":"<p>Before starting, ensure you have: - [ ] Proofpoint Essentials administrator access - [ ] Office 365 connectors from Part 3 configured and tested - [ ] DNS records from Part 2 implemented - [ ] Your Office 365 tenant's mail protection endpoint (e.g., yourdomain-com.mail.protection.outlook.com)</p>"},{"location":"email-o365-proofpoint-part4/#domain-relay-configuration","title":"\ud83d\udce7 Domain Relay Configuration","text":"<p>This is the foundation that tells Proofpoint where to send clean emails after filtering.</p>"},{"location":"email-o365-proofpoint-part4/#step-1-access-domain-configuration","title":"Step 1: Access Domain Configuration","text":"<ol> <li>Log into Proofpoint Essentials</li> <li>Navigate to your Proofpoint admin portal</li> <li> <p>Sign in with administrator credentials</p> </li> <li> <p>Navigate to Domains</p> </li> <li>Click Email in the left navigation</li> <li>Select Domains</li> <li>Click Add Domain or select existing domain</li> </ol>"},{"location":"email-o365-proofpoint-part4/#step-2-configure-domain-settings","title":"Step 2: Configure Domain Settings","text":"<pre><code>Domain Information:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Domain Type: Relay                  \u2502\n\u2502 Domain Name: yourdomain.com         \u2502\n\u2502 Primary Delivery Destination:       \u2502\n\u2502 yourdomain-com.mail.protection.     \u2502\n\u2502 outlook.com                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#finding-your-o365-mail-protection-endpoint","title":"Finding Your O365 Mail Protection Endpoint:","text":"<p>Your endpoint follows this pattern: - Format: <code>[domain-with-dashes].mail.protection.outlook.com</code> - Example: For <code>yourdomain.com</code> \u2192 <code>yourdomain-com.mail.protection.outlook.com</code> - Verification: Check your MX record in Office 365 Admin Center</p>"},{"location":"email-o365-proofpoint-part4/#step-3-verify-domain-configuration","title":"Step 3: Verify Domain Configuration","text":"<p>After saving, ensure: - [ ] Domain status shows as Active - [ ] Primary delivery destination is correct - [ ] No error messages in domain configuration</p>"},{"location":"email-o365-proofpoint-part4/#security-features-configuration","title":"\ud83d\udee1\ufe0f Security Features Configuration","text":"<p>Proofpoint Essentials offers comprehensive protection. Here's what to enable and why:</p>"},{"location":"email-o365-proofpoint-part4/#step-1-core-protection-features","title":"Step 1: Core Protection Features","text":"<p>Navigate to Email &gt; Settings &gt; Features and enable:</p>"},{"location":"email-o365-proofpoint-part4/#enable-outbound-relaying","title":"\u2705 Enable Outbound relaying","text":"<pre><code>Purpose: Ensures outbound mail is protected with outbound scanning\nBenefit: Prevents your domain from sending malicious content\nRecommendation: Always enable\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-disclaimers","title":"\u2705 Enable Disclaimers","text":"<pre><code>Purpose: Adds disclaimers to emails\nBenefit: Legal protection and professional branding\nRecommendation: Enable if required by legal/compliance\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-smtp-discovery","title":"\u2705 Enable SMTP Discovery","text":"<pre><code>Purpose: Another way to provision users to the service\nBenefit: Automatic user discovery and provisioning\nRecommendation: Enable for easier user management\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-2-data-loss-prevention","title":"Step 2: Data Loss Prevention","text":""},{"location":"email-o365-proofpoint-part4/#enable-data-loss-prevention-dlp","title":"\u2705 Enable Data Loss Prevention (DLP)","text":"<pre><code>Purpose: Adds various data loss prevention options\nFeatures: \n- Dictionaries for sensitive content detection\n- Smart identifiers (SSN, Credit Cards, etc.)\n- Custom content policies\nRecommendation: Essential for compliance\n</code></pre> <p>DLP Configuration Steps: 1. Navigate to Email &gt; Filters &gt; Data Loss Prevention 2. Configure dictionaries for your industry 3. Set up smart identifiers for relevant data types 4. Create policies for different user groups</p>"},{"location":"email-o365-proofpoint-part4/#step-3-advanced-threat-protection","title":"Step 3: Advanced Threat Protection","text":""},{"location":"email-o365-proofpoint-part4/#enable-url-defense","title":"\u2705 Enable URL Defense","text":"<pre><code>Purpose: Scans inbound emails for malicious links\nHow it works:\n1. Rewrites URLs in emails\n2. Performs click-time analysis\n3. Blocks access to malicious sites\nRecommendation: Critical security feature\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-attachment-defense","title":"\u2705 Enable Attachment Defense","text":"<pre><code>Purpose: Scans emails for known malicious attachments\nProtection: Against attachment-based threats\nRecommendation: Always enable\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-attachment-defense-sandboxing","title":"\u2705 Enable Attachment Defense Sandboxing","text":"<pre><code>Purpose: Scans unknown attachments in isolated environment\nProcess:\n1. Unknown attachments are held temporarily\n2. Analyzed in secure sandbox\n3. Released or quarantined based on analysis\nRecommendation: Enable for maximum protection\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-4-additional-protection-features","title":"Step 4: Additional Protection Features","text":""},{"location":"email-o365-proofpoint-part4/#enable-social-media-account-protection","title":"\u2705 Enable Social Media Account Protection","text":"<pre><code>Purpose: Protects against social media-based threats\nBenefit: Extends protection beyond traditional email threats\nRecommendation: Enable for comprehensive protection\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-email-encryption","title":"\u2705 Enable Email Encryption","text":"<pre><code>Purpose: Provides email encryption capabilities\nUse cases: \n- Sensitive data transmission\n- Compliance requirements\n- Secure communication\nRecommendation: Enable if encryption is required\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-anti-spoofing-policies","title":"\u2705 Enable Anti-Spoofing Policies","text":"<pre><code>Purpose: Provides additional DMARC policy controls\nBenefit: Enhanced protection against domain spoofing\nRecommendation: Always enable\nNote: This unlocks Email Warning Tags (next section)\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-5-administrative-features","title":"Step 5: Administrative Features","text":""},{"location":"email-o365-proofpoint-part4/#enable-one-click-removal","title":"\u2705 Enable One Click Removal","text":"<pre><code>Purpose: Allow admins to remove mail from user mailboxes\nRequirement: Properly configured Microsoft environment\nBenefit: Quick response to identified threats\nRecommendation: Enable for incident response\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#enable-automatic-remediation","title":"\u2705 Enable Automatic Remediation","text":"<pre><code>Purpose: Removes malicious email discovered after delivery\nProcess:\n1. Threat identified post-delivery\n2. Automatically removed from user mailboxes\n3. Users notified of removal\nRequirement: Correctly configured Microsoft environment\nRecommendation: Essential for advanced threat response\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#email-warning-tags-configuration","title":"\ud83c\udff7\ufe0f Email Warning Tags Configuration","text":"<p>Email Warning Tags provide visual cues to users about potentially dangerous emails.</p>"},{"location":"email-o365-proofpoint-part4/#prerequisites_1","title":"Prerequisites","text":"<p>Anti-Spoofing Policies must be enabled first (see previous section).</p>"},{"location":"email-o365-proofpoint-part4/#step-1-access-email-warning-tags","title":"Step 1: Access Email Warning Tags","text":"<ol> <li>Navigate to Email &gt; Email Tagging</li> <li>Confirm Email Warning Tags is enabled</li> <li>Access Tag Types configuration</li> </ol>"},{"location":"email-o365-proofpoint-part4/#step-2-configure-informational-tags","title":"Step 2: Configure Informational Tags","text":""},{"location":"email-o365-proofpoint-part4/#external-sender-tag","title":"\u2705 External Sender Tag","text":"<pre><code>Purpose: Informs users when email comes from outside the organization\nDisplay: Banner at top of email\nUser Impact: Promotes security awareness\nConfiguration: Enable with custom messaging\n</code></pre> <p>Recommended Message: <pre><code>\u26a0\ufe0f EXTERNAL EMAIL: This email originated from outside your organization. \nExercise caution with links and attachments.\n</code></pre></p>"},{"location":"email-o365-proofpoint-part4/#step-3-configure-warning-tags","title":"Step 3: Configure Warning Tags","text":""},{"location":"email-o365-proofpoint-part4/#dmarc-failure-tag","title":"\u2705 DMARC Failure Tag","text":"<pre><code>Purpose: Informs users when email fails DMARC authentication\nDisplay: Warning banner\nSignificance: High security risk indicator\nAction: Usually blocks email, but provides user notification\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#newly-registered-domain-tag","title":"\u2705 Newly Registered Domain Tag","text":"<pre><code>Purpose: Warns about emails from recently registered domains\nRisk: Newly registered domains often used in phishing\nDisplay: Warning banner with age information\nRecommendation: Enable with 30-day threshold\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#high-risk-geo-ip-tag","title":"\u2705 High Risk GEO IP Tag","text":"<pre><code>Purpose: Warns about emails from high-risk geographical locations\nRisk Assessment: Based on threat intelligence\nDisplay: Country/region information\nCustomization: Configure risk levels per region\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-4-configure-tag-display-options","title":"Step 4: Configure Tag Display Options","text":""},{"location":"email-o365-proofpoint-part4/#display-a-link-in-the-warning-tag-to-learn-more","title":"\u2705 Display a link in the warning tag to learn more","text":"<pre><code>Purpose: Provides users with additional security education\nImplementation: Links to your security training materials\nContent: Explain why the warning appeared\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#allow-users-to-perform-actions-on-learn-more","title":"\u2705 Allow users to perform actions on learn more","text":"<pre><code>Purpose: Enable user reporting and feedback\nActions: \n- Report as phishing\n- Report as safe\n- Request review\nBenefit: Improves threat intelligence\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#include-additional-text-below-the-warning-tag","title":"\u2705 Include additional text below the warning tag","text":"<pre><code>Purpose: Provide specific guidance to users\nContent Examples:\n- \"Contact IT if you believe this is legitimate\"\n- \"Do not click links or download attachments\"\n- \"Forward suspicious emails to security@yourdomain.com\"\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-5-tag-customization","title":"Step 5: Tag Customization","text":"<p>Create organization-specific messaging:</p> <p>External Sender Template: <pre><code>\ud83c\udf10 EXTERNAL EMAIL \nThis message originated from outside yourdomain.com. \nVerify sender identity before clicking links or downloading attachments.\nQuestions? Contact IT at extension 1234.\n</code></pre></p> <p>High Risk Warning Template: <pre><code>\u26a0\ufe0f HIGH RISK EMAIL DETECTED\nThis email has characteristics associated with phishing or malware.\n\u2022 Do not click any links\n\u2022 Do not download attachments  \n\u2022 Forward to security@yourdomain.com for analysis\n</code></pre></p>"},{"location":"email-o365-proofpoint-part4/#dkim-key-management","title":"\ud83d\udd11 DKIM Key Management","text":""},{"location":"email-o365-proofpoint-part4/#step-1-generate-dkim-keys","title":"Step 1: Generate DKIM Keys","text":"<ol> <li>Navigate to DKIM Configuration</li> <li>Email &gt; Authentication &gt; DKIM</li> <li> <p>Select your domain</p> </li> <li> <p>Generate Key Pair</p> </li> <li>Click Generate New Key</li> <li>Select key size (2048-bit recommended)</li> <li> <p>Choose selector name (or use auto-generated)</p> </li> <li> <p>Obtain Public Key</p> </li> <li>Copy the public key provided</li> <li>Note the selector name (e.g., <code>selector-1678913997</code>)</li> </ol>"},{"location":"email-o365-proofpoint-part4/#step-2-dns-publication","title":"Step 2: DNS Publication","text":"<p>The public key from Step 1 should already be in your DNS from Part 2:</p> <pre><code>Type: TXT\nName: selector-1678913997._domainkey.yourdomain.com  \nValue: \"v=DKIM1; k=rsa; t=s; n=core; p=[LONG_PUBLIC_KEY_STRING]\"\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-3-key-verification","title":"Step 3: Key Verification","text":"<ol> <li>In Proofpoint: Click Verify DNS Record</li> <li>External Verification: Use online DKIM checkers</li> <li>Test Email: Send test email and check headers</li> </ol>"},{"location":"email-o365-proofpoint-part4/#step-4-key-rotation-quarterly","title":"Step 4: Key Rotation (Quarterly)","text":"<ol> <li>Generate new key pair in Proofpoint</li> <li>Publish new public key to DNS</li> <li>Wait for DNS propagation (24-48 hours)</li> <li>Activate new key in Proofpoint</li> <li>Remove old key from DNS after 1 week</li> </ol>"},{"location":"email-o365-proofpoint-part4/#user-provisioning-and-sso-setup","title":"\ud83d\udc65 User Provisioning and SSO Setup","text":""},{"location":"email-o365-proofpoint-part4/#user-provisioning-options","title":"User Provisioning Options","text":""},{"location":"email-o365-proofpoint-part4/#option-1-smtp-discovery-recommended","title":"Option 1: SMTP Discovery (Recommended)","text":"<ul> <li>Enabled in Features (see previous section)</li> <li>Process: Users automatically discovered when they send/receive email</li> <li>Benefit: No manual user management required</li> </ul>"},{"location":"email-o365-proofpoint-part4/#option-2-manual-provisioning","title":"Option 2: Manual Provisioning","text":"<ul> <li>Navigate: Users &gt; Add Users</li> <li>Process: Manually add individual users</li> <li>Use case: Small organizations or specific user groups</li> </ul>"},{"location":"email-o365-proofpoint-part4/#option-3-bulk-import","title":"Option 3: Bulk Import","text":"<ul> <li>Navigate: Users &gt; Import Users</li> <li>Process: Upload CSV file with user information</li> <li>Use case: Large organizations or initial setup</li> </ul>"},{"location":"email-o365-proofpoint-part4/#single-sign-on-sso-configuration","title":"Single Sign-On (SSO) Configuration","text":"<p>While specific SSO configuration varies by identity provider, here are the general steps:</p>"},{"location":"email-o365-proofpoint-part4/#step-1-identity-provider-setup","title":"Step 1: Identity Provider Setup","text":"<pre><code>Common Providers:\n- Azure Active Directory (most common with O365)\n- ADFS\n- Okta\n- Ping Identity\n</code></pre>"},{"location":"email-o365-proofpoint-part4/#step-2-proofpoint-sso-configuration","title":"Step 2: Proofpoint SSO Configuration","text":"<ol> <li>Navigate: Settings &gt; Authentication &gt; Single Sign-On</li> <li>Configure: </li> <li>Identity Provider metadata</li> <li>Attribute mappings</li> <li>Group assignments</li> <li>Test: Verify SSO login functionality</li> </ol>"},{"location":"email-o365-proofpoint-part4/#step-3-user-communication","title":"Step 3: User Communication","text":"<p>Inform users about: - New login process - Portal access URL - Quarantine management capabilities</p> <p>\ud83d\udca1 Tip: For detailed SSO configuration, consult Proofpoint documentation specific to your identity provider.</p>"},{"location":"email-o365-proofpoint-part4/#configuration-verification-checklist","title":"\ud83d\udccb Configuration Verification Checklist","text":""},{"location":"email-o365-proofpoint-part4/#domain-configuration","title":"Domain Configuration:","text":"<ul> <li>[ ] Domain type set to \"Relay\"</li> <li>[ ] Correct O365 mail protection endpoint configured</li> <li>[ ] Domain status shows as \"Active\"</li> </ul>"},{"location":"email-o365-proofpoint-part4/#security-features","title":"Security Features:","text":"<ul> <li>[ ] All critical features enabled (URL Defense, Attachment Defense, etc.)</li> <li>[ ] DLP policies configured for your organization</li> <li>[ ] Anti-spoofing policies enabled</li> </ul>"},{"location":"email-o365-proofpoint-part4/#email-warning-tags","title":"Email Warning Tags:","text":"<ul> <li>[ ] External sender tags configured</li> <li>[ ] Warning tags for high-risk scenarios enabled</li> <li>[ ] Custom messaging appropriate for your organization</li> <li>[ ] \"Learn more\" links configured</li> </ul>"},{"location":"email-o365-proofpoint-part4/#dkim-configuration","title":"DKIM Configuration:","text":"<ul> <li>[ ] DKIM keys generated in Proofpoint</li> <li>[ ] Public keys published in DNS</li> <li>[ ] DKIM verification successful</li> <li>[ ] Key rotation schedule established</li> </ul>"},{"location":"email-o365-proofpoint-part4/#user-access","title":"User Access:","text":"<ul> <li>[ ] User provisioning method selected and configured</li> <li>[ ] SSO setup completed (if required)</li> <li>[ ] User communication plan executed</li> </ul>"},{"location":"email-o365-proofpoint-part4/#security-best-practices","title":"\ud83d\udea8 Security Best Practices","text":""},{"location":"email-o365-proofpoint-part4/#feature-management","title":"Feature Management","text":"<ul> <li>Enable gradually: Don't enable all features at once</li> <li>Monitor impact: Watch for false positives and user complaints</li> <li>Regular review: Quarterly assessment of feature effectiveness</li> </ul>"},{"location":"email-o365-proofpoint-part4/#tag-configuration","title":"Tag Configuration","text":"<ul> <li>Clear messaging: Use simple, actionable language</li> <li>Consistent branding: Match your organization's communication style</li> <li>Regular updates: Refresh messaging based on threat landscape</li> </ul>"},{"location":"email-o365-proofpoint-part4/#dkim-management","title":"DKIM Management","text":"<ul> <li>Secure key storage: Protect private keys appropriately</li> <li>Regular rotation: Change keys quarterly</li> <li>Multiple selectors: Consider using multiple DKIM keys for redundancy</li> </ul>"},{"location":"email-o365-proofpoint-part4/#whats-next","title":"\ud83c\udfaf What's Next","text":"<p>With Proofpoint fully configured, we'll move to Part 5 where we'll cover: - Comprehensive testing methodology - Office 365 message tracing and troubleshooting - Mail flow verification procedures - Common issues and their solutions - Monitoring and maintenance procedures</p> <p>The integration is nearly complete - Part 5 will ensure everything works correctly and provide you with the tools to maintain and troubleshoot the system.</p>"},{"location":"email-o365-proofpoint-part4/#series-navigation_1","title":"\ud83d\udcd6 Series Navigation","text":"<ul> <li>\u2190 Part 3: Office 365 Connector Configuration</li> <li>Part 4: Proofpoint Integration Setup (Current)</li> <li>Part 5: Testing and Troubleshooting \u2192</li> </ul> <p>Remember to enable features gradually and monitor their impact on mail flow and user experience.</p>"},{"location":"email-o365-proofpoint-part5/","title":"\ud83d\udce7 Email Security Implementation (O365 and Proof Point Essentials) Series","text":""},{"location":"email-o365-proofpoint-part5/#part-5-testing-and-troubleshooting","title":"Part 5: Testing and Troubleshooting","text":""},{"location":"email-o365-proofpoint-part5/#series-navigation","title":"\ud83d\udcda Series Navigation","text":"<ul> <li>Part 1: Understanding SPF, DKIM, and DMARC</li> <li>Part 2: DNS Configuration and Setup</li> <li>Part 3: Office 365 Connector Configuration</li> <li>Part 4: Proofpoint Integration Setup</li> <li>Part 5: Testing and Troubleshooting (Current)</li> </ul>"},{"location":"email-o365-proofpoint-part5/#what-well-cover","title":"\ud83c\udfaf What We'll Cover","text":"<p>In this final part, we'll ensure your email security implementation works correctly: 1. Comprehensive Testing Methodology - Systematic testing approach 2. Office 365 Message Tracing - Track and troubleshoot mail flow 3. Mail Flow Verification - Confirm proper routing and security 4. Common Issues and Solutions - Troubleshoot typical problems 5. Monitoring and Maintenance - Keep your system healthy</p>"},{"location":"email-o365-proofpoint-part5/#comprehensive-testing-methodology","title":"\ud83e\uddea Comprehensive Testing Methodology","text":""},{"location":"email-o365-proofpoint-part5/#phase-1-dns-verification","title":"Phase 1: DNS Verification","text":"<p>Before testing mail flow, verify all DNS records are properly configured:</p>"},{"location":"email-o365-proofpoint-part5/#spf-record-testing","title":"SPF Record Testing","text":"<pre><code># Test main SPF record\ndig TXT yourdomain.com | grep \"v=spf1\"\n\n# Verify SPF includes are resolving\ndig TXT _spf1.yourdomain.com\ndig TXT _spf2.yourdomain.com\n\n# Use online SPF checker tools\n# Recommended: mxtoolbox.com/spf.aspx\n</code></pre> <p>Expected Results: - \u2705 Main SPF record includes all necessary sources - \u2705 _spf1 and _spf2 records resolve correctly - \u2705 Total DNS lookups under 10 (SPF limit)</p>"},{"location":"email-o365-proofpoint-part5/#dkim-record-testing","title":"DKIM Record Testing","text":"<pre><code># Test Office 365 DKIM keys\ndig CNAME hs1-19543953._domainkey.yourdomain.com\ndig CNAME hs2-19543953._domainkey.yourdomain.com\n\n# Test Proofpoint DKIM key\ndig TXT selector-1678913997._domainkey.yourdomain.com\n</code></pre> <p>Expected Results: - \u2705 CNAME records point to Microsoft infrastructure - \u2705 Proofpoint DKIM TXT record contains valid public key - \u2705 No DNS resolution errors</p>"},{"location":"email-o365-proofpoint-part5/#dmarc-record-testing","title":"DMARC Record Testing","text":"<pre><code># Test DMARC record\ndig TXT _dmarc.yourdomain.com\n\n# Verify DMARC syntax online\n# Recommended: dmarcian.com/dmarc-inspector/\n</code></pre> <p>Expected Results: - \u2705 DMARC record has valid syntax - \u2705 Reporting addresses are accessible - \u2705 Policy is set appropriately (start with p=none)</p>"},{"location":"email-o365-proofpoint-part5/#phase-2-mail-flow-testing","title":"Phase 2: Mail Flow Testing","text":""},{"location":"email-o365-proofpoint-part5/#test-scenario-1-internal-to-internal","title":"Test Scenario 1: Internal to Internal","text":"<pre><code>From: user1@yourdomain.com\nTo: user2@yourdomain.com\nPurpose: Verify basic Exchange Online functionality\nExpected Path: Sender \u2192 Exchange Online \u2192 Recipient\n</code></pre> <p>Test Steps: 1. Send email between internal users 2. Verify delivery within 1-2 minutes 3. Check email headers for routing information</p> <p>Success Criteria: - \u2705 Email delivered successfully - \u2705 No unusual delays - \u2705 Headers show internal routing only</p>"},{"location":"email-o365-proofpoint-part5/#test-scenario-2-outbound-email","title":"Test Scenario 2: Outbound Email","text":"<pre><code>From: user@yourdomain.com\nTo: external-user@gmail.com (or other external provider)\nPurpose: Test outbound mail flow through Proofpoint\nExpected Path: Sender \u2192 Exchange Online \u2192 Proofpoint \u2192 Internet\n</code></pre> <p>Test Steps: 1. Send email to external address you control 2. Check delivery time and headers 3. Verify SPF, DKIM, DMARC authentication</p> <p>Success Criteria: - \u2705 Email delivered to external recipient - \u2705 SPF authentication passes - \u2705 DKIM signature present and valid - \u2705 DMARC alignment achieved</p>"},{"location":"email-o365-proofpoint-part5/#test-scenario-3-inbound-email","title":"Test Scenario 3: Inbound Email","text":"<pre><code>From: external-user@gmail.com\nTo: user@yourdomain.com\nPurpose: Test inbound mail flow through Proofpoint\nExpected Path: Internet \u2192 Proofpoint \u2192 Exchange Online \u2192 Recipient\n</code></pre> <p>Test Steps: 1. Send email from external address to your domain 2. Monitor delivery time and path 3. Check for proper security scanning</p> <p>Success Criteria: - \u2705 Email delivered to internal recipient - \u2705 Email shows signs of Proofpoint processing - \u2705 Security tags applied appropriately - \u2705 SCL set to -1 (spam bypass working)</p>"},{"location":"email-o365-proofpoint-part5/#test-scenario-4-smtp-relay","title":"Test Scenario 4: SMTP Relay","text":"<pre><code>From: Internal system (printer, ERP, etc.)\nTo: user@yourdomain.com or external address\nPurpose: Test direct SMTP relay functionality\nExpected Path: Internal System \u2192 Exchange Online \u2192 Recipient\n</code></pre> <p>Test Steps: 1. Configure internal system to send through O365 SMTP relay 2. Send test message 3. Verify delivery and authentication</p> <p>Success Criteria: - \u2705 Internal system can authenticate and send - \u2705 Messages delivered successfully - \u2705 SPF authentication passes for system's IP</p>"},{"location":"email-o365-proofpoint-part5/#office-365-message-tracing","title":"\ud83d\udd0d Office 365 Message Tracing","text":"<p>Message tracing is your primary tool for troubleshooting mail flow issues in Office 365.</p>"},{"location":"email-o365-proofpoint-part5/#accessing-message-trace","title":"Accessing Message Trace","text":"<ol> <li>Navigate to Exchange Admin Center</li> <li>Go to admin.exchange.microsoft.com</li> <li> <p>Sign in with administrator credentials</p> </li> <li> <p>Access Message Trace</p> </li> <li>Click Mail flow in left navigation</li> <li>Select Message trace</li> </ol>"},{"location":"email-o365-proofpoint-part5/#basic-message-trace","title":"Basic Message Trace","text":""},{"location":"email-o365-proofpoint-part5/#setting-up-a-trace","title":"Setting Up a Trace","text":"<pre><code>Time range: Last 24 hours (or specific timeframe)\nSender: specific-user@yourdomain.com (optional)\nRecipient: target@external-domain.com (optional)\nDelivery status: All (or specific status)\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#understanding-trace-results","title":"Understanding Trace Results","text":"<p>Status Indicators: - \u2705 Delivered: Message successfully delivered - \u23f3 Pending: Message still processing - \u274c Failed: Delivery failed - \ud83d\udeab Filtered: Blocked by spam/security filters - \u21a9\ufe0f Quarantined: Held in quarantine</p> <p>Key Information to Review: - Date/Time: When message was processed - Message Size: Helps identify large attachments - Connector: Which connector processed the message - Transport Rules: Which rules were applied</p>"},{"location":"email-o365-proofpoint-part5/#advanced-message-trace","title":"Advanced Message Trace","text":"<p>For detailed troubleshooting, use Extended Message Trace:</p>"},{"location":"email-o365-proofpoint-part5/#powershell-method","title":"PowerShell Method","text":"<pre><code># Connect to Exchange Online\nConnect-ExchangeOnline\n\n# Advanced trace with details\nGet-MessageTrace -StartDate (Get-Date).AddHours(-24) -EndDate (Get-Date) -SenderAddress \"user@yourdomain.com\" | Get-MessageTraceDetail\n\n# Trace specific message\nGet-MessageTrace -MessageId \"message-id-here\" | Get-MessageTraceDetail\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#interpreting-detailed-results","title":"Interpreting Detailed Results","text":"<p>Common Events to Look For: - RECEIVE: Message received by Office 365 - SEND: Message sent from Office 365 - DELIVER: Message delivered to recipient - REDIRECT: Message redirected (check connectors) - FAIL: Delivery failure (check error details)</p> <p>Transport Rule Actions: - SetScl: SCL (Spam Confidence Level) modified - Quarantine: Message quarantined - Redirect: Message redirected to different destination</p>"},{"location":"email-o365-proofpoint-part5/#mail-flow-verification-procedures","title":"\u2705 Mail Flow Verification Procedures","text":""},{"location":"email-o365-proofpoint-part5/#header-analysis","title":"Header Analysis","text":"<p>Email headers contain valuable information about the mail flow path and authentication results.</p>"},{"location":"email-o365-proofpoint-part5/#key-headers-to-examine","title":"Key Headers to Examine","text":"<p>Authentication Headers: <pre><code>Authentication-Results: Shows SPF, DKIM, DMARC results\nReceived-SPF: SPF authentication outcome\nDKIM-Signature: DKIM signature information\n</code></pre></p> <p>Routing Headers: <pre><code>Received: Shows each server that handled the message\nX-MS-Exchange-Organization-MessageDirectionality: Originating/Incoming\nX-MS-Exchange-Transport-Rules-Executed: Applied transport rules\n</code></pre></p> <p>Security Headers: <pre><code>X-MS-Exchange-Organization-SCL: Spam Confidence Level\nX-Proofpoint-*: Proofpoint processing information\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#sample-header-analysis","title":"Sample Header Analysis","text":"<p>Good Inbound Message Headers: <pre><code>Authentication-Results: yourdomain.com; spf=pass; dkim=pass; dmarc=pass\nX-MS-Exchange-Organization-SCL: -1\nX-MS-Exchange-Transport-Rules-Executed: ProofPoint Spam bypass\nReceived: from mx2-us1.ppe-hosted.com (67.231.149.xxx)\n</code></pre></p> <p>Analysis: - \u2705 SPF/DKIM/DMARC all pass - \u2705 SCL -1 (spam bypass rule working) - \u2705 Message came from Proofpoint IP - \u2705 Transport rule executed correctly</p>"},{"location":"email-o365-proofpoint-part5/#end-to-end-flow-verification","title":"End-to-End Flow Verification","text":""},{"location":"email-o365-proofpoint-part5/#verification-checklist","title":"Verification Checklist","text":"<p>Outbound Mail Path: 1. [ ] Message originates in Exchange Online 2. [ ] Routes through ProofPoint Outbound connector 3. [ ] Processed by Proofpoint filtering 4. [ ] Delivered to external recipient 5. [ ] SPF/DKIM/DMARC authentication successful</p> <p>Inbound Mail Path: 1. [ ] Message received by Proofpoint MX 2. [ ] Filtered and scanned by Proofpoint 3. [ ] Forwarded to Office 365 via connector 4. [ ] Spam bypass rule applied (SCL -1) 5. [ ] Delivered to user mailbox</p> <p>SMTP Relay Path: 1. [ ] Internal system connects to Office 365 2. [ ] Authentication succeeds via IP allowlist 3. [ ] Message processed normally 4. [ ] SPF authentication passes</p>"},{"location":"email-o365-proofpoint-part5/#common-issues-and-solutions","title":"\ud83d\udea8 Common Issues and Solutions","text":""},{"location":"email-o365-proofpoint-part5/#issue-1-inbound-mail-rejected","title":"Issue 1: Inbound Mail Rejected","text":"<p>Symptoms: - External senders report bounce messages - Message trace shows \"Access Denied\" errors</p> <p>Error Messages: <pre><code>550 5.7.64 TenantAttribution; Relay Access Denied\n</code></pre></p> <p>Root Causes: 1. Proofpoint IP not in connector allowlist 2. <code>RestrictDomainsToIPAddresses</code> not enabled 3. Incorrect Proofpoint IP ranges</p> <p>Solutions: <pre><code># Verify connector configuration\nGet-InboundConnector -Identity \"Proofpoint Inbound connector\" | \n    Select-Object Name, RestrictDomainsToIPAddresses, TlsSettings, Enabled\n\n# Check IP ranges in connector\nGet-InboundConnector -Identity \"Proofpoint Inbound connector\" | \n    Select-Object -ExpandProperty RestrictDomainsToCertificate\n\n# Correct the setting if needed\nSet-InboundConnector -Identity \"Proofpoint Inbound connector\" -RestrictDomainsToIPAddresses $True\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#issue-2-outbound-mail-not-routing-through-proofpoint","title":"Issue 2: Outbound Mail Not Routing Through Proofpoint","text":"<p>Symptoms: - Outbound emails bypass Proofpoint filtering - No Proofpoint headers in sent messages</p> <p>Root Causes: 1. Outbound connector not configured properly 2. Connector scope too restrictive 3. Connector disabled</p> <p>Solutions: <pre><code># Check outbound connector\nGet-OutboundConnector -Identity \"ProofPoint Outbound connector\" | \n    Select-Object Name, Enabled, SmartHosts, ConnectorType\n\n# Verify connector scope\nGet-OutboundConnector -Identity \"ProofPoint Outbound connector\" | \n    Select-Object -ExpandProperty RecipientDomains\n\n# Enable if disabled\nSet-OutboundConnector -Identity \"ProofPoint Outbound connector\" -Enabled $True\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#issue-3-smtp-relay-authentication-failures","title":"Issue 3: SMTP Relay Authentication Failures","text":"<p>Symptoms: - Internal systems can't send email - Authentication errors in logs</p> <p>Error Messages: <pre><code>550 5.7.60 SMTP; Client does not have permissions to send as this sender\n</code></pre></p> <p>Root Causes: 1. IP address not in SMTP relay connector 2. Sender domain not accepted 3. Connector misconfigured</p> <p>Solutions: <pre><code># Check SMTP relay connector\nGet-InboundConnector -Identity \"SMTP Relay\" | \n    Select-Object Name, Enabled, RestrictDomainsToCertificate\n\n# Verify IP ranges\nGet-InboundConnector -Identity \"SMTP Relay\" | \n    Select-Object -ExpandProperty RestrictDomainsToCertificate\n\n# Add missing IP if needed\n$currentIPs = (Get-InboundConnector -Identity \"SMTP Relay\").RestrictDomainsToCertificate\n$newIPs = $currentIPs + \"192.168.1.100\"\nSet-InboundConnector -Identity \"SMTP Relay\" -RestrictDomainsToCertificate $newIPs\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#issue-4-double-scanning-scl-not-set-to-1","title":"Issue 4: Double-Scanning (SCL Not Set to -1)","text":"<p>Symptoms: - Legitimate emails marked as spam - Proofpoint-approved emails quarantined</p> <p>Root Causes: 1. Spam bypass rule not working 2. Incorrect IP ranges in transport rule 3. Rule priority too low</p> <p>Solutions: <pre><code># Check transport rule\nGet-TransportRule -Identity \"ProofPoint Spam bypass\" | \n    Select-Object Name, State, Priority, SetSCL\n\n# Verify rule conditions\nGet-TransportRule -Identity \"ProofPoint Spam bypass\" | \n    Select-Object -ExpandProperty SenderIPRanges\n\n# Fix rule priority if needed\nSet-TransportRule -Identity \"ProofPoint Spam bypass\" -Priority 1\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#issue-5-spf-authentication-failures","title":"Issue 5: SPF Authentication Failures","text":"<p>Symptoms: - Outbound emails fail SPF checks - DMARC failures due to SPF issues</p> <p>Root Causes: 1. Missing IP addresses in SPF record 2. Too many DNS lookups (over 10) 3. Incorrect SPF syntax</p> <p>Solutions: <pre><code># Test SPF record\ndig TXT yourdomain.com | grep \"v=spf1\"\n\n# Check lookup count\n# Use online SPF checker to count DNS lookups\n\n# Verify all includes resolve\ndig TXT _spf1.yourdomain.com\ndig TXT _spf2.yourdomain.com\n</code></pre></p> <p>SPF Record Fixes: - Add missing IP addresses to _spf1 or _spf2 records - Consolidate includes to reduce lookup count - Fix syntax errors in SPF record</p>"},{"location":"email-o365-proofpoint-part5/#issue-6-dkim-signature-failures","title":"Issue 6: DKIM Signature Failures","text":"<p>Symptoms: - DKIM authentication fails - DMARC alignment issues</p> <p>Root Causes: 1. DKIM keys not properly configured 2. DNS propagation issues 3. Key rotation problems</p> <p>Solutions: <pre><code># Test DKIM records\ndig TXT selector-1678913997._domainkey.yourdomain.com\ndig CNAME hs1-19543953._domainkey.yourdomain.com\n\n# Verify in Proofpoint admin portal\n# Check DKIM configuration and key status\n\n# Test with online DKIM validators\n</code></pre></p>"},{"location":"email-o365-proofpoint-part5/#monitoring-and-maintenance","title":"\ud83d\udcca Monitoring and Maintenance","text":""},{"location":"email-o365-proofpoint-part5/#daily-monitoring-tasks","title":"Daily Monitoring Tasks","text":""},{"location":"email-o365-proofpoint-part5/#mail-flow-health-check","title":"Mail Flow Health Check","text":"<pre><code># Daily connector status check\nGet-InboundConnector | Select-Object Name, Enabled, LastModifiedDateTime\nGet-OutboundConnector | Select-Object Name, Enabled, LastModifiedDateTime\n\n# Transport rule status\nGet-TransportRule -Identity \"ProofPoint Spam bypass\" | Select-Object Name, State\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#message-volume-analysis","title":"Message Volume Analysis","text":"<ul> <li>Check message trace for unusual patterns</li> <li>Monitor delivery failure rates</li> <li>Review quarantine reports</li> </ul>"},{"location":"email-o365-proofpoint-part5/#weekly-maintenance-tasks","title":"Weekly Maintenance Tasks","text":""},{"location":"email-o365-proofpoint-part5/#dmarc-report-analysis","title":"DMARC Report Analysis","text":"<ol> <li>Collect DMARC Reports</li> <li>Check configured reporting email addresses</li> <li>Download and analyze aggregate reports</li> <li> <p>Identify authentication failures</p> </li> <li> <p>Review Authentication Results</p> </li> <li>SPF pass/fail rates</li> <li>DKIM signature success rates</li> <li>DMARC alignment percentage</li> </ol>"},{"location":"email-o365-proofpoint-part5/#connector-performance-review","title":"Connector Performance Review","text":"<pre><code># Weekly connector usage statistics\nGet-MessageTrace -StartDate (Get-Date).AddDays(-7) -EndDate (Get-Date) | \n    Group-Object ConnectorName | \n    Select-Object Name, Count | \n    Sort-Object Count -Descending\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#monthly-maintenance-tasks","title":"Monthly Maintenance Tasks","text":""},{"location":"email-o365-proofpoint-part5/#security-policy-review","title":"Security Policy Review","text":"<ol> <li>Proofpoint Features Audit</li> <li>Review enabled security features</li> <li>Analyze quarantine patterns</li> <li> <p>Adjust filtering sensitivity if needed</p> </li> <li> <p>Transport Rule Effectiveness</p> </li> <li>Review rule hit counts</li> <li>Analyze false positives/negatives</li> <li>Update rule conditions as needed</li> </ol>"},{"location":"email-o365-proofpoint-part5/#ip-address-management","title":"IP Address Management","text":"<pre><code># Monthly IP address audit\n$spfRecord = (Resolve-DnsName -Name yourdomain.com -Type TXT | Where-Object {$_.Strings -like \"*v=spf1*\"}).Strings\n$smtpRelay = Get-InboundConnector -Identity \"SMTP Relay\" | Select-Object -ExpandProperty RestrictDomainsToCertificate\n\n# Compare and identify discrepancies\n# Update records as needed\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#quarterly-maintenance-tasks","title":"Quarterly Maintenance Tasks","text":""},{"location":"email-o365-proofpoint-part5/#dkim-key-rotation","title":"DKIM Key Rotation","text":"<ol> <li>Generate New Keys</li> <li>Create new DKIM keys in Proofpoint</li> <li> <p>Generate new Office 365 keys if needed</p> </li> <li> <p>DNS Updates</p> </li> <li>Publish new public keys to DNS</li> <li>Wait for propagation (24-48 hours)</li> <li>Activate new keys</li> <li>Remove old keys after verification</li> </ol>"},{"location":"email-o365-proofpoint-part5/#security-posture-review","title":"Security Posture Review","text":"<ol> <li>DMARC Policy Progression</li> <li>Analyze DMARC reports for 90 days</li> <li> <p>If ready, progress from p=none \u2192 p=quarantine \u2192 p=reject</p> </li> <li> <p>SPF Hardening</p> </li> <li>If DMARC policy is mature, consider changing from ~all to -all</li> <li>Ensure no legitimate sources will be blocked</li> </ol>"},{"location":"email-o365-proofpoint-part5/#maintenance-checklist-templates","title":"\ud83d\udccb Maintenance Checklist Templates","text":""},{"location":"email-o365-proofpoint-part5/#daily-checklist","title":"Daily Checklist","text":"<ul> <li>[ ] Check connector status (all enabled)</li> <li>[ ] Review message trace for errors</li> <li>[ ] Monitor delivery failure rates</li> <li>[ ] Check quarantine for false positives</li> </ul>"},{"location":"email-o365-proofpoint-part5/#weekly-checklist","title":"Weekly Checklist","text":"<ul> <li>[ ] Download and review DMARC reports</li> <li>[ ] Analyze authentication failure patterns</li> <li>[ ] Review Proofpoint quarantine reports</li> <li>[ ] Check transport rule effectiveness</li> </ul>"},{"location":"email-o365-proofpoint-part5/#monthly-checklist","title":"Monthly Checklist","text":"<ul> <li>[ ] Audit IP address lists for accuracy</li> <li>[ ] Review security feature effectiveness</li> <li>[ ] Update documentation if changes made</li> <li>[ ] Train users on new security features</li> </ul>"},{"location":"email-o365-proofpoint-part5/#quarterly-checklist","title":"Quarterly Checklist","text":"<ul> <li>[ ] Rotate DKIM keys</li> <li>[ ] Review DMARC policy progression</li> <li>[ ] Conduct full mail flow testing</li> <li>[ ] Update emergency procedures</li> <li>[ ] Review and update monitoring alerts</li> </ul>"},{"location":"email-o365-proofpoint-part5/#performance-optimization-tips","title":"\ud83c\udfaf Performance Optimization Tips","text":""},{"location":"email-o365-proofpoint-part5/#dns-optimization","title":"DNS Optimization","text":"<ul> <li>Lower TTL during changes: Use 300 seconds during updates</li> <li>Optimize SPF lookups: Minimize DNS queries to stay under 10</li> <li>Use IP addresses: Where possible, use IP4/IP6 instead of includes</li> </ul>"},{"location":"email-o365-proofpoint-part5/#connector-optimization","title":"Connector Optimization","text":"<ul> <li>Monitor connection limits: Ensure adequate capacity</li> <li>Optimize TLS settings: Use appropriate encryption levels</li> <li>Regular IP list updates: Keep allowlists current</li> </ul>"},{"location":"email-o365-proofpoint-part5/#proofpoint-optimization","title":"Proofpoint Optimization","text":"<ul> <li>Fine-tune filtering: Adjust sensitivity based on false positive rates</li> <li>Optimize quarantine policies: Set appropriate retention periods</li> <li>Regular feature review: Enable new features as they become available</li> </ul>"},{"location":"email-o365-proofpoint-part5/#advanced-troubleshooting-tools","title":"\ud83d\ude80 Advanced Troubleshooting Tools","text":""},{"location":"email-o365-proofpoint-part5/#powershell-diagnostic-scripts","title":"PowerShell Diagnostic Scripts","text":""},{"location":"email-o365-proofpoint-part5/#comprehensive-mail-flow-test","title":"Comprehensive Mail Flow Test","text":"<pre><code># Test all connectors\nWrite-Host \"Testing Inbound Connectors...\" -ForegroundColor Green\nGet-InboundConnector | Test-InboundConnector\n\nWrite-Host \"Testing Outbound Connectors...\" -ForegroundColor Green  \nGet-OutboundConnector | Test-OutboundConnector\n\n# Check transport rules\nWrite-Host \"Checking Transport Rules...\" -ForegroundColor Green\nGet-TransportRule | Where-Object {$_.State -eq \"Enabled\"} | Select-Object Name, Priority, State\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#authentication-status-check","title":"Authentication Status Check","text":"<pre><code># Check recent authentication results\nGet-MessageTrace -StartDate (Get-Date).AddHours(-4) -EndDate (Get-Date) | \n    Get-MessageTraceDetail | \n    Where-Object {$_.Event -eq \"RECEIVE\"} | \n    Select-Object Date, SenderAddress, RecipientAddress, Data\n</code></pre>"},{"location":"email-o365-proofpoint-part5/#external-validation-tools","title":"External Validation Tools","text":""},{"location":"email-o365-proofpoint-part5/#recommended-online-tools","title":"Recommended Online Tools","text":"<ul> <li>MXToolbox: Comprehensive DNS and mail server testing</li> <li>DMARC Analyzer: DMARC record validation and reporting</li> <li>Mail Tester: End-to-end email deliverability testing</li> <li>DKIM Validator: DKIM signature verification</li> <li>SPF Record Checker: SPF syntax and lookup validation</li> </ul>"},{"location":"email-o365-proofpoint-part5/#implementation-complete","title":"\ud83c\udf89 Implementation Complete!","text":"<p>Congratulations! You've successfully implemented a comprehensive email security solution with:</p>"},{"location":"email-o365-proofpoint-part5/#what-youve-achieved","title":"What You've Achieved","text":"<ul> <li>\u2705 SPF Protection: Prevents domain spoofing</li> <li>\u2705 DKIM Authentication: Ensures email integrity</li> <li>\u2705 DMARC Policy: Provides reporting and enforcement</li> <li>\u2705 Proofpoint Filtering: Advanced threat protection</li> <li>\u2705 Office 365 Integration: Seamless mail flow</li> <li>\u2705 Monitoring Capabilities: Ongoing security visibility</li> </ul>"},{"location":"email-o365-proofpoint-part5/#security-benefits","title":"Security Benefits","text":"<ul> <li>\ud83d\udee1\ufe0f Reduced Phishing: Email authentication blocks spoofed emails</li> <li>\ud83d\udd0d Threat Detection: Advanced scanning catches malicious content</li> <li>\ud83d\udcca Visibility: DMARC reports show email ecosystem health</li> <li>\u26a1 Incident Response: Tools for quick threat remediation</li> <li>\ud83d\udc65 User Awareness: Warning tags educate users about risks</li> </ul>"},{"location":"email-o365-proofpoint-part5/#next-steps","title":"Next Steps","text":"<ol> <li>Monitor for 30 days: Watch for any issues or false positives</li> <li>Train users: Educate staff on new security features</li> <li>Review DMARC reports: Analyze authentication patterns</li> <li>Plan policy hardening: Gradually increase security strictness</li> <li>Regular maintenance: Follow the maintenance schedules provided</li> </ol>"},{"location":"email-o365-proofpoint-part5/#series-navigation_1","title":"\ud83d\udcd6 Series Navigation","text":"<ul> <li>\u2190 Part 4: Proofpoint Integration Setup</li> <li>Part 5: Testing and Troubleshooting (Current)</li> </ul> <p>Your email security implementation is now complete and operational. Remember to follow the maintenance procedures to keep your system secure and effective.</p>"},{"location":"email-o365-proofpoint-part5/#support-resources","title":"\ud83d\udcde Support Resources","text":""},{"location":"email-o365-proofpoint-part5/#microsoft-resources","title":"Microsoft Resources","text":"<ul> <li>Exchange Online Documentation: docs.microsoft.com/exchange</li> <li>Message Trace Guide: Search Microsoft 365 admin center help</li> <li>PowerShell Reference: docs.microsoft.com/powershell/exchange</li> </ul>"},{"location":"email-o365-proofpoint-part5/#proofpoint-resources","title":"Proofpoint Resources","text":"<ul> <li>Admin Portal: Your Proofpoint Essentials admin interface</li> <li>Documentation: Available within the admin portal</li> <li>Support: Contact Proofpoint support for service-specific issues</li> </ul>"},{"location":"email-o365-proofpoint-part5/#community-resources","title":"Community Resources","text":"<ul> <li>DMARC.org: Comprehensive DMARC information and best practices</li> <li>M3AAWG: Messaging Anti-Abuse Working Group resources</li> <li>Email Authentication: Industry best practices and standards</li> </ul>"},{"location":"fabric-wharehouse-fact-import/","title":"Importing Data into Fabric Data Warehouse","text":""},{"location":"fabric-wharehouse-fact-import/#introduction","title":"Introduction","text":"<p>In this article, we will walk through a robust Python-based workflow for importing employee data into Azure Fabric Data Warehouse (DW). This workflow leverages a combination of Azure SDKs, ODBC, and custom utility functions to automate the process of uploading, transforming, and loading data from CSV files into a cloud-based data warehouse. We will explore the key functions involved, their implementation, and how they work together to achieve a seamless data integration pipeline.</p>"},{"location":"fabric-wharehouse-fact-import/#python-libraries-used","title":"Python Libraries Used","text":"<ul> <li>pyodbc: Enables Python to connect to ODBC-compliant databases, such as SQL Server, for executing SQL queries and transactions.</li> <li>os, sys: Standard Python libraries for interacting with the operating system and system-specific parameters.</li> <li>csv: Provides tools for reading and writing CSV files.</li> <li>datetime: Used for manipulating dates and times.</li> <li>io.StringIO: Allows treating strings as file-like objects, useful for CSV parsing.</li> <li>typing: Provides type hints for better code clarity and static analysis.</li> <li>azure.storage.filedatalake: Azure SDK for interacting with Azure Data Lake Storage Gen2, including file and directory operations.</li> </ul>"},{"location":"fabric-wharehouse-fact-import/#the-data-import-workflow","title":"The Data Import Workflow","text":"<p>The workflow is orchestrated by the <code>process_fabric_employees_dw</code> function, which coordinates the following steps:</p> <ol> <li>Upload the latest HR system file to the Azure Data Lake 'unprocessed' folder.</li> <li>Retrieve all unprocessed files from the Data Lake.</li> <li>Fetch metadata from the HR SQL Data Warehouse.</li> <li>For each unprocessed file:<ul> <li>Parse the CSV content.</li> <li>Determine the date identifier for the data.</li> <li>Transform the data for database insertion.</li> <li>Insert the transformed data into the database.</li> <li>Move the processed file to the 'processed' folder in Data Lake.</li> </ul> </li> </ol> <p>Let's examine the key functions involved in this workflow.</p>"},{"location":"fabric-wharehouse-fact-import/#1-process_fabric_employees_dw","title":"1. <code>process_fabric_employees_dw</code>","text":"<p>This is the main orchestration function. It manages the end-to-end process of uploading, processing, and loading employee data files.</p> <pre><code>def process_fabric_employees_dw(adp_file):\n    try:\n        upload_file_to_datalake(adp_file)\n        files = get_employee_unprocessed_files()\n        if files:\n            metadata = get_metadata_lists_from_hr_sql_wh()\n            department, costcenter, jobfunction, location, payrate, terminationreason, *_ = metadata\n            try:\n                for file in files:\n                    file_name = file['file_name']\n                    file_datetime = file['file_datetime']\n                    logger.info(f\"Processing file: {file_name} (Datetime: {file_datetime})\")\n                    headers, rows = parse_csv(file['file_content'])\n                    date_auto_id = determine_date_auto_id(file_datetime.date().strftime('%Y-%m-%d'))\n                    transformed_data = transform_data(headers, rows, date_auto_id, department, costcenter, jobfunction, location, payrate, terminationreason)\n                    insertsuccessfull = insert_into_db(date_auto_id, transformed_data, f\"{HR_DBOBJECT_PREFIX}.[FactEmployee]\", batch_size=50)\n                    if insertsuccessfull:\n                        move_file_to_processed_folder(file_name, f'{DATALAKE_UNPROCESSED_FOLDER}', f'{DATALAKE_PROCESSED_FOLDER}')\n                        logger.info(f\"Finished Successfully Processing file: {file_name} (Datetime: {file_datetime})\")\n            except Exception as localexception:\n                handle_global_exception(sys._getframe().f_code.co_name, localexception)\n    except Exception as exception_obj:\n        handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function first uploads the provided HR system file to the Data Lake, then processes all unprocessed files by parsing, transforming, and loading them into the database. Successfully processed files are moved to a 'processed' folder.</p>"},{"location":"fabric-wharehouse-fact-import/#2-upload_file_to_datalake","title":"2. <code>upload_file_to_datalake</code>","text":"<p>Handles uploading a local file to the Azure Data Lake 'unprocessed' folder.</p> <pre><code>def upload_file_to_datalake(file_path: str):\n    try:\n        datalake_folder = DATALAKE_UNPROCESSED_FOLDER\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        file_name = os.path.basename(file_path)\n        destination_path = f\"{datalake_folder}/{file_name}\"\n        with open(ADP_LOCAL_FOLDER + file_path, \"rb\") as file_data:\n            file_client = file_system_client.get_file_client(destination_path)\n            file_client.upload_data(file_data, overwrite=True)\n        logger.info(f\"Uploaded file {file_name} to {destination_path} in Data Lake.\")\n    except Exception as e:\n        logger.error(f\"Failed to upload file {file_path} to Data Lake: {e}\")\n        raise\n</code></pre> <p>This function uses the Azure Data Lake SDK to authenticate and upload the file. The file is placed in the 'unprocessed' folder, ready for further processing.</p>"},{"location":"fabric-wharehouse-fact-import/#3-get_employee_unprocessed_files","title":"3. <code>get_employee_unprocessed_files</code>","text":"<p>Retrieves all files in the 'unprocessed' folder of the Data Lake that match the expected filename format.</p> <pre><code>def get_employee_unprocessed_files() -&gt; List[Dict[str, str]]:\n    try:\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        paths = file_system_client.get_paths(path=f\"{DATALAKE_UNPROCESSED_FOLDER}\")\n        valid_files = []\n        for path in paths:\n            if not path.is_directory:\n                file_name = path.name.split(\"/\")[-1]\n                file_datetime = GDEPUtils.parse_datetime_from_filename(file_name)\n                if file_datetime:\n                    try:\n                        file_client = file_system_client.get_file_client(path.name)\n                        file_content = file_client.download_file().readall().decode('utf-8-sig')\n                        valid_files.append({\n                            \"file_name\": file_name,\n                            \"file_datetime\": file_datetime,\n                            \"file_content\": file_content,\n                        })\n                    except Exception as exception_obj:\n                        logger.warning(f\"Failed to process file {file_name}: {exception_obj}\")\n        valid_files.sort(key=lambda x: x[\"file_datetime\"])\n        GDEPUtils.send_email(recipients=GDEPUtils.EMAIL_TO_SEND_EXCEPTIONS, subject='Number of files', plain_message=f'Found {len(valid_files)} valid files.')\n        logger.info(f\"Found {len(valid_files)} valid files.\")\n        return valid_files\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n        return []\n</code></pre> <p>This function lists all files in the 'unprocessed' directory, filters them by filename pattern, downloads their content, and returns a list of valid files for processing.</p>"},{"location":"fabric-wharehouse-fact-import/#4-get_metadata_lists_from_hr_sql_wh","title":"4. <code>get_metadata_lists_from_hr_sql_wh</code>","text":"<p>Fetches reference metadata from the HR SQL Data Warehouse, such as departments, cost centers, job functions, etc.</p> <pre><code>def get_metadata_lists_from_hr_sql_wh():\n    try:\n        metadata_sources = [\n            (f\"{HR_DBOBJECT_PREFIX}.[DimDepartment]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimCostCenter]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimJobFunction]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimLocation]\", lambda row: (row[1], {row[0]: row[3]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimPayRate]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimTerminationReason]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimDate]\", lambda row: (row[1].strftime('%m-%d-%Y'), {row[0]: row[1]})),\n        ]\n        return_list = []\n        sql_connection = get_sql_server_connection_hr_wh()\n        db_cursor = sql_connection.cursor()\n        for query, processor in metadata_sources:\n            working_dict = {}\n            try:\n                db_cursor.execute(f\"SELECT * FROM {query}\")\n                rows = db_cursor.fetchall()\n                for row in rows:\n                    key, value = processor(row)\n                    working_dict[key] = value\n            except Exception as exception_obj:\n                GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n            return_list.append(working_dict)\n        return return_list\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function queries several dimension tables in the data warehouse and returns their contents as dictionaries for use in data transformation.</p>"},{"location":"fabric-wharehouse-fact-import/#5-parse_csv","title":"5. <code>parse_csv</code>","text":"<p>Parses the content of a CSV file into headers and rows.</p> <pre><code>def parse_csv(file_content: str) -&gt; Tuple[List[str], List[List[str]]]:\n    try:\n        csv_reader = csv.reader(StringIO(file_content))\n        file_data = list(csv_reader)\n        headers = file_data[0]\n        rows = file_data[1:]\n        return headers, rows\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function uses Python's built-in <code>csv</code> module to parse the file content, returning the header row and the data rows separately.</p>"},{"location":"fabric-wharehouse-fact-import/#6-determine_date_auto_id","title":"6. <code>determine_date_auto_id</code>","text":"<p>Determines or creates a unique date identifier for a given transaction date in the data warehouse.</p> <pre><code>def determine_date_auto_id(date: str) -&gt; int:\n    sql_connection = None\n    try:\n        sql_connection = get_sql_server_connection_hr_wh()\n        cursor = sql_connection.cursor()\n        cursor.execute(f\"SELECT DateAutoID FROM {HR_DBOBJECT_PREFIX}.[DimDate] WHERE TransactionDate = ?\", (date,))\n        result = cursor.fetchone()\n        if result:\n            return result[0]\n        cursor.execute(f\"SELECT MAX(DateAutoID) FROM {HR_DBOBJECT_PREFIX}.[DimDate]\")\n        max_date_auto_id = cursor.fetchone()[0]\n        new_date_auto_id = (max_date_auto_id or 0) + 1\n        cursor.execute(f\"INSERT INTO {HR_DBOBJECT_PREFIX}.[DimDate] (DateAutoID, TransactionDate) VALUES (?, ?)\", (new_date_auto_id, date))\n        sql_connection.commit()\n        return new_date_auto_id\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n        raise\n    finally:\n        if sql_connection:\n            sql_connection.close()\n</code></pre> <p>This function checks if a date already exists in the <code>DimDate</code> table. If not, it inserts a new record and returns the new identifier.</p>"},{"location":"fabric-wharehouse-fact-import/#7-transform_data","title":"7. <code>transform_data</code>","text":"<p>Transforms raw CSV data into the format required for database insertion, mapping codes to IDs using metadata.</p> <pre><code>def transform_data(headers: List[str], rows: List[List[str]], date_auto_id: int, department: Dict[str, Dict], costcenter: Dict[int, Dict], jobfunction: Dict[str, Dict], location: Dict[str, Dict], payrate: Dict[str, Dict], terminationreason: Dict[str, Dict]) -&gt; List[List]:\n    try:\n        def get_code_value(dictionary: Dict, code, default=None) -&gt; int:\n            if code is None or (isinstance(code, str) and not code.strip()):\n                return int(list(dictionary.get(default, {0: None}).keys())[0])\n            return int(list(dictionary.get(code, {0: None}).keys())[0])\n        transformed_data = []\n        for row in rows:\n            row_dict = dict(zip(headers, row))\n            cost_center_number = row_dict.get('Home Cost Number Code', 0)\n            try:\n                cost_center_number = int(cost_center_number)\n            except (ValueError, TypeError):\n                cost_center_number = 0\n            is_management_position = row_dict.get('This is a Management position', 'false').strip()\n            try:\n                if is_management_position.lower() == 'yes':\n                    is_management_position = True\n                else:\n                    is_management_position = False\n            except Exception:\n                is_management_position = False\n            transformed_data.append([\n                date_auto_id,\n                get_code_value(department, row_dict.get('Home Department Code', 'UNDF')),\n                get_code_value(costcenter, cost_center_number),\n                get_code_value(jobfunction, row_dict.get('Job Function Code', 'UNDF')),\n                get_code_value(location, row_dict.get('Location Code', 'UNDF')),\n                get_code_value(payrate, row_dict.get('Regular Pay Rate Code', 'UNDF')),\n                get_code_value(terminationreason, row_dict.get('Termination Reason Code', 'UNDF')),\n                str(row_dict.get('File Number', '')).strip(),\n                str(row_dict.get('Position ID', '')).strip(),\n                str(row_dict.get('Legal First Name', '')).strip(),\n                str(row_dict.get('Preferred First Name', '')).strip(),\n                '',\n                str(row_dict.get('Last Name', '')).strip(),\n                convert_to_date(row_dict.get('Hire Date', '')),\n                convert_to_date(row_dict.get('Rehire Date', '')),\n                str(row_dict.get('Job Title Description', '')).strip(),\n                is_management_position,\n                str(row_dict.get('Work Contact: Work Email', '')).strip(),\n                str(row_dict.get('Personal Contact: Personal Email', '')).strip(),\n                str(row_dict.get('Reports To Name', '')).strip(),\n                str(row_dict.get('Reports To Position ID', '')).strip(),\n                str(row_dict.get('Payroll Company Code', '')).strip(),\n                str(row_dict.get('Job Class Description', '')).strip(),\n                str(row_dict.get('Position Status', '')).strip(),\n                str(row_dict.get('Personal Contact: Personal Mobile', '')).strip(),\n                str(row_dict.get('Work Contact: Work Phone', '')).strip(),\n                convert_to_date(row_dict.get('Termination Date', '')),\n                str(row_dict.get('Worker Category Code', '')).strip(),\n                str(row_dict.get('Associate ID', '')).strip(),\n                str(row_dict.get('Assigned Shift Description', '')).strip(),\n                str(row_dict.get('FLSA Description', '')).strip(),\n            ])\n        return transformed_data\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function maps each row of the CSV to the required database schema, converting codes to IDs and handling data type conversions.</p>"},{"location":"fabric-wharehouse-fact-import/#8-insert_into_db","title":"8. <code>insert_into_db</code>","text":"<p>Inserts the transformed data into the target SQL table in batches for efficiency.</p> <pre><code>def insert_into_db(date_auto_id: int, data: List[List], table_name: str, batch_size: int = 50):\n    try:\n        returnvalue = False\n        sql_connection = get_sql_server_connection_hr_wh()\n        cursor = sql_connection.cursor()\n        cursor.execute(f\"DELETE FROM {table_name} WHERE DateAutoID = {date_auto_id}\")\n        sql_connection.commit()\n        sql_query = f\"INSERT INTO {table_name} ([DateAutoID],[DepartmentAutoID],[CostCenterAutoID],[JobFunctionAutoID],[LocationAutoID],[PayRateAutoID],[TerminationReasonAutoID],[FileNumber],[PositionID],[FirstName],[PreferredFirstName],[MiddleInitial],[LastName],[HireDate],[RehireDate],[JobTitleDescription],[IsManagementPosition],[WorkEmail],[PersonalEmail],[ManagerName],[ManagerPositionID],[PayrollCompanyCode],[JobClassDescription],[PositionStatus],[PersonalMobile],[WorkMobile],[TerminationDate],[WorkerCategoryCode],[AssociateID],[AssignedShiftDescription],[FLSADescription]) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n        cursor.fast_executemany = True\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i + batch_size]\n            cursor.executemany(sql_query, batch)\n        sql_connection.commit()\n        logger.info(f\"Inserted {len(data)} rows into {table_name}.\")\n        returnvalue = True\n        return returnvalue\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n    finally:\n        sql_connection.close()\n        return returnvalue\n</code></pre> <p>This function first deletes any existing records for the given date, then inserts the new data in batches for performance.</p>"},{"location":"fabric-wharehouse-fact-import/#9-move_file_to_processed_folder","title":"9. <code>move_file_to_processed_folder</code>","text":"<p>Moves a file from the 'unprocessed' folder to the 'processed' folder in Azure Data Lake by renaming it.</p> <pre><code>def move_file_to_processed_folder(file_name: str, source_folder: str, destination_folder: str):\n    try:\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        source_path = f\"{source_folder}/{file_name}\"\n        destination_path = f\"{destination_folder}/{file_name}\"\n        logger.info(f\"Source Path: {source_path}\")\n        logger.info(f\"Destination Path: {destination_path}\")\n        file_client = file_system_client.get_file_client(source_path)\n        file_client.rename_file(f\"{file_system_client.file_system_name}/{destination_path}\")\n        logger.info(f\"Moved file {file_name} from {source_folder} to {destination_folder}.\")\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function uses the Azure Data Lake SDK to rename (move) the file, ensuring that processed files are archived and not reprocessed.</p>"},{"location":"fabric-wharehouse-fact-import/#supplement-get_client_sceret_credential","title":"Supplement: <code>get_client_sceret_credential</code>","text":"<p>This function provides the Azure credential used for authentication in all Data Lake and Azure SDK operations. It is a wrapper around Azure's <code>ClientSecretCredential</code> and is essential for secure, programmatic access to Azure resources.</p> <pre><code>def get_client_sceret_credential():\n    try:\n        return ClientSecretCredential(\n            tenant_id=AZURE_TENANT_ID,\n            client_id=AZURE_CONFIDENTIAL_APP_ID,\n            client_secret=AZURE_CONFIDENTIAL_SECRET\n        )\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre> <p>This function should be used in place of any direct reference to <code>GDEPAzure.get_client_sceret_credential</code> in the workflow. For example, in the <code>upload_file_to_datalake</code> and other related functions, replace <code>GDEPAzure.get_client_sceret_credential()</code> with <code>get_client_sceret_credential()</code> for clarity and modularity.</p>"},{"location":"fabric-wharehouse-fact-import/#references","title":"References","text":"<ul> <li>Azure Data Lake Storage Gen2 Python SDK</li> </ul>"},{"location":"fabric-wharehouse-fact-import/#conclusion","title":"Conclusion","text":"<p>By combining Python's data processing capabilities with Azure's scalable storage and compute, this workflow provides a reliable and efficient way to automate the import of employee data into Azure Fabric Data Warehouse. The modular design allows for easy extension and maintenance, while leveraging batch operations and cloud-native features ensures performance and scalability. This approach can be adapted for similar ETL scenarios involving other data sources and targets in the Azure ecosystem.</p>"},{"location":"github-clean-deployments/","title":"Automating GitHub Deployment Cleanup with Bash, GitHub CLI, and jq","text":"<p>Managing deployments in GitHub can be tedious, especially when you want to delete all deployments and there is no UI option to do so. This article explains how to automate the cleanup of deployments in a GitHub repository using a Bash script, the GitHub CLI, and the <code>jq</code> tool for JSON parsing.</p> <p>Note: This is a focused, practical solution for a common DevOps quirk\u2014bulk deleting deployments from a GitHub repo when no UI exists for this action.</p>"},{"location":"github-clean-deployments/#prerequisites","title":"Prerequisites","text":"<ol> <li>GitHub CLI: Install the GitHub CLI (<code>gh</code>) for interacting with GitHub from the command line.</li> <li>jq: Install <code>jq</code> for parsing JSON responses.</li> <li>Personal Access Token (PAT): Export your GitHub PAT as an environment variable:    <pre><code>export GITHUB_TOKEN=\"&lt;YOUR_PERSONAL_ACCESS_TOKEN&gt;\"\n</code></pre></li> <li>Make the Script Executable:    <pre><code>chmod +x ./scripts/github/deployments.sh\n</code></pre></li> </ol>"},{"location":"github-clean-deployments/#the-script-deploymentssh","title":"The Script: <code>deployments.sh</code>","text":"<p>Below is the full code for the script that automates the deletion of deployments in a GitHub repository:</p> <pre><code>#!/bin/sh\n\n# Set your repository and token\n# Save the token in the environment variable GITHUB_TOKEN\n# Example: export GITHUB_TOKEN=\"&lt;PAT GOES HERE&gt;\"\n\n# Ensure the script is executable\n# Example: chmod +x ./scripts/deployments.sh\n\n# Ensure GitHub CLI and jq libraries are available\n# They should be included in your Dockerfile or build environment\n\n# Repository to manage deployments\nREPO=\"GDEnergyproducts/GDEP-IAC\"\n\n# Retrieve the GitHub token from environment variable\nTOKEN=\"${GITHUB_TOKEN}\"\n\n# Fetch the list of deployments and extract deployment IDs\necho \"Fetching deployment IDs...\"\nDEPLOYMENTS=$(curl -s -H \"Authorization: token $TOKEN\" \\\n                    -H \"Accept: application/vnd.github.v3+json\" \\\n                    https://api.github.com/repos/$REPO/deployments \\\n                    | jq -r '.[] | .id')\n\n# Check if there are any deployments\nif [ -z \"$DEPLOYMENTS\" ]; then\n  echo \"No deployments found.\"\n  exit 0\nfi\n\n# Print the list of deployment IDs\necho \"Deployments found:\"\necho \"$DEPLOYMENTS\"\n\n# Determine the active deployment ID\nACTIVE_DEPLOYMENT_ID=$(curl -s -H \"Authorization: token $TOKEN\" \\\n                             -H \"Accept: application/vnd.github.v3+json\" \\\n                             https://api.github.com/repos/$REPO/deployments \\\n                             | jq -r '.[] | select(.status == \"active\") | .id')\n\necho \"Active Deployment ID: $ACTIVE_DEPLOYMENT_ID\"\n\n# Loop through each deployment ID and delete it, skipping the active deployment\necho \"Deleting deployments...\"\nfor ID in $DEPLOYMENTS; do\n  if [ \"$ID\" != \"$ACTIVE_DEPLOYMENT_ID\" ]; then\n    echo \"Deleting deployment $ID\"\n    curl -X DELETE -H \"Authorization: token $TOKEN\" \\\n         -H \"Accept: application/vnd.github.v3+json\" \\\n         https://api.github.com/repos/$REPO/deployments/$ID\n  else\n    echo \"Skipping active deployment $ID\"\n  fi\n done\n</code></pre>"},{"location":"github-clean-deployments/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Set Up Environment Variables</li> <li> <p>The script expects your GitHub Personal Access Token to be set as <code>GITHUB_TOKEN</code> in your environment.</p> </li> <li> <p>Fetch Deployment IDs</p> </li> <li>Uses <code>curl</code> to call the GitHub API and retrieve all deployments for the specified repository.</li> <li> <p>Pipes the JSON response to <code>jq</code> to extract all deployment IDs.</p> </li> <li> <p>Check for Deployments</p> </li> <li> <p>If no deployments are found, the script exits gracefully.</p> </li> <li> <p>Identify the Active Deployment</p> </li> <li>Fetches deployments again and uses <code>jq</code> to find the deployment with status <code>active</code> (if any).</li> <li> <p>This deployment is skipped during deletion to avoid disrupting an active deployment.</p> </li> <li> <p>Delete Deployments</p> </li> <li>Loops through all deployment IDs.</li> <li>For each deployment, if it is not the active deployment, sends a DELETE request to the GitHub API to remove it.</li> <li>Prints a message for each deletion or if skipping the active deployment.</li> </ol>"},{"location":"github-clean-deployments/#why-use-jq","title":"Why Use jq?","text":"<ul> <li><code>jq</code> is a lightweight and flexible command-line JSON processor.</li> <li>It allows you to extract, filter, and manipulate JSON data returned by the GitHub API with ease.</li> <li>In this script, <code>jq</code> is used to:</li> <li>List all deployment IDs: <code>.[] | .id</code></li> <li>Find the active deployment: <code>.[] | select(.status == \"active\") | .id</code></li> </ul>"},{"location":"github-clean-deployments/#usage","title":"Usage","text":"<ol> <li>Export your GitHub token:    <pre><code>export GITHUB_TOKEN=\"&lt;YOUR_PERSONAL_ACCESS_TOKEN&gt;\"\n</code></pre></li> <li>Make the script executable:    <pre><code>chmod +x ./scripts/github/deployments.sh\n</code></pre></li> <li>Run the script from your project root:    <pre><code>./scripts/github/deployments.sh\n</code></pre></li> </ol>"},{"location":"github-clean-deployments/#summary","title":"Summary","text":"<ul> <li>This script provides a simple, automated way to clean up deployments in a GitHub repository.</li> <li>It leverages the GitHub API, <code>curl</code>, and <code>jq</code> for powerful, flexible automation.</li> <li>There is no UI in GitHub to bulk delete deployments\u2014this script fills that gap for DevOps teams.</li> </ul> <p>Automate your GitHub deployment cleanup today!</p>"},{"location":"meraki-nagios-device-sync/","title":"Automating Cisco Meraki Device Discovery and Nagios XI Monitoring Integration","text":""},{"location":"meraki-nagios-device-sync/#introduction","title":"Introduction","text":"<p>Keeping your network monitoring system in sync with your actual device inventory is critical for reliable operations. This article provides a deep dive into a robust Python workflow that:</p> <ul> <li>Discovers all current devices from the Cisco Meraki cloud API</li> <li>Uses SNMP OIDs to obtain Meraki hostnames</li> <li>Compares Meraki inventory to Nagios XI monitored hosts</li> <li>Adds missing devices to Nagios XI, including handling special device types</li> <li>Checks firmware status for compliance</li> </ul> <p>All code is provided and explained so you can adapt this solution for your own environment.</p>"},{"location":"meraki-nagios-device-sync/#required-python-libraries","title":"Required Python Libraries","text":"<p>This workflow uses the following Python libraries:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library. Used for all Meraki cloud API calls.</li> <li>requests: For making HTTP requests to the Nagios XI REST API.</li> <li>subprocess: To run SNMP commands (e.g., <code>snmpwalk</code>) from Python.</li> <li>re: For parsing SNMP command output with regular expressions.</li> </ul> <p>Install any missing libraries with pip:</p> <pre><code>pip install meraki requests\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1-authenticating-to-cisco-meraki-and-nagios-xi-apis","title":"1. Authenticating to Cisco Meraki and Nagios XI APIs","text":""},{"location":"meraki-nagios-device-sync/#cisco-meraki-api-authentication","title":"Cisco Meraki API Authentication","text":"<p>To connect to the Meraki Dashboard API, you need an API key. This key can be generated in your Meraki dashboard under Organization &gt; Settings &gt; Dashboard API access.</p> <pre><code>import meraki\n\nMERAKI_API_KEY = 'YOUR_MERAKI_API_KEY'  # Replace with your Meraki API key\nMERAKI_BASE_URL = 'https://api.meraki.com/api/v1/'\n\ndashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    base_url=MERAKI_BASE_URL,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre>"},{"location":"meraki-nagios-device-sync/#nagios-xi-api-authentication","title":"Nagios XI API Authentication","text":"<p>Nagios XI provides a REST API. You need an API key, which can be generated in the Nagios XI web interface under My Account &gt; API Keys.</p> <pre><code>import requests\n\nNAGIOS_XI_API_URL = 'https://your-nagios-server.example.com/nagiosxi/api/v1/'  # Replace with your Nagios XI URL\nNAGIOS_XI_API_KEY = 'YOUR_NAGIOS_API_KEY'  # Replace with your Nagios XI API key\n\ndef call_nagios_api(endpoint, method='GET', data=None):\n    url = f\"{NAGIOS_XI_API_URL}{endpoint}\"\n    headers = {'Authorization': f'Bearer {NAGIOS_XI_API_KEY}'}\n    if method == 'GET':\n        response = requests.get(url, headers=headers)\n    elif method == 'POST':\n        response = requests.post(url, headers=headers, json=data)\n    elif method == 'PUT':\n        response = requests.put(url, headers=headers, json=data)\n    elif method == 'DELETE':\n        response = requests.delete(url, headers=headers)\n    else:\n        raise ValueError('Unsupported HTTP method')\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1a-understanding-and-setting-up-meraki_dashboard_snmp_community_string","title":"1a. Understanding and Setting Up <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>","text":""},{"location":"meraki-nagios-device-sync/#what-is-meraki_dashboard_snmp_community_string","title":"What is <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>?","text":"<p>The <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code> is a shared secret (like a password) used for authenticating SNMP v2c queries to the Meraki cloud SNMP endpoint. It is required to retrieve device information via SNMP, such as hostnames and other device attributes.</p>"},{"location":"meraki-nagios-device-sync/#how-to-set-up-the-snmp-community-string-in-meraki-dashboard","title":"How to Set Up the SNMP Community String in Meraki Dashboard","text":"<ol> <li>Log in to your Meraki Dashboard</li> <li>Navigate to Organization &gt; Settings</li> <li>Scroll to the SNMP section</li> <li>Enable Cloud Monitoring (SNMP v2c)</li> <li>Set your desired SNMP Community String (e.g., <code>mysnmpcommunity</code>)</li> <li>Save your changes</li> <li>Whitelist your public IP address in the SNMP section to allow SNMP queries from your monitoring server</li> </ol> <p>Note: The SNMP community string acts as a password for SNMP v2c. Keep it secure and do not share it publicly.</p>"},{"location":"meraki-nagios-device-sync/#plugging-the-community-string-into-your-code","title":"Plugging the Community String into Your Code","text":"<p>In your Python code, set the value as follows:</p> <pre><code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING = 'mysnmpcommunity'  # Replace with your actual SNMP community string\n</code></pre> <p>This value is then used in SNMP queries, for example:</p> <pre><code>import subprocess\nimport re\n\ndef get_snmp_data(snmp_server, port, oid, community):\n    command = [\n        \"snmpwalk\",\n        \"-v\", \"2c\",\n        \"-c\", community,\n        f\"{snmp_server}:{port}\",\n        oid\n    ]\n    try:\n        result = subprocess.run(command, capture_output=True, text=True, check=True)\n        output = result.stdout\n        snmp_dict = {}\n        pattern = re.compile(r'(\\S+)\\s+=\\s+STRING:\\s+\"([^\"]+)\"')\n        for match in pattern.finditer(output):\n            oid = match.group(1)\n            string_value = match.group(2)\n            snmp_dict[string_value] = oid\n        return snmp_dict\n    except Exception as e:\n        print(f\"SNMP error: {e}\")\n        return None\n\nMERAKI_DASHBOARD_SNMP_HOST_NAME = 'snmp.meraki.com'\nMERAKI_DASHBOARD_SNMP_PORT = '16100'\n\nmerakihostnames = get_snmp_data(\n    MERAKI_DASHBOARD_SNMP_HOST_NAME,\n    MERAKI_DASHBOARD_SNMP_PORT,\n    '1.3.6.1.4.1.29671.1.1.4.1.2',\n    MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING\n)\n</code></pre> <p>If you change the community string in the Meraki dashboard, update it in your code as well.</p>"},{"location":"meraki-nagios-device-sync/#2-obtaining-all-current-devices-from-meraki","title":"2. Obtaining All Current Devices from Meraki","text":"<p>We use the official Meraki Dashboard API to fetch all organizations, devices, and networks:</p> <p><pre><code>gdepOrganizations = dashboard.organizations.getOrganizations()\norganizationid = gdepOrganizations[0]['id']\ngdepdevices = dashboard.organizations.getOrganizationDevices(organizationid, -1)\ngdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid, -1)\n</code></pre> - <code>getOrganizations()</code> returns all organizations your API key can access. - <code>getOrganizationDevices()</code> fetches all devices (appliances, switches, cameras, wireless, etc.). - <code>getOrganizationNetworks()</code> fetches all networks (logical groupings of devices).</p>"},{"location":"meraki-nagios-device-sync/#3-obtaining-meraki-hostnames-via-snmp-oid","title":"3. Obtaining Meraki Hostnames via SNMP OID","text":"<p>To get hostnames as seen by Meraki's SNMP dashboard, we use the SNMP OID <code>1.3.6.1.4.1.29671.1.1.4.1.2</code> (see code above).</p> <ul> <li>This function runs an <code>snmpwalk</code> command and parses the output into a dictionary of hostnames and OIDs.</li> <li>SNMP access must be enabled and your IP whitelisted in the Meraki dashboard.</li> </ul>"},{"location":"meraki-nagios-device-sync/#4-checking-firmware-status-for-each-network","title":"4. Checking Firmware Status for Each Network","text":"<p>For each network, we check the current firmware status of all products using the Meraki Dashboard API's <code>getNetworkFirmwareUpgrades</code> method.</p>"},{"location":"meraki-nagios-device-sync/#what-is-getnetworkfirmwareupgrades","title":"What is <code>getNetworkFirmwareUpgrades</code>?","text":"<p>This method retrieves the current and available firmware versions for all devices in a given Meraki network. It helps you: - Audit firmware compliance - Identify devices that need upgrades - Track which products are running which firmware</p>"},{"location":"meraki-nagios-device-sync/#example-usage","title":"Example Usage","text":"<pre><code># For each network, get firmware upgrade status\nfor network in gdepnetworks:\n    network_id = network['id']\n    networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network_id)\n    print(f\"Firmware info for network {network['name']}:\\n\", networkupgrades)\n    if 'products' in networkupgrades:\n        products = networkupgrades['products']\n        for product_type, firmware_info in products.items():\n            print(f\"Product: {product_type}\")\n            print(f\"Current Version: {firmware_info.get('currentVersion', {}).get('name', 'N/A')}\")\n            print(f\"Available Version: {firmware_info.get('availableVersion', {}).get('name', 'N/A')}\")\n            print(f\"Status: {firmware_info.get('status', 'N/A')}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#sample-output-structure","title":"Sample Output Structure","text":"<p>The returned dictionary typically looks like:</p> <pre><code>{\n  \"products\": {\n    \"appliance\": {\n      \"currentVersion\": {\"name\": \"MX 18.107.2\"},\n      \"availableVersion\": {\"name\": \"MX 18.107.4\"},\n      \"status\": \"Up to date\"\n    },\n    \"switch\": {\n      \"currentVersion\": {\"name\": \"MS 15.21\"},\n      \"availableVersion\": {\"name\": \"MS 15.22\"},\n      \"status\": \"Upgrade available\"\n    }\n  }\n}\n</code></pre> <ul> <li><code>currentVersion</code>: The firmware currently running on the product type.</li> <li><code>availableVersion</code>: The latest available firmware for that product type.</li> <li><code>status</code>: Whether the device is up to date or needs an upgrade.</li> </ul> <p>This information can be used to automate firmware compliance checks and trigger upgrades as needed.</p>"},{"location":"meraki-nagios-device-sync/#5-comparing-meraki-devices-to-nagios-xi-hosts","title":"5. Comparing Meraki Devices to Nagios XI Hosts","text":"<p>We fetch all hosts from Nagios XI and compare them to the Meraki inventory:</p> <p><pre><code>nagioshost = call_nagios_api('objects/host')\nfor device in gdepdevices:\n    nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n    if len(nagioshostitems) == 0:\n        # Device is missing from Nagios XI\n        # ...add to missing list and prepare for addition...\n</code></pre> - Devices not found in Nagios XI are flagged for addition. - Special handling for device types (appliance, switch, camera, wireless, etc.).</p>"},{"location":"meraki-nagios-device-sync/#6-adding-missing-devices-to-nagios-xi","title":"6. Adding Missing Devices to Nagios XI","text":"<p>For each missing device, we call helper functions to create/update hosts and services in Nagios XI:</p> <p><pre><code>if len(str(device['name']).strip()) != 0:\n    if (str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS):\n        if (str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS):\n            if (device['productType'] == 'appliance' and 'VMX' not in device['model']):\n                applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                if len(vlan999) == 0:\n                    device['lanIp'] = '0.0.0.0'\n                else:\n                    device['lanIp'] = vlan999[0]['applianceIp']\n            if device['lanIp'] is None:\n                device['lanIp'] = '0.0.0.0'\n            create_update_meraki_host(device, nagioshostitems, gdepnetworks, False)\n            nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n            create_update_meraki_host_services(device, nagiosserviceitems, False, merakihostnames)\n</code></pre> - <code>create_update_meraki_host()</code> and <code>create_update_meraki_host_services()</code> are responsible for adding/updating hosts and their services in Nagios XI. - VLAN and IP logic ensures correct addressing for appliances.</p>"},{"location":"meraki-nagios-device-sync/#7-applying-configuration","title":"7. Applying Configuration","text":"<p>After all additions/updates, we apply the Nagios XI configuration:</p> <pre><code>data = {'alias': 'Nagios XI', 'applyconfig': '1'}\ncall_nagios_api('config/host/localhost', method='PUT', data=data)\nnagioshost = call_nagios_api('objects/host')\n</code></pre>"},{"location":"meraki-nagios-device-sync/#8-full-function-code-add_missing_network_device_to_nagios","title":"8. Full Function Code: <code>add_missing_network_device_to_nagios</code>","text":"<p>Below is the complete function, ready to adapt for your own environment:</p> <pre><code>def add_missing_network_device_to_nagios():\n    try:\n        # OID to obtain all host names from Meraki SNMP Dashboard\n        merakihostnames = get_snmp_data(\n            MERAKI_DASHBOARD_SNMP_HOST_NAME, \n            MERAKI_DASHBOARD_SNMP_PORT,\n            '1.3.6.1.4.1.29671.1.1.4.1.2',\n            MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING)\n        gdepOrganizations = dashboard.organizations.getOrganizations()\n        organizationid = gdepOrganizations[0]['id']\n        gdepdevices = dashboard.organizations.getOrganizationDevices(organizationid,-1)\n        gdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid,-1)\n\n        for network in gdepnetworks:\n            networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network['id'])\n            if ('products' in networkupgrades):\n                products = networkupgrades['products']\n                # ...process firmware info as needed...\n\n        nagioshost = call_nagios_api('objects/host')\n        nagioshostservices = call_nagios_api('objects/service')\n        nagioshostconfig = call_nagios_api('config/host')\n        nagioshostgroupmembers = call_nagios_api('objects/hostgroupmembers')\n        nagioshostservicesconfig = call_nagios_api('config/service')\n\n        SKIP_MERAKI_HOSTS = ['tst','tes']\n\n        for device in gdepdevices:\n            if (device['productType'] == 'appliance'):\n                # ...handle appliance types...\n                pass\n            elif (device['productType'] == 'camera'):\n                pass\n            elif (device['productType'] == 'switch'):\n                pass\n            elif (device['productType'] == 'wireless'):\n                pass\n            # ...other device handling as needed...\n            nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n            if (len(nagioshostitems) == 0):\n                if (len(str(device['name']).strip()) != 0):\n                    if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                            # ...add to Nagios XI...\n                            pass\n            if (len(str(device['name']).strip()) != 0):\n                if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                    if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((device['productType'] == 'appliance') and ('VMX' not in device['model'])):\n                            applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                            vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                            if (len(vlan999) == 0):\n                                device['lanIp'] = '0.0.0.0'\n                            else:\n                                device['lanIp'] = vlan999[0]['applianceIp']\n                        if (device['lanIp'] is None):\n                            device['lanIp'] = '0.0.0.0'\n                        create_update_meraki_host(device,nagioshostitems,gdepnetworks,False)\n                        nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n                        create_update_meraki_host_services(device,nagiosserviceitems,False,merakihostnames)\n        data = {'alias': 'Nagios XI', 'applyconfig': '1'}\n        call_nagios_api('config/host/localhost', method='PUT', data=data)\n        nagioshost = call_nagios_api('objects/host')\n    except Exception as exception_obj:\n        print(f\"Error: {exception_obj}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#9-conclusion","title":"9. Conclusion","text":"<p>This workflow ensures your Nagios XI monitoring system is always in sync with your actual Meraki device inventory, with full visibility into firmware status and device types. By automating device discovery, comparison, and configuration, you can maintain a reliable, up-to-date monitoring environment with minimal manual effort.</p>"},{"location":"meraki-nagios-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Python meraki library</li> </ul>"},{"location":"meraki-unused-device-sync/","title":"Identifying Unused Meraki Inventory Devices","text":""},{"location":"meraki-unused-device-sync/#introduction","title":"Introduction","text":"<p>In many enterprise environments, it's important to keep track of network hardware inventory and ensure that all devices are properly assigned and utilized. Unused devices can represent wasted resources or missed opportunities for redeployment. This article demonstrates how to use the Meraki Dashboard API and Python to programmatically identify all Meraki appliances in your organization's inventory that are not currently assigned to any network.</p>"},{"location":"meraki-unused-device-sync/#python-libraries-and-imports","title":"Python Libraries and Imports","text":"<p>The following Python libraries are used in the solution:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library for interacting with Meraki cloud resources.</li> <li>sys: Provides access to system-specific parameters and functions.</li> <li>subprocess: Used for running shell commands from Python (not directly used in the main function, but present for SNMP examples).</li> <li>re: Regular expressions for parsing SNMP output (not used in the main function).</li> <li>gdepcommon.utils: Custom utility module for error handling and secret management.</li> </ul>"},{"location":"meraki-unused-device-sync/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid Meraki API key with read access to your organization's inventory.</li> <li>The <code>meraki</code> Python package installed (<code>pip install meraki</code>).</li> <li>The API key securely stored and retrieved (in this example, from Azure Key Vault via a utility function).</li> </ul>"},{"location":"meraki-unused-device-sync/#the-get_unused_inventory-function","title":"The <code>get_unused_inventory</code> Function","text":"<p>Below is the complete function to retrieve all unused Meraki inventory devices:</p> <pre><code>def get_unused_inventory():\n    \"\"\"\n    Returns a list of unused inventory devices from the first Meraki organization.\n    Unused inventory is defined as devices in inventory that are not assigned to any network.\n    \"\"\"\n    dashboard = meraki.DashboardAPI(\n        api_key=MERAKI_API_KEY,\n        output_log=False,\n        print_console=False,\n        suppress_logging=True\n    )\n    organizations = dashboard.organizations.getOrganizations()\n    if not organizations:\n        return []\n    organization_id = organizations[0]['id']\n    # Get all inventory devices\n    inventory = dashboard.organizations.getOrganizationInventoryDevices(organization_id)\n    # Filter for unused devices (not assigned to any network)\n    unused_devices = [\n        device for device in inventory\n        if not device.get('networkId')\n    ]\n    for device in unused_devices:\n        product_type = device.get('productType')\n        if product_type == 'camera':\n            device['ciproductType'] = 'Camera (' + device['model'] + ')'\n        elif product_type == 'switch':\n            device['ciproductType']  = 'Switch (' + device['model'] + ')'\n        elif product_type == 'appliance':\n            device['ciproductType'] = 'Appliance (' + device['model'] + ')'\n        elif product_type == 'wireless':\n            device['ciproductType'] = 'Wireless (' + device['model'] + ')'\n        else:\n            device['ciproductType'] = 'Unknown' + device['model']\n\n    return unused_devices\n</code></pre>"},{"location":"meraki-unused-device-sync/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Dashboard API Initialization <pre><code>dashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre></li> <li> <p>Initializes the Meraki Dashboard API client using your API key. Logging and console output are suppressed for cleaner operation.</p> </li> <li> <p>Get Organizations <pre><code>organizations = dashboard.organizations.getOrganizations()\nif not organizations:\n    return []\norganization_id = organizations[0]['id']\n</code></pre></p> </li> <li> <p>Retrieves all organizations accessible by the API key. The function uses the first organization found.</p> </li> <li> <p>Get Inventory Devices <pre><code>inventory = dashboard.organizations.getOrganizationInventoryDevices(organization_id)\n</code></pre></p> </li> <li> <p>Fetches all devices in the organization's inventory.</p> </li> <li> <p>Filter for Unused Devices <pre><code>unused_devices = [\n    device for device in inventory\n    if not device.get('networkId')\n]\n</code></pre></p> </li> <li> <p>Filters the inventory for devices that do not have a <code>networkId</code> property, meaning they are not assigned to any network.</p> </li> <li> <p>Label Device Types <pre><code>for device in unused_devices:\n    product_type = device.get('productType')\n    if product_type == 'camera':\n        device['ciproductType'] = 'Camera (' + device['model'] + ')'\n    elif product_type == 'switch':\n        device['ciproductType']  = 'Switch (' + device['model'] + ')'\n    elif product_type == 'appliance':\n        device['ciproductType'] = 'Appliance (' + device['model'] + ')'\n    elif product_type == 'wireless':\n        device['ciproductType'] = 'Wireless (' + device['model'] + ')'\n    else:\n        device['ciproductType'] = 'Unknown' + device['model']\n</code></pre></p> </li> <li> <p>Adds a human-readable label to each unused device based on its type and model.</p> </li> <li> <p>Return the List <pre><code>return unused_devices\n</code></pre></p> </li> <li>Returns the list of unused devices, each with additional labeling for easier reporting or further processing.</li> </ol>"},{"location":"meraki-unused-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Meraki Python Library on PyPI</li> <li>Official Meraki Python SDK GitHub</li> </ul>"},{"location":"meraki-unused-device-sync/#conclusion","title":"Conclusion","text":"<p>By using the Meraki Dashboard API and Python, you can quickly identify unused inventory devices in your organization. This enables better asset management, cost savings, and improved operational efficiency. The approach can be extended to automate device assignment, generate reports, or integrate with other IT asset management systems.</p>"},{"location":"meraki-vm-deployment/","title":"Deploying Cisco Meraki vMX with BICEP","text":"<p>This technical article walks you through deploying a Cisco Meraki vMX virtual appliance in Azure using a Bicep template. The vMX is commonly used for SD-WAN and secure connectivity between Azure and on-premises or branch locations. This guide explains the Bicep code, required parameters, and best practices for secure deployment.</p>"},{"location":"meraki-vm-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Meraki Dashboard Authentication Token: Obtain your Meraki authentication string from the Meraki Dashboard. Do not hardcode this in your template or documentation.</li> <li>Azure Subscription: Ensure you have sufficient permissions to deploy marketplace resources and managed applications.</li> <li>Resource Groups: Identify or create the resource groups for your vMX and virtual network.</li> </ul>"},{"location":"meraki-vm-deployment/#bicep-template-merakibicep","title":"Bicep Template: <code>meraki.bicep</code>","text":"<p>Below is the Bicep template for deploying the Cisco Meraki vMX. Sensitive values (such as the authentication token) are not included and should be provided securely at deployment time.</p> <pre><code>@description('Deployment location')\nparam location string = 'westus'\n\n@description('This is the name of your VM')\n@metadata({ title: 'VM Name' })\nparam vmName string = 'DRAZGDEPMEDGE01'\n\n@description('This is your authentication string generated by Meraki Dashboard')\nparam merakiAuthToken string // Provide securely at deployment time\n\n@description('Availability zone number for the vMX')\n@allowed([\n  '0'\n  '1'\n  '2'\n  '3'\n])\nparam zone string = '0'\n\n@description('New or Existing VNet Name')\nparam virtualNetworkName string = 'vnet-gdep-pwus-fortinet'\n\n@description('Boolean indicating whether the VNet is new or existing')\nparam virtualNetworkNewOrExisting string = 'existing'\n\n@description('VNet address prefix')\nparam virtualNetworkAddressPrefix string = '10.27.1.0/24'\n\n@description('Resource group of the VNet')\nparam virtualNetworkResourceGroup string = 'dr-rg-gdep-pwus-vnets'\n\n@description('The size of the VM')\nparam virtualMachineSize string = 'Standard_F4s_v2'\n\n@description('New or Existing subnet Name')\nparam subnetName string = 'snet-gdep-pwus-sdwan-public-new'\n\n@description('Subnet address prefix')\nparam subnetAddressPrefix string = '10.27.35.0/24'\nparam applicationResourceName string = 'DRCiscoMeraki'\nparam managedResourceGroupId string = '/subscriptions/&lt;your-subscription-id&gt;/resourceGroups/&lt;your-managed-rg&gt;'\n\nparam managedIdentity object = { type: 'SystemAssigned' }\n\nresource applicationResource 'Microsoft.Solutions/applications@2021-07-01' = {\n  name: applicationResourceName\n  location: location\n  kind: 'MarketPlace'\n  identity: managedIdentity\n  plan: {\n    name: 'cisco-meraki-vmx'\n    product: 'cisco-meraki-vmx'\n    publisher: 'cisco'\n    version: '15.37.4'\n  }\n  properties: {\n    managedResourceGroupId: managedResourceGroupId\n    parameters: {\n      location: {\n        value: location\n      }\n      merakiAuthToken: {\n        value: merakiAuthToken\n      }\n      subnetAddressPrefix: {\n        value: subnetAddressPrefix\n      }\n      subnetName: {\n        value: subnetName\n      }\n      virtualMachineSize: {\n        value: virtualMachineSize\n      }\n      virtualNetworkAddressPrefix: {\n        value: virtualNetworkAddressPrefix\n      }\n      virtualNetworkName: {\n        value: virtualNetworkName\n      }\n      virtualNetworkNewOrExisting: {\n        value: virtualNetworkNewOrExisting\n      }\n      virtualNetworkResourceGroup: {\n        value: virtualNetworkResourceGroup\n      }\n      vmName: {\n        value: vmName\n      }\n      zone: {\n        value: zone\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"meraki-vm-deployment/#parameter-explanations","title":"Parameter Explanations","text":"<ul> <li>location: Azure region for deployment (e.g., <code>westus</code>).</li> <li>vmName: Name for the Meraki vMX VM.</li> <li>merakiAuthToken: Meraki Dashboard authentication string. Provide this securely at deployment time (e.g., via parameter file or secret).</li> <li>zone: Availability zone for the vMX (0-3).</li> <li>virtualNetworkName: Name of the VNet to deploy into.</li> <li>virtualNetworkNewOrExisting: Specify if the VNet is new or existing.</li> <li>virtualNetworkAddressPrefix: Address prefix for the VNet.</li> <li>virtualNetworkResourceGroup: Resource group containing the VNet.</li> <li>virtualMachineSize: Azure VM size for the vMX.</li> <li>subnetName: Name of the subnet for the vMX.</li> <li>subnetAddressPrefix: Address prefix for the subnet.</li> <li>applicationResourceName: Name for the managed application resource.</li> <li>managedResourceGroupId: Resource ID of the managed resource group for the application (update with your values).</li> <li>managedIdentity: System-assigned managed identity for the deployment.</li> </ul>"},{"location":"meraki-vm-deployment/#how-the-bicep-template-works","title":"How the Bicep Template Works","text":"<ul> <li>Marketplace Deployment: Uses the <code>Microsoft.Solutions/applications</code> resource to deploy the Cisco Meraki vMX from the Azure Marketplace.</li> <li>Parameterization: All key settings (network, VM size, zone, etc.) are parameterized for flexibility.</li> <li>Security: The Meraki authentication token is never hardcoded\u2014always provide it securely.</li> <li>Managed Identity: Uses a system-assigned managed identity for secure resource access.</li> </ul>"},{"location":"meraki-vm-deployment/#best-practices","title":"Best Practices","text":"<ul> <li>Never commit sensitive tokens to source control. Use parameter files, Azure Key Vault, or pipeline secrets.</li> <li>Review and update the <code>managedResourceGroupId</code> and other resource IDs for your environment.</li> <li>Monitor deployment via Azure Portal or CLI for success and troubleshooting.</li> </ul>"},{"location":"meraki-vm-deployment/#summary","title":"Summary","text":"<p>This Bicep template enables automated, secure deployment of Cisco Meraki vMX in Azure. By parameterizing all key settings and handling secrets securely, you can quickly integrate Meraki SD-WAN into your Azure landing zone.</p> <p>Ready to deploy? Use this template with your own parameters and secrets for a secure, repeatable deployment.</p>"},{"location":"monitoring-itsm-integration/","title":"Integrating Monitoring System with ITSM System","text":"<p>This technical article provides a comprehensive, vendor-neutral guide to integrating a monitoring system (Nagios) with an ITSM system (ServiceDesk Plus by ManageEngine) using Python. The solution demonstrates how to automatically open and close ITSM tickets based on monitoring events, with all relevant code and in-line explanations. </p>"},{"location":"monitoring-itsm-integration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Non-Standard Python Libraries Used</li> <li>Connecting to Nagios and ServiceDesk Plus APIs</li> <li>Core Integration Workflow<ul> <li>open_and_close_incidents Function</li> <li>closeincidents Function</li> <li>openincidents Function</li> </ul> </li> <li>Supporting Functions</li> <li>Conclusion</li> </ol>"},{"location":"monitoring-itsm-integration/#overview","title":"Overview","text":"<p>Automating the integration between a monitoring system (such as Nagios) and an ITSM tool (such as ServiceDesk Plus) enables organizations to streamline incident management. This integration ensures that alerts from the monitoring system automatically generate, update, or close tickets in the ITSM tool, reducing manual effort and improving response times.</p> <p>This article provides a step-by-step guide, including all relevant Python code, to: - Connect to both Nagios and ServiceDesk Plus via their APIs - Open tickets in the ITSM tool when monitoring events occur - Close tickets in the ITSM tool when issues are resolved in the monitoring system</p>"},{"location":"monitoring-itsm-integration/#non-standard-python-libraries-used","title":"Non-Standard Python Libraries Used","text":"<p>The following non-standard Python libraries are used in this solution:</p> <ul> <li><code>requests</code>: For making HTTP requests to Nagios and ServiceDesk Plus APIs</li> <li><code>datetime</code>, <code>calendar</code>: For date and time manipulations</li> <li><code>csv</code>: For exporting data to CSV (if needed)</li> </ul> <p>Install these libraries using pip if not already available:</p> <pre><code>pip install requests\n</code></pre>"},{"location":"monitoring-itsm-integration/#connecting-to-nagios-and-servicedesk-plus-apis","title":"Connecting to Nagios and ServiceDesk Plus APIs","text":"<p>To interact with both systems, you need API endpoints and credentials. Below are example constants (replace with your own values):</p> <pre><code># Constants for ServiceDesk Plus (ITSM)\nSERVICE_DESK_API_KEY = 'YOUR_SERVICEDESKPLUS_API_KEY'\nSERVICEDESK_BASE_URL = 'https://your-servicedeskplus-instance/api/v3/'\nSERVICE_DESK_USER = 'automation_user'  # The user that creates tickets via API\n\n# Constants for Nagios\nNAGIOS_API_KEY = 'YOUR_NAGIOS_API_KEY'\nNAGIOSXI_BASE_URL = 'https://your-nagios-instance/nagiosxi/api/v1/'\nNAGIOS_COMMENT_FORMAT = 'Automated Ticket Number Added By Interface Engine '\n</code></pre>"},{"location":"monitoring-itsm-integration/#core-integration-workflow","title":"Core Integration Workflow","text":"<p>The main workflow is orchestrated by the <code>open_and_close_incidents</code> function, which: - Retrieves the current Nagios host inventory from a database - Fetches the current state of monitored hosts and services from Nagios - Closes incidents in the ITSM tool if the corresponding monitoring issue is resolved - Opens new incidents in the ITSM tool for new monitoring issues</p>"},{"location":"monitoring-itsm-integration/#open_and_close_incidents-function","title":"open_and_close_incidents Function","text":"<pre><code>def open_and_close_incidents():\n    try:\n        # Retrieve Nagios host inventory from a database\n        nagioshostinventory = execute_sql_fetch_dicts('select lower(host_name) as host_name, notes from fact_nagios_hosts ')\n        # Fetch current Nagios host group members via API\n        nagioshostgroupmembers = get_data_from_Nagios(None, 'objects/hostgroupmembers', 'hostgroup')\n\n        # Close incidents in ITSM tool for resolved monitoring issues\n        hostandservicesjustclosed = closeincidents(nagioshostgroupmembers)\n        # Open new incidents in ITSM tool for new monitoring issues\n        openincidents(nagioshostgroupmembers, nagioshostinventory, hostandservicesjustclosed)\n\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation","title":"Explanation","text":"<ul> <li>execute_sql_fetch_dicts: Retrieves the current Nagios host inventory from a database table.</li> <li>get_data_from_Nagios: Calls the Nagios API to get host group membership information.</li> <li>closeincidents: Handles closing tickets in the ITSM tool for issues that have been resolved in Nagios.</li> <li>openincidents: Handles opening new tickets in the ITSM tool for new issues detected by Nagios.</li> </ul>"},{"location":"monitoring-itsm-integration/#closeincidents-function","title":"closeincidents Function","text":"<p>This function closes tickets in the ITSM tool when the corresponding monitoring issue is resolved in Nagios.</p> <pre><code>def closeincidents(nagioshostgroupmembers):\n    try:\n        # Calculate timestamp for incidents created in the last 10 days\n        twoweeksago = datetime.datetime.now() - datetime.timedelta(days=10)\n        twoweeksago = str(calendar.timegm(twoweeksago.timetuple())) + '000'\n        # Search for resolved/closed/cancelled tickets created by the automation user\n        searchcrteria = [\n            {\"field\": \"status.name\", \"condition\": \"is \", \"values\": [\"Resolved\", \"Closed\", \"Cancelled\"]},\n            {\"field\": \"created_by.name\", \"condition\": \"is\", \"logical_operator\": \"and\", \"value\": str(SERVICE_DESK_USER)},\n            {\"field\": \"created_time\", \"condition\": \"greater than\", \"value\": twoweeksago, \"logical_operator\": \"and\"}\n        ]\n        servicedeskincidents = get_all_Service_Desk_Requests('requests', 'requests', searchcrteria, SERVICE_DESK_API_KEY, SERVICEDESK_BASE_URL)\n        nagiosallcurrentcomments = get_data_from_Nagios(None, 'objects/comment', 'comment')\n        for eachNagiosComment in nagiosallcurrentcomments:\n            commentData = str(eachNagiosComment['comment_data'])\n            if NAGIOS_COMMENT_FORMAT in commentData:\n                nagiosticketnumber = commentData[(len(NAGIOS_COMMENT_FORMAT) - len(commentData)):]\n                deleteNagiosAcknowledgement = False\n                for eachiSightRequest in servicedeskincidents:\n                    if eachiSightRequest['id'] == nagiosticketnumber:\n                        deleteNagiosAcknowledgement = True\n                        break\n                if deleteNagiosAcknowledgement:\n                    delete_nagios_acknowledgement(eachNagiosComment['host_name'], eachNagiosComment['service_description'])\n        # Close requests in ITSM tool if not present in Nagios comments\n        for eachiSightRequest in servicedeskincidents:\n            hname, sname, onerowaffected = close_Request_if_ticket_not_in_comments(eachiSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers)\n            if onerowaffected and hname is not None:\n                hostandservicesjustclosed.append({\"hname\": hname, \"sname\": sname})\n        # Repeat for open tickets\n        searchcrteria = [\n            {\"field\": \"status.name\", \"condition\": \"is not\", \"values\": [\"Resolved\", \"Closed\", \"Cancelled\"]},\n            {\"field\": \"created_by.name\", \"condition\": \"is\", \"logical_operator\": \"and\", \"value\": str(SERVICE_DESK_USER)}\n        ]\n        servicedeskincidents = get_all_Service_Desk_Requests('requests', 'requests', searchcrteria, SERVICE_DESK_API_KEY, SERVICEDESK_BASE_URL)\n        nagiosallcurrentcomments = get_data_from_Nagios(None, 'objects/comment', 'comment')\n        for eachiSightRequest in servicedeskincidents:\n            hname, sname, onerowaffected = close_Request_if_ticket_not_in_comments(eachiSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers)\n            if onerowaffected and hname is not None:\n                hostandservicesjustclosed.append({\"hname\": hname, \"sname\": sname})\n        return hostandservicesjustclosed\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation_1","title":"Explanation","text":"<ul> <li>get_all_Service_Desk_Requests: Queries the ITSM tool for tickets matching certain criteria.</li> <li>get_data_from_Nagios: Retrieves current comments (acknowledgements) from Nagios.</li> <li>delete_nagios_acknowledgement: Removes acknowledgement in Nagios if the corresponding ticket is resolved.</li> <li>close_Request_if_ticket_not_in_comments: Closes the ITSM ticket if it is no longer present in Nagios comments.</li> </ul>"},{"location":"monitoring-itsm-integration/#openincidents-function","title":"openincidents Function","text":"<p>This function opens new tickets in the ITSM tool for new monitoring issues detected by Nagios.</p> <pre><code>def openincidents(nagioshostgroupmembers, nagioshostinventory, hostandservicesjustclosed):\n    try:\n        # ...implementation to open new tickets in ITSM tool based on Nagios alerts...\n        # Typically, this involves:\n        # 1. Fetching current Nagios issues (hosts/services in warning/critical state)\n        # 2. Checking if a ticket already exists for the issue\n        # 3. If not, creating a new ticket in the ITSM tool via API\n        # 4. Adding a comment/acknowledgement in Nagios with the ticket number\n        pass\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation_2","title":"Explanation","text":"<ul> <li>openincidents is responsible for creating new tickets in the ITSM tool for issues detected by Nagios that do not already have an open ticket.</li> <li>The function typically fetches current issues from Nagios, checks for existing tickets, and creates new ones as needed.</li> </ul>"},{"location":"monitoring-itsm-integration/#supporting-functions","title":"Supporting Functions","text":"<p>Below are key supporting functions referenced in the workflow. These functions handle API calls, database queries, and other integration logic.</p>"},{"location":"monitoring-itsm-integration/#execute_sql_fetch_dicts","title":"execute_sql_fetch_dicts","text":"<pre><code>def execute_sql_fetch_dicts(sqlstatement):\n    \"\"\"\n    Executes a SQL query and returns the results as a list of dictionaries.\n    Implementation depends on your database setup. Example below uses pyodbc.\n    \"\"\"\n    import pyodbc\n    results = []\n    try:\n        # Replace with your actual connection string\n        conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=your_server;DATABASE=your_db;UID=your_user;PWD=your_password')\n        cursor = conn.cursor()\n        cursor.execute(sqlstatement)\n        columns = [column[0] for column in cursor.description]\n        for row in cursor.fetchall():\n            results.append(dict(zip(columns, row)))\n    except Exception as e:\n        print(f\"SQL execution error: {e}\")\n    finally:\n        try:\n            conn.close()\n        except:\n            pass\n    return results\n</code></pre>"},{"location":"monitoring-itsm-integration/#get_data_from_nagios","title":"get_data_from_Nagios","text":"<pre><code>import requests\nimport json\n\ndef get_data_from_Nagios(queryParms, object2Query, responseField):\n    \"\"\"\n    Calls the Nagios API and returns the requested data.\n    \"\"\"\n    try:\n        params = {'apikey': NAGIOS_API_KEY}\n        if queryParms is not None:\n            params.update(queryParms)\n        response = requests.get(NAGIOSXI_BASE_URL + object2Query, params=params, verify=False)\n        response_content = response.text\n        json_data = json.loads(response_content)\n        if responseField is None:\n            return json_data\n        else:\n            return json_data[responseField]\n    except Exception as e:\n        print(f\"Nagios API error: {e}\")\n        return None\n</code></pre>"},{"location":"monitoring-itsm-integration/#get_all_service_desk_requests","title":"get_all_Service_Desk_Requests","text":"<pre><code>import requests\nimport json\n\ndef get_all_Service_Desk_Requests(object2Query, responseField, searchcriteria, apikey, apibaseurl):\n    \"\"\"\n    Calls the ServiceDesk Plus API to retrieve tickets matching the search criteria.\n    \"\"\"\n    try:\n        headers = {'Authtoken': apikey}\n        url = f\"{apibaseurl}{object2Query}\"\n        params = {\"input_data\": json.dumps({\"criteria\": searchcriteria})}\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        return response.json().get(responseField, [])\n    except Exception as e:\n        print(f\"ServiceDesk API error: {e}\")\n        return []\n</code></pre>"},{"location":"monitoring-itsm-integration/#delete_nagios_acknowledgement","title":"delete_nagios_acknowledgement","text":"<pre><code>import requests\n\ndef delete_nagios_acknowledgement(hostname, servicedescription):\n    \"\"\"\n    Removes an acknowledgement (comment) from Nagios for the given host/service.\n    \"\"\"\n    try:\n        if not servicedescription:\n            nagiosCommand = f'cmd=REMOVE_HOST_ACKNOWLEDGEMENT;{hostname}'\n        else:\n            nagiosCommand = f'cmd=REMOVE_SVC_ACKNOWLEDGEMENT;{hostname};{servicedescription}'\n        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n        OBJECT_URL = 'system/corecommand'\n        response = requests.post(\n            NAGIOSXI_BASE_URL + OBJECT_URL,\n            params={'apikey': NAGIOS_API_KEY},\n            headers=headers,\n            data=nagiosCommand,\n            verify=False\n        )\n        return response.status_code == 200\n    except Exception as e:\n        print(f\"Nagios acknowledgement deletion error: {e}\")\n        return False\n</code></pre>"},{"location":"monitoring-itsm-integration/#close_request_if_ticket_not_in_comments","title":"close_Request_if_ticket_not_in_comments","text":"<pre><code>def close_Request_if_ticket_not_in_comments(iSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers):\n    \"\"\"\n    Closes the ITSM ticket if it is no longer present in Nagios comments.\n    Returns (hostname, servicename, rowaffected)\n    \"\"\"\n    lookforRequestNumber = str(iSightRequest['id'])\n    closerequest = True\n    hostname = None\n    servicename = None\n    onerowaffected = False\n    for eachNagiosComment in nagiosallcurrentcomments:\n        commentData = str(eachNagiosComment['comment_data'])\n        if lookforRequestNumber in commentData:\n            closerequest = False\n            break\n    if closerequest:\n        # Here you would call the ITSM API to close the ticket\n        # For demonstration, we just print and return\n        print(f\"Closing ITSM ticket {lookforRequestNumber} as it is not present in Nagios comments.\")\n        onerowaffected = True\n        # Optionally, update the ticket status via API here\n    return hostname, servicename, onerowaffected\n</code></pre>"},{"location":"monitoring-itsm-integration/#handle_global_exception","title":"handle_global_exception","text":"<pre><code>def handle_global_exception(functionName, exceptionObject):\n    \"\"\"\n    Handles exceptions and sends notification emails if needed.\n    \"\"\"\n    import traceback\n    print(f\"Exception in {functionName}: {exceptionObject}\")\n    print(traceback.format_exc())\n    # Optionally, send an email notification here\n    # send_email(recipients=[...], subject='Exception occurred', plain_message=str(exceptionObject))\n</code></pre>"},{"location":"monitoring-itsm-integration/#conclusion","title":"Conclusion","text":"<p>By following the approach and code provided in this article, you can automate the integration between your monitoring system (Nagios) and ITSM tool (ServiceDesk Plus or similar). This enables automatic ticket creation and closure based on real-time monitoring events, improving incident response and reducing manual workload.</p> <p>Adapt the code and API calls as needed for your specific environment and ITSM/monitoring platforms.</p>"},{"location":"proofpoint-user-management/","title":"Automating User Management in Proofpoint Essentials","text":""},{"location":"proofpoint-user-management/#introduction","title":"Introduction","text":"<p>Proofpoint Essentials provides robust APIs for managing users and optimizing licensing. Marking certain users as \"functional accounts\" (such as service, shared, or terminated accounts) can help reduce licensing costs and improve compliance. This article demonstrates how to:</p> <ul> <li>Connect to the Proofpoint Essentials API using an API user and key</li> <li>Retrieve active users from Proofpoint</li> <li>Compare users to your HR system (e.g., ADP or any HRIS)</li> <li>Mark users as functional accounts via API</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"proofpoint-user-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Proofpoint Essentials administrator access</li> <li>An API user and API key (see below)</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Access to your HR system data (e.g., via SQL, API, or CSV)</li> </ul>"},{"location":"proofpoint-user-management/#how-to-create-an-api-key-in-proofpoint-essentials","title":"How to Create an API Key in Proofpoint Essentials","text":"<ol> <li>Log in to the Proofpoint Essentials admin portal.</li> <li>Navigate to Account Management &gt; API Keys.</li> <li>Click Create API Key.</li> <li>Assign the key to a dedicated API user with appropriate permissions.</li> <li>Save the API key securely (e.g., in Azure Key Vault or a secrets manager).</li> </ol> <p>For more details, refer to the official Proofpoint Essentials API documentation. </p>"},{"location":"proofpoint-user-management/#step-1-connect-to-the-proofpoint-essentials-api","title":"Step 1: Connect to the Proofpoint Essentials API","text":"<pre><code>import requests\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\nPROOFPOINT_BASE_API = 'https://&lt;your-region&gt;.proofpointessentials.com/api/v1/'\nPROOFPOINT_API_USER = get_azure_kv_sceret('pp-api-user')\nPROOFPOINT_API_PASSWORD = get_azure_kv_sceret('pp-api-key')\n\n# Example: Get all users in your organization\nurl = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\nresponse = requests.get(\n    url=url,\n    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n)\nresponse.raise_for_status()\nusers = response.json().get('users', [])\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The API call retrieves all users for your organization.</p>"},{"location":"proofpoint-user-management/#step-2-retrieve-user-data-from-your-hr-system","title":"Step 2: Retrieve User Data from Your HR System","text":"<p>Assume you have a function to get user data from your HR system (e.g., via SQL):</p> <pre><code>def get_latest_hr_data():\n    sql_statement = \"\"\"\n        SELECT status, email, company_code, worker_category_code, location_code\n        FROM hr_employees WHERE email IS NOT NULL\n    \"\"\"\n    return execute_sql_fetch_dicts(sql_statement)\n</code></pre>"},{"location":"proofpoint-user-management/#step-3-compare-and-mark-users-as-functional-accounts","title":"Step 3: Compare and Mark Users as Functional Accounts","text":"<p>The following function compares Proofpoint users to your HR data and marks users as functional accounts via the API:</p> <pre><code>def mark_users_as_functional(hr_users):\n    try:\n        # Get active users from Proofpoint\n        url_to_invoke = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\n        response = requests.get(\n            url=url_to_invoke,\n            headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n        )\n        response.raise_for_status()\n        active_users_from_pp = response.json().get('users', [])\n\n        hr_emails = {user['email'].lower(): user for user in hr_users}\n        users_to_mark_functional = []\n\n        for user in active_users_from_pp:\n            if user.get('type') == 'end_user':\n                email = user['primary_email'].lower()\n                hr_user = hr_emails.get(email)\n                # Example logic: mark as functional if not in HR or if terminated\n                if not hr_user or hr_user['status'].lower() == 'terminated':\n                    users_to_mark_functional.append(user)\n\n        for user in users_to_mark_functional:\n            email_to_lookup = user['primary_email']\n            url_update = PROOFPOINT_BASE_API + f'orgs/&lt;your-domain&gt;/users/{email_to_lookup}'\n            json_update_values = {\n                \"uid\": user['uid'],\n                \"primary_email\": email_to_lookup,\n                \"is_active\": True,\n                \"type\": \"functional_account\"\n            }\n            try:\n                requests.put(\n                    url_update,\n                    json=json_update_values,\n                    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD}\n                ).raise_for_status()\n            except requests.RequestException:\n                pass\n        return True\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n</code></pre> <p>Explanation: - Retrieves all active users from Proofpoint. - Compares each user to the HR system. - Marks users as functional accounts if they are not in HR or are terminated. - Updates are made via the Proofpoint API.</p>"},{"location":"proofpoint-user-management/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    hr_data = get_latest_hr_data()\n    mark_users_as_functional(hr_data)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"proofpoint-user-management/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the process of marking functional accounts in Proofpoint Essentials, optimizing your licensing and compliance posture. The approach is secure, repeatable, and adaptable to any enterprise environment.</p> <p>For more details, consult the Proofpoint Essentials API documentation.</p>"},{"location":"proofpointo365connector/","title":"Understanding SPF, DKIM, and DMARC in O365 with Proofpoint: Preventing Spoofing and Bypass Attacks","text":""},{"location":"proofpointo365connector/#introduction","title":"Introduction","text":"<p>Email security is a critical aspect of any organization's IT infrastructure. This article explains the basics of SPF, DKIM, and DMARC, and details a real-world scenario involving Office 365 (O365) and Proofpoint, where attackers were able to bypass protections and how to mitigate such risks.</p> <p>After implementing traditional quarantine approaches, we discovered significant operational challenges that led us to adopt a more sophisticated redirect-based solution that maintains business continuity while providing superior security protection.</p>"},{"location":"proofpointo365connector/#1-email-flow-architecture","title":"1. Email Flow Architecture","text":"<ul> <li>Internal Emails: Sent from one user to another within the organization, these do not traverse Proofpoint.</li> <li>Outbound External Emails: Sent from O365 to Proofpoint, then to external recipients.</li> <li>Inbound External Emails: Received by Proofpoint, then relayed to O365, and finally delivered to users.</li> </ul> <p>Note: Even after configuring MX records to point only to Proofpoint (e.g., <code>mx1-us1.ppe-hosted.com</code> with IPs <code>148.163.129.50</code> and <code>67.231.154.162</code>), attackers may still find ways to bypass these controls.</p>"},{"location":"proofpointo365connector/#the-bypass-problem-explained","title":"The Bypass Problem Explained","text":"<p>Attackers can research your organization's <code>.onmicrosoft.com</code> domain and send emails directly to Microsoft's EOP servers, completely bypassing your ProofPoint security stack. This happens because:</p> <ol> <li>Discovery of .onmicrosoft.com domains: Attackers use tools like MXtoolbox to find your <code>companyname.onmicrosoft.com</code> domain</li> <li>Direct delivery to Microsoft: They send emails directly to <code>mail.protection.outlook.com</code> servers</li> <li>Bypass your MX records: Since they're not using normal MX record lookup, your ProofPoint gateway never sees these emails</li> </ol>"},{"location":"proofpointo365connector/#2-spf-dkim-and-dmarc-explained","title":"2. SPF, DKIM, and DMARC Explained","text":""},{"location":"proofpointo365connector/#spf-sender-policy-framework","title":"SPF (Sender Policy Framework)","text":"<ul> <li>Purpose: Specifies which mail servers are authorized to send email for your domain.</li> <li>How to Check: <pre><code>dig +short TXT munishsethi.com\ndig +short TXT _spf.munishsethi.com\n</code></pre></li> <li>Example Record: <pre><code>v=spf1 include:spf.protection.outlook.com include:spf.proofpoint.com -all\n</code></pre></li> </ul>"},{"location":"proofpointo365connector/#dkim-domainkeys-identified-mail","title":"DKIM (DomainKeys Identified Mail)","text":"<ul> <li>Purpose: Uses cryptographic signatures to verify that an email was sent by an authorized server and was not altered.</li> <li>How to Check: <pre><code>dig +short TXT selector1._domainkey.munishsethi.com\n</code></pre></li> <li>Example Record: <pre><code>v=DKIM1; k=rsa; p=...public-key...\n</code></pre></li> </ul>"},{"location":"proofpointo365connector/#dmarc-domain-based-message-authentication-reporting-and-conformance","title":"DMARC (Domain-based Message Authentication, Reporting, and Conformance)","text":"<ul> <li>Purpose: Tells receiving servers what to do if SPF or DKIM checks fail.</li> <li>How to Check: <pre><code>dig +short TXT _dmarc.munishsethi.com\n</code></pre></li> <li>Example Record: <pre><code>v=DMARC1; p=quarantine; rua=mailto:dmarc-reports@munishsethi.com\n</code></pre></li> <li>Policy Options:</li> <li><code>none</code>: Take no action, just report.</li> <li><code>quarantine</code>: Treat as suspicious (e.g., send to spam/quarantine).</li> <li><code>reject</code>: Reject the message outright.</li> </ul>"},{"location":"proofpointo365connector/#3-the-problem-spoofing-despite-dmarc-quarantine","title":"3. The Problem: Spoofing Despite DMARC Quarantine","text":"<p>Even after setting the MX records to Proofpoint and configuring DMARC with <code>p=quarantine</code>, spoofed emails were still being delivered. Investigation revealed that attackers were sending emails directly to the Microsoft EOP (Exchange Online Protection) endpoint (e.g., <code>SJ1PEPF000023D6.mail.protection.outlook.com</code>), bypassing the MX record lookup. Microsoft EOP sometimes relayed these messages to users if the spam score (SCL) was low, even if DMARC failed.</p>"},{"location":"proofpointo365connector/#why-this-happens","title":"Why This Happens","text":"<ul> <li>Microsoft EOP may not always quarantine emails with failed DMARC if the SCL is low.</li> <li>Attackers can send directly to O365 endpoints, bypassing Proofpoint.</li> </ul>"},{"location":"proofpointo365connector/#real-world-example-teams-meeting-forward-issue","title":"Real-World Example: Teams Meeting Forward Issue","text":"<p>During our implementation, we encountered a specific scenario that highlighted the complexity of modern email security:</p> <p>The Scenario: A user received a Teams calendar meeting invite from an external sender (<code>rmay@dmainc.com</code>). When this user forwarded the meeting to internal colleagues, the forwarded meeting was being quarantined by our security rules.</p> <p>The Problem: The forwarded email had: - <code>From: \"May, Rico\" &lt;rmay@dmainc.com&gt;</code> (external domain) - <code>Sender: Manimaran Chokkappa &lt;Manimaran.Chokkappa.ext@yourcompany.com&gt;</code> (internal) - <code>X-MS-Exchange-MeetingForward-Message: Forward</code> (indicating internal forward)</p> <p>This legitimate internal forward was being treated as an external threat because Exchange preserves the original \"From\" address while adding internal sender information.</p>"},{"location":"proofpointo365connector/#4-evolution-from-quarantine-to-redirect-approach","title":"4. Evolution from Quarantine to Redirect Approach","text":""},{"location":"proofpointo365connector/#initial-quarantine-approach-problematic","title":"Initial Quarantine Approach (Problematic)","text":"<p>Our first attempt used a broad quarantine rule:</p> <pre><code>New-TransportRule -Name \"Quarantine external spoof bypass Proofpoint\" `\n  -FromScope NotInOrganization `\n  -SetAuditSeverity High `\n  -Quarantine $true `\n  -ExceptIfSenderDomainIs @(\"skype.voicemail.microsoft.com\", \"microsoft.com\") `\n  -ExceptIfHeaderMatchesMessageHeader \"X-PPE-TRUSTED\" -ExceptIfHeaderMatchesPatterns \"[\\s\\S]*\" `\n  -ExceptIfHeaderContainsMessageHeader \"X-MS-Exchange-MeetingForward-Message\" -ExceptIfHeaderContainsWords \"Forward\"\n</code></pre>"},{"location":"proofpointo365connector/#problems-with-the-quarantine-approach","title":"Problems with the Quarantine Approach","text":"<ol> <li>Overly Broad: Quarantined ALL external email except specific exceptions</li> <li>Operational Impact: </li> <li>Customer communications blocked</li> <li>Partner emails quarantined</li> <li>Legitimate business correspondence interrupted</li> <li>Meeting forwards and internal operations affected</li> <li>Administrative Overhead: IT overwhelmed with quarantine release requests</li> <li>Business Disruption: Users couldn't receive important external communications</li> </ol>"},{"location":"proofpointo365connector/#the-better-solution-redirect-approach","title":"The Better Solution: Redirect Approach","text":"<p>After experiencing these operational challenges, we developed a redirect-based solution that:</p> <ul> <li>Maintains mail flow while ensuring security</li> <li>Forces suspicious emails back through the proper security stack</li> <li>Reduces false positives significantly</li> <li>Preserves business continuity</li> </ul>"},{"location":"proofpointo365connector/#5-allowing-legitimate-internal-applications","title":"5. Allowing Legitimate Internal Applications","text":"<p>Some internal applications need to send emails (e.g., alerts, notifications) to both internal and external recipients. These are sent from trusted IP addresses and should be allowed.</p>"},{"location":"proofpointo365connector/#step-by-step-creating-a-connector-in-o365","title":"Step-by-Step: Creating a Connector in O365","text":"<ol> <li>Log in to the Microsoft 365 Admin Center</li> <li>Go to https://admin.exchange.microsoft.com</li> <li> <p>Use an account with Exchange admin permissions.</p> </li> <li> <p>Navigate to Mail Flow &gt; Connectors</p> </li> <li> <p>In the left pane, select Mail flow &gt; Connectors.</p> </li> <li> <p>Create a New Connector</p> </li> <li>Click Add a connector.</li> <li>From: Your organization's email server</li> <li>To: Office 365</li> <li> <p>Click Next.</p> </li> <li> <p>Name and Describe the Connector</p> </li> <li>Give the connector a meaningful name (e.g., \"Internal App Relay\").</li> <li> <p>Add a description for future reference.</p> </li> <li> <p>Specify Trusted IP Addresses</p> </li> <li>Select \"By verifying that the IP address of the sending server matches one of the following IP addresses...\"</li> <li> <p>Enter the public IP addresses of your trusted internal applications/servers.</p> </li> <li> <p>Configure Security Restrictions</p> </li> <li> <p>Choose whether to require TLS, certificate, or other restrictions as needed.</p> </li> <li> <p>Review and Create</p> </li> <li> <p>Review your settings and click Create connector.</p> </li> <li> <p>Test the Connector</p> </li> <li>Send a test email from your internal application to an internal and external recipient.</li> <li>Check message headers in the recipient's mailbox to confirm successful delivery and correct routing.</li> </ol> <p>Tip: Use the \"Message Trace\" feature in Exchange Admin Center to troubleshoot delivery issues.</p>"},{"location":"proofpointo365connector/#6-the-complete-redirect-based-security-solution","title":"6. The Complete Redirect-Based Security Solution","text":""},{"location":"proofpointo365connector/#overview","title":"Overview","text":"<p>Our final solution consists of three coordinated components:</p> <ol> <li>ProofPoint SCL Rule (Highest Priority - Keep existing)</li> <li>Azure/Application Server Allow Rule (Priority 2)</li> <li>Bypass Redirect Rule (Priority 3)</li> </ol>"},{"location":"proofpointo365connector/#step-1-preserve-existing-proofpoint-scl-rule","title":"Step 1: Preserve Existing ProofPoint SCL Rule","text":"<p>Keep your existing highest priority rule that your ProofPoint partner created:</p> <pre><code>Rule: ProofPoint SCL Trust Rule\nPriority: 1 (Highest)\nConditions: sender ip addresses belong to ProofPoint ranges\nActions: Set the spam confidence level (SCL) to '-1'\n</code></pre> <p>Why this is critical: - SCL -1 = \"Trusted sender\" bypasses all EOP spam filtering - Prevents EOP from re-scanning mail already processed by ProofPoint - Ensures clean mail from ProofPoint isn't incorrectly blocked</p>"},{"location":"proofpointo365connector/#step-2-create-the-redirect-infrastructure","title":"Step 2: Create the Redirect Infrastructure","text":"<p>Create the outbound connector for redirecting bypass attempts:</p> <pre><code># Connect to Exchange Online PowerShell\nConnect-ExchangeOnline -UserPrincipalName &lt;your-admin-account&gt;\n\n# Create outbound connector for redirect functionality\nNew-OutboundConnector -Name 'Redirect Bypass to MX' `\n  -ConnectorType 'Partner' `\n  -UseMxRecord:$true `\n  -IsTransportRuleScoped:$True\n</code></pre> <p>Explanation: - <code>UseMxRecord:$true</code>: Forces mail back through proper MX record lookup - <code>IsTransportRuleScoped:$True</code>: Only activates when triggered by transport rule - This connector will route suspicious mail back to ProofPoint for proper processing</p>"},{"location":"proofpointo365connector/#step-3-create-application-server-exception-rule","title":"Step 3: Create Application Server Exception Rule","text":"<pre><code># Rule to allow legitimate application servers (Azure, Physical Company Locations etc.)\nNew-TransportRule -Name 'Allow Company Servers Direct Delivery' `\n  -FromScope NotInOrganization `\n  -RecipientDomainIs \"yourdomain.com\" `\n  -SenderIpRanges @(\n    # Azure/Application Server IPs\n    \"XXX.XX.XX.XX/32\",\"YY.YY.YYY.YYY/32\"\n  ) `\n  -SetAuditSeverity Low `\n  -StopRuleProcessing $true\n</code></pre> <p>Explanation: - Purpose: Allows legitimate application servers to deliver directly to O365 - StopRuleProcessing: If this rule matches, don't process the redirect rule - Easy Maintenance: Simple to update when adding new Azure regions or application servers</p>"},{"location":"proofpointo365connector/#step-4-create-the-main-bypass-redirect-rule","title":"Step 4: Create the Main Bypass Redirect Rule","text":"<pre><code># Main rule to redirect ProofPoint bypass attempts\nNew-TransportRule -Name 'Redirect Direct Delivery to MX' `\n  -FromScope NotInOrganization `\n  -RecipientDomainIs \"yourdomain.com\" `\n  -ExceptIfSenderIpRanges @(\n    # ProofPoint IP Ranges (Static - rarely change)\n    \"67.231.149.0/24\",\"67.231.148.0/24\",\"67.231.147.0/24\",\n    \"67.231.146.0/24\",\"67.231.145.0/24\",\"67.231.144.0/24\",\n    \"67.231.156.0/24\",\"67.231.155.0/24\",\"67.231.154.0/24\",\n    \"67.231.153.0/24\",\"67.231.152.0/24\",\"148.163.159.0/24\",\n    \"148.163.158.0/24\",\"148.163.157.0/24\",\"148.163.156.0/24\",\n    \"148.163.155.0/24\",\"148.163.154.0/24\",\"148.163.153.0/24\",\n    \"148.163.152.0/24\",\"148.163.151.0/24\",\"148.163.150.0/24\",\n    \"148.163.149.0/24\",\"148.163.148.0/24\",\"148.163.147.0/24\",\n    \"148.163.146.0/24\",\"148.163.145.0/24\",\"148.163.144.0/24\",\n    \"148.163.143.0/24\",\"148.163.142.0/24\",\"148.163.141.0/24\",\n    \"148.163.140.0/24\",\"148.163.139.0/24\",\"148.163.138.0/24\",\n    \"148.163.137.0/24\",\"148.163.136.0/24\",\"148.163.135.0/24\",\n    \"148.163.134.0/24\",\"148.163.133.0/24\",\"148.163.132.0/24\",\n    \"148.163.131.0/24\",\"148.163.130.0/24\",\"148.163.129.0/24\",\n    \"148.163.128.0/24\"\n  ) `\n  -ExceptIfHeaderMatchesMessageHeader \"X-PPE-TRUSTED\" `\n  -ExceptIfHeaderMatchesPatterns \"[\\s\\S]*\" `\n  -SetAuditSeverity Low `\n  -RouteMessageOutboundConnector 'Redirect Bypass to MX'\n</code></pre> <p>Explanation: - Target: External emails to your domain that didn't come through ProofPoint - Exceptions:    - ProofPoint IP ranges (legitimate mail flow)   - X-PPE-TRUSTED header (already processed by ProofPoint) - Action: Route back through MX records (to ProofPoint) instead of quarantining - Result: Suspicious mail gets processed by your security stack before delivery</p>"},{"location":"proofpointo365connector/#rule-processing-flow","title":"Rule Processing Flow","text":"<pre><code>Email arrives at O365 EOP\n    \u2193\nPriority 1: ProofPoint SCL Rule\n\u251c\u2500\u2500 From ProofPoint IP? \u2192 Set SCL -1 (Trusted) \u2192 Deliver\n    \u2193\nPriority 2: Azure Application Allow Rule  \n\u251c\u2500\u2500 From Azure/App IP? \u2192 Allow Direct Delivery \u2192 Stop Processing\n    \u2193\nPriority 3: Bypass Redirect Rule\n\u251c\u2500\u2500 From external &amp; not ProofPoint? \u2192 Redirect to MX (ProofPoint)\n\u251c\u2500\u2500 From ProofPoint or has X-PPE-TRUSTED? \u2192 Allow delivery\n    \u2193\nNormal delivery continues\n</code></pre>"},{"location":"proofpointo365connector/#loop-prevention-mechanisms","title":"Loop Prevention Mechanisms","text":"<p>How we prevent mail loops:</p> <ol> <li>IP-based Recognition: When ProofPoint processes redirected mail and delivers it back, it comes from ProofPoint IPs</li> <li>Header-based Recognition: ProofPoint adds X-PPE-TRUSTED header to processed mail</li> <li>Exception Logic: Both trigger exceptions in the redirect rule, preventing re-redirect</li> </ol> <p>Example Flow: <pre><code>Attacker sends direct to EOP \u2192 Redirect Rule triggers \u2192 Mail sent to ProofPoint\n    \u2193\nProofPoint processes mail \u2192 Delivers from ProofPoint IP with X-PPE-TRUSTED header\n    \u2193\nRedirect rule exceptions apply \u2192 Mail delivered normally (no loop)\n</code></pre></p>"},{"location":"proofpointo365connector/#7-why-the-redirect-approach-is-superior","title":"7. Why the Redirect Approach is Superior","text":""},{"location":"proofpointo365connector/#comparison-quarantine-vs-redirect","title":"Comparison: Quarantine vs Redirect","text":"Aspect Quarantine Approach Redirect Approach Security High (blocks everything) High (forces through security stack) False Positives Very High Low Business Impact Severe disruption Minimal disruption Maintenance High (constant release requests) Low (automated processing) User Experience Poor (missing emails) Good (delayed but delivered) IT Overhead Very High Low"},{"location":"proofpointo365connector/#business-benefits-of-redirect-approach","title":"Business Benefits of Redirect Approach","text":"<ol> <li>Maintains Business Continuity: Legitimate emails still get delivered after processing</li> <li>Reduces Help Desk Load: No quarantine release requests for legitimate mail</li> <li>Provides Complete Protection: All email goes through your security stack</li> <li>Enables Monitoring: Can track exactly how much bypass traffic you're getting</li> <li>Flexible: Easy to add exceptions for legitimate sources</li> </ol>"},{"location":"proofpointo365connector/#security-benefits","title":"Security Benefits","text":"<ol> <li>Zero Bypass: All external email is forced through ProofPoint</li> <li>Comprehensive Coverage: Catches all bypass attempts, not just obvious spoofs</li> <li>Maintains Defenses: Your security stack processes everything</li> <li>Audit Trail: Complete visibility into redirect activity</li> </ol>"},{"location":"proofpointo365connector/#8-implementation-steps-and-best-practices","title":"8. Implementation Steps and Best Practices","text":""},{"location":"proofpointo365connector/#pre-implementation-checklist","title":"Pre-Implementation Checklist","text":"<ol> <li> <p>Document Current State:    <pre><code># Get current transport rules\nGet-TransportRule | Select-Object Name, Priority, State\n\n# Get current connectors  \nGet-InboundConnector | Select-Object Name, SenderIPAddresses\nGet-OutboundConnector | Select-Object Name, SmartHosts\n</code></pre></p> </li> <li> <p>Test Environment: If possible, test in a development tenant first</p> </li> <li> <p>Communication Plan: Inform users about potential short delays during initial processing</p> </li> </ol>"},{"location":"proofpointo365connector/#implementation-order","title":"Implementation Order","text":"<ol> <li>Create redirect connector (doesn't affect mail flow until rules are created)</li> <li>Create Azure/Application allow rule (prevents legitimate app mail from being redirected)  </li> <li>Create bypass redirect rule (starts catching bypass attempts)</li> <li>Monitor and adjust as needed</li> </ol>"},{"location":"proofpointo365connector/#post-implementation-monitoring","title":"Post-Implementation Monitoring","text":"<ol> <li> <p>Check Rule Reports:    <pre><code># Check rule execution statistics\nGet-TransportRule \"Redirect Direct Delivery to MX\" | Get-TransportRuleReport\n</code></pre></p> </li> <li> <p>Monitor Message Trace: Look for emails being redirected and ensure they're delivered after processing</p> </li> <li> <p>Review Connector Validation: The redirect connector will show \"Validation failed\" - this is normal and expected</p> </li> </ol>"},{"location":"proofpointo365connector/#maintenance-commands","title":"Maintenance Commands","text":"<p>Update Azure/Application IPs: <pre><code>Set-TransportRule -Identity 'Allow Azure Application Servers Direct Delivery' `\n  -SenderIpRanges @(\"NEW_IP_LIST_HERE\")\n</code></pre></p> <p>Check Current Settings: <pre><code>Get-TransportRule \"Allow Azure Application Servers Direct Delivery\" | \n  Select-Object SenderIpRanges\n</code></pre></p>"},{"location":"proofpointo365connector/#9-troubleshooting-common-issues","title":"9. Troubleshooting Common Issues","text":""},{"location":"proofpointo365connector/#issue-connector-shows-validation-failed","title":"Issue: Connector Shows \"Validation Failed\"","text":"<p>Symptom: The redirect connector shows validation failure in Exchange Admin Center</p> <p>Cause: Normal behavior - ProofPoint has security restrictions preventing test connections</p> <p>Resolution: This is expected and the connector will work when triggered by transport rules</p>"},{"location":"proofpointo365connector/#issue-meeting-forwards-still-being-caught","title":"Issue: Meeting Forwards Still Being Caught","text":"<p>Symptom: Internal meeting forwards are being redirected</p> <p>Analysis: Check message headers for: - <code>X-MS-Exchange-MeetingForward-Message: Forward</code> - Internal sender information</p> <p>Resolution: These should not trigger the redirect rule due to <code>FromScope NotInOrganization</code>, but if they do, add specific exceptions</p>"},{"location":"proofpointo365connector/#issue-legitimate-partners-being-redirected","title":"Issue: Legitimate Partners Being Redirected","text":"<p>Symptom: Known good partners' emails are being redirected</p> <p>Analysis: Verify if they're sending directly to EOP instead of through normal MX lookup</p> <p>Resolution: Add their IP ranges to the exception list or ask them to use proper MX record delivery</p>"},{"location":"proofpointo365connector/#10-advanced-configurations","title":"10. Advanced Configurations","text":""},{"location":"proofpointo365connector/#adding-regional-proofpoint-ips","title":"Adding Regional ProofPoint IPs","text":"<p>If your organization expands to new regions with different ProofPoint IPs:</p> <pre><code># Get current ProofPoint IPs\n$currentRule = Get-TransportRule \"Redirect Direct Delivery to MX\"\n$currentIPs = $currentRule.ExceptIfSenderIpRanges\n\n# Add new regional IPs\n$newIPs = $currentIPs + @(\"NEW_PROOFPOINT_RANGE_1\",\"NEW_PROOFPOINT_RANGE_2\")\n\n# Update rule\nSet-TransportRule -Identity \"Redirect Direct Delivery to MX\" -ExceptIfSenderIpRanges $newIPs\n</code></pre>"},{"location":"proofpointo365connector/#custom-reporting","title":"Custom Reporting","text":"<p>Create custom monitoring for redirect activity:</p> <pre><code># Get message trace for redirected emails (last 7 days)\nGet-MessageTrace -StartDate (Get-Date).AddDays(-7) -EndDate (Get-Date) |\n  Where-Object {$_.MessageTraceId -like \"*Redirect*\"} |\n  Select-Object Received, SenderAddress, RecipientAddress, Status\n</code></pre>"},{"location":"proofpointo365connector/#11-security-considerations-and-best-practices","title":"11. Security Considerations and Best Practices","text":""},{"location":"proofpointo365connector/#regular-review-tasks","title":"Regular Review Tasks","text":"<ol> <li>Monthly: Review transport rule reports for unusual activity</li> <li>Quarterly: Audit IP exception lists for accuracy</li> <li>Annually: Review overall email security architecture</li> </ol>"},{"location":"proofpointo365connector/#security-hardening","title":"Security Hardening","text":"<ol> <li>Monitor Redirect Volume: Sudden increases may indicate attack campaigns</li> <li>Review Exception Lists: Ensure only legitimate IPs are excepted</li> <li>DMARC Policy Evolution: Move from <code>p=quarantine</code> to <code>p=reject</code> when confident</li> </ol>"},{"location":"proofpointo365connector/#compliance-considerations","title":"Compliance Considerations","text":"<ol> <li>Audit Trails: Ensure message tracking captures redirect activity</li> <li>Data Retention: Plan for log retention requirements</li> <li>Incident Response: Include redirect logs in security incident procedures</li> </ol>"},{"location":"proofpointo365connector/#12-summary-and-lessons-learned","title":"12. Summary and Lessons Learned","text":""},{"location":"proofpointo365connector/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Simple quarantine approaches can cause significant business disruption</li> <li>Redirect-based solutions provide better balance of security and usability  </li> <li>Layered approach with multiple rules provides comprehensive protection</li> <li>ProofPoint bypass attacks are common and require specific countermeasures</li> <li>Proper exception handling is critical for operational success</li> </ol>"},{"location":"proofpointo365connector/#the-final-architecture","title":"The Final Architecture","text":"<p>Our solution creates a comprehensive security architecture:</p> <ul> <li>Layer 1: ProofPoint SCL rule ensures processed mail is trusted</li> <li>Layer 2: Application server exceptions prevent legitimate apps from being redirected</li> <li>Layer 3: Bypass redirect ensures all other external mail goes through security stack</li> <li>Result: Zero bypass attacks while maintaining business continuity</li> </ul>"},{"location":"proofpointo365connector/#future-considerations","title":"Future Considerations","text":"<ul> <li>Monitor for new bypass techniques as attackers evolve</li> <li>Consider additional security layers as threat landscape changes</li> <li>Regular review and updates to maintain effectiveness</li> </ul>"},{"location":"proofpointo365connector/#13-additional-resources","title":"13. Additional Resources","text":""},{"location":"proofpointo365connector/#useful-powershell-commands","title":"Useful PowerShell Commands","text":"<pre><code># Connect to Exchange Online\nConnect-ExchangeOnline -UserPrincipalName admin@yourcompany.com\n\n# List all transport rules with priorities\nGet-TransportRule | Select-Object Name, Priority, State | Sort-Object Priority\n\n# Check specific rule details\nGet-TransportRule \"Redirect Direct Delivery to MX\" | Format-List\n\n# View connector status\nGet-OutboundConnector \"Redirect Bypass to MX\" | Format-List\n\n# Message trace for troubleshooting\nGet-MessageTrace -SenderAddress \"test@externaldomain.com\" -StartDate (Get-Date).AddHours(-2)\n</code></pre>"},{"location":"proofpointo365connector/#dns-verification-commands","title":"DNS Verification Commands","text":"<pre><code># Check MX records\ndig +short MX yourdomain.com\n\n# Verify SPF\ndig +short TXT yourdomain.com | grep spf\n\n# Check DMARC policy  \ndig +short TXT _dmarc.yourdomain.com\n\n# Verify DKIM\ndig +short TXT selector1._domainkey.yourdomain.com\n</code></pre> <p>This comprehensive approach to email security demonstrates that effective protection requires balancing security needs with operational requirements. The redirect-based solution provides robust protection against bypass attacks while maintaining the business continuity essential for modern organizations.</p> <p>For more information, consult Microsoft and Proofpoint documentation, and regularly audit your email security setup. Remember that email security is an ongoing process that requires continuous monitoring and adjustment as threats evolve.</p>"},{"location":"sap-concur-expense-reports-aggregation/","title":"Automating SAP Concur Expense Report Aggregation and Adaptive Card Notifications","text":""},{"location":"sap-concur-expense-reports-aggregation/#introduction","title":"Introduction","text":"<p>This article provides a comprehensive, company-agnostic walkthrough for automating SAP Concur expense report aggregation and delivering actionable, interactive notifications to managers using Adaptive Cards. We\u2019ll cover:</p> <ul> <li>Securely connecting to SAP Concur with OAuth2</li> <li>Fetching and processing users and expense reports</li> <li>Aggregating by employee and by full management chain (organization-wide rollup)</li> <li>Creating and sending Adaptive Card emails with summary/detail toggles</li> <li>All supporting functions, with code and explanations</li> </ul> <p>By the end, you\u2019ll be able to connect to your own SAP Concur instance and deliver organization-wide expense insights to managers in a modern, interactive format.</p>"},{"location":"sap-concur-expense-reports-aggregation/#1-connecting-to-sap-concur-api","title":"1. Connecting to SAP Concur API","text":"<p>To fetch expense reports, you need to: - Obtain an OAuth2 access token using your SAP Concur client credentials and refresh token. - Use the access token to call the Concur API endpoints for users and expense reports.</p>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_scope","title":"Supporting Function: <code>get_scope()</code>","text":"<p>SAP Concur APIs require a specific OAuth2 scope string. This function returns the required scope for all expense and user operations:</p> <pre><code>def get_scope():\n    return (\n        \"openid USER user.read user.write LIST spend.list.read spend.listitem.read CONFIG EXPRPT FISVC \"\n        \"creditcardaccount.read IMAGE expense.exchangerate.writeonly profile.user.generaluser.read \"\n        \"profile.user.generalemployee.read expense.report.read expense.report.readwrite spend.list.write \"\n        \"spend.listitem.write identity.user.ids.read identity.user.core.read identity.user.coresensitive.read \"\n        \"identity.user.enterprise.read identity.user.event.read\"\n    )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_access_token","title":"Supporting Function: <code>get_access_token()</code>","text":"<p>This function retrieves an OAuth2 access token using your client ID, secret, and refresh token:</p> <pre><code>def get_access_token():\n    try:\n        return get_authentication_token(\n            client_id=SAP_CONCUR_CLIENT_APP_ID,\n            client_secret=SAP_CONCUR_CLIENT_SECRET,\n            refresh_token=SAP_CONCUR_REFRESH_TOKEN,\n            scope=get_scope(),\n        )\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_authentication_token","title":"Supporting Function: <code>get_authentication_token()</code>","text":"<p>Handles the actual OAuth2 token request:</p> <pre><code>def get_authentication_token(client_id, client_secret, refresh_token, scope):\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n        \"scope\": scope,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n    response = requests.post(SAP_CONCUR_OAUTH_END_POINT, headers=headers, data=data)\n    response.raise_for_status()\n    return response.json().get(\"access_token\")\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_cached_access_token","title":"Supporting Function: <code>get_cached_access_token()</code>","text":"<p>Caches the access token to avoid unnecessary requests:</p> <pre><code>access_token_cache = {\"token\": None, \"expires_at\": None}\n\ndef get_cached_access_token():\n    if access_token_cache[\"token\"] and access_token_cache[\"expires_at\"] &gt; datetime.now(timezone.utc):\n        return access_token_cache[\"token\"]\n    new_token = get_access_token()\n    if new_token:\n        access_token_cache[\"token\"] = new_token\n        access_token_cache[\"expires_at\"] = datetime.now(timezone.utc) + timedelta(hours=1)\n    return new_token\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#2-fetching-users-and-expense-reports","title":"2. Fetching Users and Expense Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_all_sap_concur_users","title":"Supporting Function: <code>get_all_sap_concur_users()</code>","text":"<p>Fetches all users from SAP Concur (with pagination):</p> <pre><code>def get_all_sap_concur_users():\n    try:\n        access_token = get_cached_access_token()\n        base_url = \"https://us.api.concursolutions.com/profile/identity/v4.1/Users\"\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\"\n        }\n        all_users = []\n        next_cursor = None\n        while True:\n            url = base_url\n            if next_cursor:\n                url += f\"?cursor={next_cursor}\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n            all_users.extend(data.get(\"items\", []))\n            next_cursor = data.get(\"nextCursor\")\n            if not next_cursor:\n                break\n        return all_users\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_expense_reports","title":"Supporting Function: <code>fetch_expense_reports()</code>","text":"<p>Fetches all expense reports for a given user:</p> <pre><code>def fetch_expense_reports(user_name, query_parameters):\n    access_token = get_cached_access_token()\n    base_url = f\"https://us.api.concursolutions.com/api/v3.0/expense/reports\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\", \"Accept\": \"application/json\"}\n    reports = []\n    next_page = f\"{base_url}?user={user_name}{query_parameters}\"\n    while next_page:\n        response = requests.get(next_page, headers=headers)\n        response.raise_for_status()\n        data = response.json()\n        reports.extend(data.get(\"Items\", []))\n        next_page = data.get(\"NextPage\")\n    return reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_all_expense_reports","title":"Supporting Function: <code>fetch_all_expense_reports()</code>","text":"<p>Fetches all reports for all users:</p> <pre><code>def fetch_all_expense_reports(user_mappings, query_parameters):\n    all_reports = []\n    for user in user_mappings:\n        reports = fetch_expense_reports(user, query_parameters)\n        for report in reports:\n            report[\"UserId\"] = user\n        all_reports.extend(reports)\n    return all_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#3-processing-and-aggregating-reports","title":"3. Processing and Aggregating Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-process_reports","title":"Supporting Function: <code>process_reports()</code>","text":"<p>Normalizes report data for aggregation:</p> <pre><code>def process_reports(all_reports):\n    return [\n        {\n            \"UserId\": report.get(\"UserId\"),\n            \"Name\": report.get(\"Name\"),\n            \"Total\": report.get(\"Total\"),\n            \"CurrencyCode\": report.get(\"CurrencyCode\"),\n            \"SubmitDate\": report.get(\"SubmitDate\"),\n            \"OwnerLoginID\": report.get(\"OwnerLoginID\"),\n            \"OwnerName\": report.get(\"OwnerName\"),\n            \"ApproverLoginID\": report.get(\"ApproverLoginID\"),\n            \"ApproverName\": report.get(\"ApproverName\"),\n            \"ApprovalStatusName\": report.get(\"ApprovalStatusName\"),\n            \"ApprovalStatusCode\": report.get(\"ApprovalStatusCode\"),\n            \"PaymentStatusName\": report.get(\"PaymentStatusName\"),\n            \"PaymentStatusCode\": report.get(\"PaymentStatusCode\"),\n            \"LastModifiedDate\": report.get(\"LastModifiedDate\"),\n            \"AmountDueEmployee\": report.get(\"AmountDueEmployee\"),\n            \"AmountDueCompanyCard\": report.get(\"AmountDueCompanyCard\"),\n            \"TotalClaimedAmount\": report.get(\"TotalClaimedAmount\"),\n            \"TotalApprovedAmount\": report.get(\"TotalApprovedAmount\"),\n            \"LedgerName\": report.get(\"LedgerName\"),\n            \"PolicyID\": report.get(\"PolicyID\"),\n            \"EverSentBack\": report.get(\"EverSentBack\"),\n            \"HasException\": report.get(\"HasException\"),\n            \"URI\": report.get(\"URI\"),\n        }\n        for report in all_reports\n    ]\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-employee-aggregate_expense_reports_by_employee","title":"Aggregating by Employee: <code>aggregate_expense_reports_by_employee()</code>","text":"<p>Groups and sums expense reports for each employee, optionally by approval status or by individual report.</p> <pre><code>def aggregate_expense_reports_by_employee(processed_reports, summary):\n    employee_reports = {}\n    for report in processed_reports:\n        user_name = str(report.get(\"OwnerLoginID\", \"\") or \"\").lower()\n        report_name = report.get(\"Name\", \"\")\n        report_id = report.get(\"ReportID\", \"\")\n        approval_status_code = str(report.get(\"ApprovalStatusCode\", \"\") or \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        key = (\n            f\"{approval_status_code}-({approval_status_name})\"\n            if summary\n            else f\"{report_name}-({report_id})-{approval_status_code}-({approval_status_name})\"\n        )\n        total = report.get(\"Total\", 0)\n        employee_reports.setdefault(user_name, {}).setdefault(key, 0)\n        employee_reports[user_name][key] += total\n    return employee_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-organization-aggregate_expense_reports_by_full_oraganization","title":"Aggregating by Organization: <code>aggregate_expense_reports_by_full_oraganization()</code>","text":"<p>Rolls up expense totals for each manager, including all direct and indirect reports, using a recursive helper.</p> <pre><code>def aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, summary):\n    object_organization_reports = {}\n    # Build a reverse mapping of manager to their direct reports\n    manager_to_reports = {}\n    for employee, details in management_upns.items():\n        manager = details.get(\"manager\")\n        if manager:\n            manager_to_reports.setdefault(manager.lower(), []).append(employee.lower())\n    def aggregate_totals_upwards(manager, visited):\n        if manager in visited:\n            return\n        visited.add(manager)\n        if manager not in object_organization_reports:\n            object_organization_reports[manager] = {}\n        for employee in manager_to_reports.get(manager, []):\n            aggregate_totals_upwards(employee, visited)\n            for status, total in object_organization_reports.get(employee, {}).items():\n                if status not in object_organization_reports[manager]:\n                    object_organization_reports[manager][status] = 0\n                object_organization_reports[manager][status] += total\n    # Populate initial totals for each employee based on processed reports\n    for report in processed_reports:\n        if report.get(\"UserManager\"):\n            user_manager = report.get(\"UserManager\", \"\").lower()\n        else:\n            continue\n        approval_status_code = report.get(\"ApprovalStatusCode\", \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        user_name = report.get(\"OwnerLoginID\", \"\")\n        key = f\"{approval_status_code}-({approval_status_name})\" if summary else f\"{user_name}-{approval_status_code}-({approval_status_name})\"\n        total = report.get(\"Total\", 0)\n        if user_manager not in object_organization_reports:\n            object_organization_reports[user_manager] = {}\n        if key not in object_organization_reports[user_manager]:\n            object_organization_reports[user_manager][key] = 0\n        object_organization_reports[user_manager][key] += total\n    # Aggregate totals upwards starting from all unique managers\n    visited = set()\n    for manager in manager_to_reports.keys():\n        aggregate_totals_upwards(manager, visited)\n    return object_organization_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#4-creating-adaptive-card-emails-summary-vs-detail-toggle","title":"4. Creating Adaptive Card Emails (Summary vs. Detail Toggle)","text":"<p>Adaptive Cards are JSON payloads that Outlook and Teams can render as interactive UI. Here\u2019s how to create a card with a summary and a toggle for details:</p> <pre><code>def create_adaptive_info_card_for_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    try:\n        summary_total = summary_by_organization.get(manager_email, 0)\n        detail_items = [\n            {\n                \"type\": \"TextBlock\",\n                \"text\": f\"{user}: {summary_by_employee.get(user, 0):,.2f}\",\n                \"wrap\": True\n            }\n            for user in detail_by_organization.get(manager_email, [])\n        ]\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.4\",\n            \"body\": [\n                {\"type\": \"TextBlock\", \"text\": \"Expense Report Summary\", \"weight\": \"Bolder\", \"size\": \"Large\"},\n                {\"type\": \"TextBlock\", \"text\": f\"Total for your organization: {summary_total:,.2f}\", \"wrap\": True},\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Click below to view details.\",\n                    \"wrap\": True,\n                    \"spacing\": \"Medium\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"id\": \"detailsContainer\",\n                    \"isVisible\": False,\n                    \"items\": detail_items\n                }\n            ],\n            \"actions\": [\n                {\n                    \"type\": \"Action.ToggleVisibility\",\n                    \"title\": \"Show/Hide Details\",\n                    \"targetElements\": [\"detailsContainer\"]\n                }\n            ]\n        }\n        return adaptive_card\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#sending-the-adaptive-card-email","title":"Sending the Adaptive Card Email","text":"<pre><code>def send_adaptive_info_email_to_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    adaptive_card = create_adaptive_info_card_for_manager(\n        manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports\n    )\n    email_payload = {\n        \"message\": {\n            \"subject\": \"Expense Report Summary\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                    f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                    f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                    f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n        }\n    }\n    send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#5-end-to-end-workflow-example","title":"5. End-to-End Workflow Example","text":"<p>Here\u2019s a high-level workflow you can adapt:</p> <pre><code>def main():\n    # 1. Fetch management hierarchy from your HR system\n    management_upns = fetch_management_upns()  # {employee: {\"manager\": manager_email, ...}}\n    # 2. Fetch all SAP Concur users\n    sap_concur_users = get_all_sap_concur_users()\n    # 3. Fetch all expense reports for all users\n    all_reports = fetch_all_expense_reports(sap_concur_users, \"&amp;submitDateAfter=2025-01-01\")\n    # 4. Normalize and process reports\n    processed_reports = process_reports(all_reports)\n    # 5. Aggregate by employee and organization\n    summary_by_employee = aggregate_expense_reports_by_employee(processed_reports, True)\n    summary_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, True)\n    detail_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, False)\n    # 6. Send Adaptive Card emails to each manager\n    for manager_email in summary_by_organization:\n        send_adaptive_info_email_to_manager(\n            manager_email, summary_by_employee, summary_by_organization, detail_by_organization, processed_reports\n        )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#references","title":"References","text":"<ul> <li>SAP Concur API Reference</li> <li>Microsoft Adaptive Cards</li> <li>Microsoft Graph API for Sending Mail</li> </ul>"},{"location":"sap-concur-expense-reports-aggregation/#conclusion","title":"Conclusion","text":"<p>With these patterns and supporting functions, you can connect to your own SAP Concur instance, fetch and aggregate expense reports by employee and by full reporting chain, and deliver actionable, interactive notifications to managers using Adaptive Cards. This enables powerful, organization-wide financial insights and automated reporting for managers at every level.</p>"},{"location":"sap-rfc-python-container/","title":"Installing the <code>PyRFC</code> Module for SAP Integration: A Step-by-Step Guide","text":"<p>Integrating Python with SAP systems using the <code>PyRFC</code> module can unlock powerful automation and data access capabilities. This article provides a clear, professional walkthrough for setting up the SAP NetWeaver RFC SDK and building the <code>PyRFC</code> Python package from scratch.</p>"},{"location":"sap-rfc-python-container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the SAP NetWeaver RFC SDK (download from the official SAP website)</li> <li>Basic familiarity with Linux command line</li> <li>Python 3.x and administrative privileges on your system</li> </ul>"},{"location":"sap-rfc-python-container/#1-download-the-netweaver-rfc-sdk","title":"1. Download the NetWeaver RFC SDK","text":"<ul> <li>Download the latest NetWeaver RFC SDK from the SAP website.</li> <li>Place the downloaded file (<code>nwrfc750P_14-70002752.zip</code>) in your repository's <code>assets</code> folder for easy access.</li> </ul>"},{"location":"sap-rfc-python-container/#2-prepare-the-sap-sdk-directory","title":"2. Prepare the SAP SDK Directory","text":"<p>Create the target directory for the SAP SDK:</p> <pre><code>sudo mkdir -p /usr/local/sap/\n</code></pre>"},{"location":"sap-rfc-python-container/#3-extract-and-copy-the-sdk","title":"3. Extract and Copy the SDK","text":"<ul> <li>Extract the <code>nwrfcsdk</code> folder from the ZIP file.</li> <li>Copy the extracted <code>nwrfcsdk</code> folder to <code>/usr/local/sap/</code>.</li> </ul>"},{"location":"sap-rfc-python-container/#4-configure-the-library-path","title":"4. Configure the Library Path","text":"<p>Create a configuration file for the dynamic linker and add the SDK library path:</p> <pre><code>sudo nano /etc/ld.so.conf.d/nwrfcsdk.conf\n\n# Add the following line to the file:\n/usr/local/sap/nwrfcsdk/lib\n</code></pre>"},{"location":"sap-rfc-python-container/#5-update-the-library-cache-and-set-environment-variable","title":"5. Update the Library Cache and Set Environment Variable","text":"<p>Update the system's library cache and set the required environment variable:</p> <pre><code>sudo ldconfig\n# Verify the path configuration should not have any errors\nldconfig -p | grep sap\n# Set Environment Variable\nexport SAPNWRFC_HOME=/usr/local/sap/nwrfcsdk\n</code></pre>"},{"location":"sap-rfc-python-container/#6-install-cython-and-build-essentials","title":"6. Install Cython and Build Essentials","text":"<p>Install the necessary build tools and Python dependencies:</p> <pre><code>pip install Cython\nsudo apt-get update\nsudo apt-get install -y build-essential python3-dev\n</code></pre>"},{"location":"sap-rfc-python-container/#7-build-and-install-pyrfc","title":"7. Build and Install <code>pyrfc</code>","text":"<p>Clone the PyRFC repository and build the package:</p> <pre><code>git clone https://github.com/SAP/PyRFC.git\ncd PyRFC\npython -m pip install --upgrade build\nPYRFC_BUILD_CYTHON=yes python -m build --wheel --sdist --outdir dist\npip install --upgrade --no-index --find-links=dist pyrfc\n</code></pre> <p>Pro Tip: Double-check all paths and environment variables before building. For troubleshooting, consult the PyRFC documentation or reach out to the SAP community forums.</p> <p>By following these steps, you\u2019ll have a working Python-to-SAP integration environment using the <code>pyrfc</code> module. Happy coding!</p>"},{"location":"sap-rfc-python/","title":"Calling SAP RFC Function Modules from Python Using PyRFC: A Step-by-Step Guide","text":"<p>Note: For details on installing and configuring the <code>PyRFC</code> module inside a container, see the companion article: Installing the PyRFC Module for SAP Integration</p>"},{"location":"sap-rfc-python/#introduction","title":"Introduction","text":"<p>SAP ECC systems expose powerful RFC (Remote Function Call) interfaces that allow external programs to interact with SAP data and business logic. Python, with the help of the PyRFC library, makes it possible to call these RFC function modules directly and process the results in a modern, flexible way.</p> <p>This article demonstrates how to: - Connect to an SAP ECC 6.0 (EHP 8) system from Python - Call a custom RFC function module  - Pass parameters to the RFC - Retrieve tabular data - Save the results to a CSV file</p> <p>We will use a modular, production-ready approach inspired by real-world enterprise integration scripts.</p>"},{"location":"sap-rfc-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an SAP ECC system with a custom RFC function module you can call</li> <li>SAP user credentials with RFC permissions</li> <li>The PyRFC library installed (see Installing the PyRFC Module for SAP Integration for setup)</li> <li>Python 3.7+</li> </ul>"},{"location":"sap-rfc-python/#example-extracting-data-from-sap-via-rfc","title":"Example: Extracting Data from SAP via RFC","text":"<p>Suppose you want to extract financial data from SAP using a custom RFC function module. The following example shows how to do this in a robust, reusable way.</p>"},{"location":"sap-rfc-python/#1-define-your-rfc-connection-and-extract-configuration","title":"1. Define Your RFC Connection and Extract Configuration","text":"<pre><code>from pyrfc import Connection, LogonError, ABAPApplicationError, ABAPRuntimeError\nimport csv\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"sap_rfc_extract\")\n\n# --- RFC Connection Parameters (replace with your SAP system details) ---\nSAP_CONN_PARAMS = {\n    'ashost': 'SAP_APP_SERVER_HOST',   # SAP application server\n    'sysnr': '00',                     # System number\n    'client': '100',                   # Client number\n    'user': 'SAP_USERNAME',            # SAP user\n    'passwd': 'SAP_PASSWORD',          # SAP password\n    'lang': 'EN',                      # Language\n}\n\n# --- RFC Extract Configuration ---\nEXTRACT_CONFIG = {\n    'example_extract': {\n        'function_module': 'ZMY_CUSTOM_RFC_MODULE',  # Replace with your RFC FM name\n        'table_name': 'IT_RESULT_TAB',              # The table returned by the RFC\n        'params': ['IM_CC', 'IM_YEAR', 'IM_PERIOD'],\n        'default_params': {'IM_CC': '1000', 'IM_YEAR': '2025', 'IM_PERIOD': '05'},\n        'filename_fmt': 'sap_extract_{cc}_{year}_{period}.csv',\n    },\n}\n</code></pre>"},{"location":"sap-rfc-python/#2-utility-functions-for-rfc-calls-and-csv-export","title":"2. Utility Functions for RFC Calls and CSV Export","text":"<pre><code>def call_rfc(conn_params, function_module, params):\n    try:\n        conn = Connection(**conn_params)\n        logger.info(f\"Calling RFC: {function_module} with params: {params}\")\n        return conn.call(function_module, **params)\n    except LogonError as e:\n        logger.error(f\"Logon Error: {e}\")\n    except ABAPApplicationError as e:\n        logger.error(f\"ABAP Application Error: {e}\")\n    except ABAPRuntimeError as e:\n        logger.error(f\"ABAP Runtime Error: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n    return None\n\ndef export_result_to_csv(table_data, filename):\n    if not table_data:\n        logger.warning(\"No data to export.\")\n        return\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=table_data[0].keys())\n        writer.writeheader()\n        writer.writerows(table_data)\n    logger.info(f\"Exported data to {filename}\")\n</code></pre>"},{"location":"sap-rfc-python/#3-main-script-running-the-extract","title":"3. Main Script: Running the Extract","text":"<pre><code>import argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run SAP RFC extract via PyRFC.\")\n    parser.add_argument('--im_cc', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_CC'], help='Company code')\n    parser.add_argument('--im_year', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_YEAR'], help='Fiscal year')\n    parser.add_argument('--im_period', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_PERIOD'], help='Fiscal period')\n    args = parser.parse_args()\n\n    # Prepare parameters for RFC call\n    params = {\n        'IM_CC': args.im_cc,\n        'IM_YEAR': args.im_year,\n        'IM_PERIOD': args.im_period,\n    }\n\n    config = EXTRACT_CONFIG['example_extract']\n    result = call_rfc(SAP_CONN_PARAMS, config['function_module'], params)\n    if result and config['table_name'] in result:\n        # Build filename\n        filename = config['filename_fmt'].format(\n            cc=args.im_cc, year=args.im_year, period=args.im_period\n        )\n        export_result_to_csv(result[config['table_name']], filename)\n    else:\n        logger.error(\"No data returned from RFC or table not found in result.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sap-rfc-python/#4-running-the-script","title":"4. Running the Script","text":"<p>You can run the script from the command line, specifying parameters as needed:</p> <pre><code>python sap_rfc_extract.py --im_cc=1000 --im_year=2025 --im_period=05\n</code></pre> <ul> <li>The script will connect to SAP, call the RFC, and save the results to a CSV file (e.g., <code>sap_extract_1000_2025_05.csv</code>).</li> <li>You can override any parameter using the command line.</li> </ul>"},{"location":"sap-rfc-python/#5-step-by-step-explanation","title":"5. Step-by-Step Explanation","text":"<ol> <li>Configuration:</li> <li>All SAP connection details and extract metadata are defined at the top for easy maintenance.</li> <li> <p>The RFC function module name and table name are generic placeholders\u2014replace them with your actual SAP details.</p> </li> <li> <p>Calling the RFC:</p> </li> <li>The <code>call_rfc</code> function establishes a connection and calls the RFC, handling common SAP errors.</li> <li> <p>Parameters are passed as a dictionary, matching the RFC signature.</p> </li> <li> <p>Exporting Data:</p> </li> <li> <p>The <code>export_result_to_csv</code> function writes the returned table to a CSV file, using the first row's keys as headers.</p> </li> <li> <p>Command-Line Interface:</p> </li> <li> <p>The script uses <code>argparse</code> to allow easy parameter overrides from the command line.</p> </li> <li> <p>Error Handling:</p> </li> <li>All errors are logged, and the script will not crash on SAP or network errors.</li> </ol>"},{"location":"sap-rfc-python/#conclusion","title":"Conclusion","text":"<p>With this approach, you can easily: - Call any SAP RFC function module from Python - Parameterize your extracts - Save results to CSV for downstream processing - Integrate SAP data into modern Python workflows</p> <p>For more advanced scenarios (multi-company code loops, dynamic extract configuration, etc.), see the full project code or reach out for further examples.</p>"},{"location":"sap-rfc-python/#further-reading","title":"Further Reading","text":"<ul> <li>PyRFC Documentation</li> <li>SAP RFC SDK</li> <li>Installing the PyRFC Module for SAP Integration \u2014 How to install and configure PyRFC in a container</li> </ul>"},{"location":"sap-system-start-stop/","title":"Automating SAP System Start/Stop Operations with PowerShell","text":""},{"location":"sap-system-start-stop/#introduction","title":"Introduction","text":"<p>Managing SAP systems efficiently requires reliable automation tools that can handle both routine maintenance and emergency situations. This article demonstrates how to implement a comprehensive PowerShell-based solution for automating SAP system start and stop operations across multiple environments including ECC (Enterprise Central Component), BW (Business Warehouse), and Dispatch systems.</p> <p>The solution provides a centralized approach to managing different SAP landscapes while maintaining proper logging, error handling, and notification mechanisms.</p>"},{"location":"sap-system-start-stop/#architecture-overview","title":"Architecture Overview","text":"<p>The automation script is built around several key components:</p> <ul> <li>Environment-specific configurations for different SAP systems</li> <li>Secure password management using PowerShell's credential system</li> <li>Robust logging with timestamped entries</li> <li>Email notifications for operation status updates</li> <li>Error handling to ensure graceful failure recovery</li> </ul>"},{"location":"sap-system-start-stop/#core-functions","title":"Core Functions","text":""},{"location":"sap-system-start-stop/#1-process-execution-helper","title":"1. Process Execution Helper","text":"<p>The <code>InvokeStartProcess</code> function serves as the foundation for executing SAP control commands:</p> <pre><code>Function InvokeStartProcess\n{\n    Param(  [string]$Command,\n            [string]$WorkingDirectory,\n            [string]$Arguments,\n            [System.Int16]$SleepInSeconds,\n            [string]$Replaceinlogfile\n        )\n    try \n    {\n        $Processrun = Start-Process -FilePath $Command `\n                                        -WorkingDirectory $WorkingDirectory `\n                                        -ArgumentList $Arguments `\n                                        -Wait `\n                                        -Passthru\n\n        Write-Log \"Executed Command \", $Command, $Arguments.Replace($Replaceinlogfile,\"nothing2writeinlog\") -join\n        Write-Log \"Sleeping for \", $SleepInSeconds.ToString(), \" seconds\" -join\n        Start-Sleep -Seconds $SleepInSeconds\n    }\n    catch \n    {\n        Write-Log \"Error While running command \", $Command, $Arguments -join\n        $Processrun.Kill()\n    }\n    finally \n    {\n    }\n}\n</code></pre> <p>Key Features: - Executes SAP control commands with proper parameters - Implements security by masking sensitive information in logs - Includes configurable sleep intervals for system stabilization - Provides error handling with process termination on failure</p>"},{"location":"sap-system-start-stop/#2-sap-system-stop-function","title":"2. SAP System Stop Function","text":"<p>The <code>StopSAP</code> function handles the shutdown sequence for different SAP environments:</p> <pre><code>Function StopSAP \n{\n    Param([System.String]$Environment)\n    try \n    {\n        $Command = \"sapcontrol.exe\"\n        SendEmail \"SAP stop has been initiated via Automated script.\" \"Stop Initiated\" $Environment\n        $serviceaccountpwd = GetPassword $Environment\n        $WorkingDirectory = \"C:\\Program Files\\SAP\\hostctrl\\exe\"\n\n        if ($Environment -eq \"ECCDEV\")\n        {\n            $CommandlineArgs = \"-nr 0 -host [MASKED-DEV-HOST] -user devadm \" + $serviceaccountpwd + \" -function StopSystem\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        if ($Environment -eq \"ECCTEST\")\n        {\n            $CommandlineArgs = \"-nr 0 -host [MASKED-TEST-HOST] -user tstadm \" + $serviceaccountpwd + \" -function StopSystem\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        if ($Environment -eq \"ECCPROD\")\n        {\n            $CommandlineArgs = \"-nr 0 -host [MASKED-PROD-HOST] -user prdadm \" + $serviceaccountpwd + \" -function StopSystem\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        # BW Development - Multi-step shutdown process\n        if ($Environment -eq \"BWDEV\")\n        {\n            $CommandlineArgs = \"-nr 0 -host [MASKED-BWDEV-HOST] -user bwdadm \" + $serviceaccountpwd + \" -function StopSystem DIALOG\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n            $CommandlineArgs = \"-nr 1 -host [MASKED-BWDEV-HOST] -user bwdadm \" + $serviceaccountpwd + \" -function StopSystem SCS\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        # Additional environment configurations...\n\n        SendEmail \"SAP stop has now been completed via Automated script.\" \"Stop Completed\" $Environment\n    }\n    catch \n    {\n        Write-Output \"Error While Stopping SAP\"    \n        $Error.Clear()\n    }\n}\n</code></pre> <p>Environment-Specific Handling: - ECC Systems: Standard single-command shutdown - BW Systems: Multi-step process (DIALOG \u2192 SCS components) - Dispatch Systems: Specialized user account handling</p>"},{"location":"sap-system-start-stop/#3-sap-system-start-function","title":"3. SAP System Start Function","text":"<p>The <code>StartSAP</code> function manages the startup sequence with proper component ordering:</p> <pre><code>Function StartSAP \n{\n    Param([System.String]$Environment)\n    try \n    {\n        $Command = \"sapcontrol.exe\"\n        SendEmail \"SAP start has been initiated via Automated script.\" \"Start Initiated\" $Environment\n        $serviceaccountpwd = GetPassword $Environment\n        $WorkingDirectory = \"C:\\Program Files\\SAP\\hostctrl\\exe\"\n\n        if ($Environment -eq \"ECCDEV\")\n        {\n            $CommandlineArgs = \"-nr 1 -host [MASKED-DEV-HOST] -user devadm \" + $serviceaccountpwd + \" -function StartSystem\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        # BW Development - Ordered startup process\n        if ($Environment -eq \"BWDEV\")\n        {\n            # Start SCS first\n            $CommandlineArgs = \"-nr 1 -host [MASKED-BWDEV-HOST] -user bwdadm \" + $serviceaccountpwd + \" -function StartSystem SCS\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n\n            # Then start DIALOG instances\n            $CommandlineArgs = \"sapcontrol -nr 0 -host [MASKED-BWDEV-HOST] -user bwdadm \" + $serviceaccountpwd + \" -function StartSystem DIALOG\"\n            InvokeStartProcess $Command $WorkingDirectory $CommandlineArgs 60 $serviceaccountpwd\n        }\n\n        SendEmail \"SAP start has completed via Automated script.\" \"Start Completed\" $Environment\n    }\n    catch \n    {\n        Write-Output \"Error While Starting SAP\"    \n        $Error.Clear()\n    }\n}\n</code></pre> <p>Startup Sequencing: - SCS (SAP Central Services) components start first - DIALOG instances follow after SCS stabilization - Each step includes proper timing delays</p>"},{"location":"sap-system-start-stop/#security-implementation","title":"Security Implementation","text":""},{"location":"sap-system-start-stop/#password-management","title":"Password Management","text":"<p>The script implements secure password handling through encrypted storage:</p> <pre><code>Function GetPassword\n{\n    Param([string]$Environment)\n    try \n    {\n        $localpassword = \"\"\n        $passwordFileName = \"\"\n\n        $passwordFileName = \".\\\" + $Environment + \".txt\"\n        $localpassword = Get-Content $passwordFileName | ConvertTo-SecureString \n        $localpassword = [System.Net.NetworkCredential]::new(\"\", $localpassword).Password\n        return \"\"\"\" + $localpassword + \"\"\"\"\n    }\n    catch \n    {\n        Write-Output \"Error while getting password\"    \n        $Error.Clear()\n    }\n}\n</code></pre> <p>Security Features: - Passwords stored as encrypted SecureString objects - Environment-specific password files - No plain-text credentials in script or logs - Proper credential object handling</p>"},{"location":"sap-system-start-stop/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"sap-system-start-stop/#structured-logging","title":"Structured Logging","text":"<pre><code>function Write-Log {\n    Param($message)\n    Write-Output \"$(get-date -format 'yyyyMMdd HH:mm:ss') $message\" | Out-File -Encoding utf8 $logFile -Append\n}\n</code></pre>"},{"location":"sap-system-start-stop/#email-notifications","title":"Email Notifications","text":"<pre><code>Function SendEmail\n{\n    Param([string]$Body,[string]$Subject, [string]$Environment)\n\n    $From = \"SAP-\" + $Environment + '@[MASKED-DOMAIN].Com'\n    $To = \"[MASKED-EMAIL]@[MASKED-DOMAIN].com\",\"[MASKED-ADMIN]@[MASKED-DOMAIN].com\"\n\n    Send-MailMessage    -To $To `\n                        -Subject $Subject `\n                        -Body $Body `\n                        -From $From `\n                        -SmtpServer \"[MASKED-SMTP-SERVER]\" `\n                        -Port 25 \n}\n</code></pre> <p>Notification Features: - Environment-specific sender addresses - Multiple recipient support - Operation status tracking (Initiated/Completed) - Error notification capability</p>"},{"location":"sap-system-start-stop/#main-execution-logic","title":"Main Execution Logic","text":"<p>The script's main execution block handles command-line arguments and orchestrates the operations:</p> <pre><code>try \n{\n    $ScriptFolder = 'D:\\Scripts\\'\n    $LogFileName = 'SAPAutomation.txt'\n    $logFile = $ScriptFolder + (get-date -format 'yyyyMMdd') + $LogFileName\n\n    Write-Log \"Started SAP Automation Script\"\n    $StartStop = $args[0]      # \"START\" or \"STOP\"\n    $SapEnvironment = $args[1] # Environment identifier\n\n    if ($StartStop -eq \"STOP\")\n    {\n        StopSAP $SapEnvironment\n    }\n    if ($StartStop -eq \"START\")\n    {\n        StartSAP $SapEnvironment\n    }\n    Write-Log \"Finished SAP Automation Script\"\n}\ncatch \n{\n    $ErrorMessage = $_.Exception.message\n    write-log \"Error - $ErrorMessage\"\n}\n</code></pre>"},{"location":"sap-system-start-stop/#usage-examples","title":"Usage Examples","text":""},{"location":"sap-system-start-stop/#starting-an-ecc-development-system","title":"Starting an ECC Development System","text":"<pre><code>.\\SAPAutomation.ps1 START ECCDEV\n</code></pre>"},{"location":"sap-system-start-stop/#stopping-a-bw-production-system","title":"Stopping a BW Production System","text":"<pre><code>.\\SAPAutomation.ps1 STOP BWPROD\n</code></pre>"},{"location":"sap-system-start-stop/#batch-operations","title":"Batch Operations","text":"<pre><code># Stop all development systems for maintenance\n.\\SAPAutomation.ps1 STOP ECCDEV\n.\\SAPAutomation.ps1 STOP BWDEV\n.\\SAPAutomation.ps1 STOP DISPATCHDEV\n\n# Start systems after maintenance\n.\\SAPAutomation.ps1 START ECCDEV\n.\\SAPAutomation.ps1 START BWDEV\n.\\SAPAutomation.ps1 START DISPATCHDEV\n</code></pre>"},{"location":"sap-system-start-stop/#supported-environments","title":"Supported Environments","text":"Environment System Type Special Handling ECCDEV ECC Development Standard operation ECCTEST ECC Test Standard operation ECCPROD ECC Production Standard operation BWDEV BW Development Multi-component sequence BWTEST BW Test Multi-component sequence BWPROD BW Production Standard operation DISPATCHDEV Dispatch Development Local account handling DISPATCHPROD Dispatch Production Local account handling"},{"location":"sap-system-start-stop/#best-practices","title":"Best Practices","text":""},{"location":"sap-system-start-stop/#1-pre-execution-checks","title":"1. Pre-execution Checks","text":"<ul> <li>Verify system status before operations</li> <li>Ensure proper user permissions</li> <li>Validate network connectivity to target hosts</li> </ul>"},{"location":"sap-system-start-stop/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Implement comprehensive try-catch blocks</li> <li>Log all operations and errors</li> <li>Provide meaningful error messages</li> </ul>"},{"location":"sap-system-start-stop/#3-security-considerations","title":"3. Security Considerations","text":"<ul> <li>Use encrypted password storage</li> <li>Implement least-privilege access</li> <li>Regular password rotation</li> <li>Audit trail maintenance</li> </ul>"},{"location":"sap-system-start-stop/#4-maintenance","title":"4. Maintenance","text":"<ul> <li>Regular log file cleanup</li> <li>Password file security validation</li> <li>Script version control</li> <li>Testing in non-production environments</li> </ul>"},{"location":"sap-system-start-stop/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sap-system-start-stop/#common-issues","title":"Common Issues","text":"<p>Connection Failures: - Verify network connectivity to SAP hosts - Check firewall rules for SAP ports - Validate DNS resolution</p> <p>Authentication Errors: - Verify password file encryption/decryption - Check user account permissions - Ensure service account status</p> <p>Process Failures: - Review SAP system logs - Check system resource availability - Verify SAP service status</p>"},{"location":"sap-system-start-stop/#conclusion","title":"Conclusion","text":"<p>This PowerShell automation solution provides a robust framework for managing SAP system operations across multiple environments. The implementation demonstrates key principles of enterprise automation including security, logging, error handling, and notification management.</p> <p>The modular design allows for easy extension to additional SAP environments while maintaining consistency in operations and monitoring. Regular maintenance and testing ensure reliable operation in production environments.</p>"},{"location":"sap-system-start-stop/#references","title":"References","text":"<ul> <li>SAP Administration Guide</li> <li>PowerShell Documentation</li> </ul> <p>This article demonstrates enterprise-level SAP automation practices. Always test scripts in development environments before production deployment.</p>"},{"location":"sharepoint-site-library-enumeration/","title":"SharePoint Files and Folders Inventory with Python and Microsoft Graph API","text":""},{"location":"sharepoint-site-library-enumeration/#introduction","title":"Introduction","text":"<p>This article provides a detailed, company-agnostic guide to inventorying all files and folders across all SharePoint sites and document libraries in a Microsoft 365 tenant using Python and the Microsoft Graph API. It focuses on the <code>get_all_files_from_sp</code> function and its supporting functions, with best practices for handling large environments, including recommendations for running the code in an Azure container.</p>"},{"location":"sharepoint-site-library-enumeration/#microsoft-graph-api-endpoint-constant","title":"Microsoft Graph API Endpoint Constant","text":"<p>The code uses the following constant for all Microsoft Graph API v1.0 calls:</p> <p><pre><code>AZURE_GRAPH_V1 = 'https://graph.microsoft.com/v1.0/'\n</code></pre> This ensures all API requests are made to the correct Microsoft Graph endpoint.</p>"},{"location":"sharepoint-site-library-enumeration/#key-functions-and-code-walkthrough","title":"Key Functions and Code Walkthrough","text":""},{"location":"sharepoint-site-library-enumeration/#1-get_all_files_from_sp","title":"1. <code>get_all_files_from_sp</code>","text":"<p>This is the main orchestration function for SharePoint inventory. It: - Retrieves all root SharePoint sites using <code>get_all_sp_sites</code>. - Expands the list to include all subsites with <code>fetch_all_sites_including_subsites</code>. - Iterates through every site and its document libraries, calling <code>process_document_library</code> for each. - Sends notification emails on progress and completion.</p> <p>Full Function Code: <pre><code>def get_all_files_from_sp():\n    try:\n        gdep_sharepoint_root_sites = get_all_sp_sites()\n        gdep_all_sites = fetch_all_sites_including_subsites(gdep_sharepoint_root_sites)\n\n        for site in gdep_all_sites:\n            site_id = site[\"id\"]\n            site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives\"\n\n            document_libraries = execute_odata_query_get(site_url)\n            for library in document_libraries:\n                process_document_library(site_id, library[\"id\"], library[\"name\"], gdep_all_sites)\n                send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n                    subject=f'Completed Doc Lib --&gt;{library[\"name\"]} - on site {site[\"webUrl\"]}',\n                    plain_message=f'Update on SP Library{library[\"name\"]} - for site {site_url}')\n\n        send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n            subject=f'Finished all Sites',\n            plain_message=f'Finished all Sites')\n\n    except Exception as e:\n        handle_global_exception(inspect.currentframe().f_code.co_name, e)\n    finally:\n        pass\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#explanation","title":"Explanation","text":"<ul> <li>Site Discovery: Uses <code>get_all_sp_sites()</code> to get all root sites, then <code>fetch_all_sites_including_subsites()</code> to get all subsites.</li> <li>Document Library Enumeration: For each site, queries all document libraries (drives) and processes them.</li> <li>Progress Notification: Sends emails after each library and when all sites are complete.</li> <li>Error Handling: All exceptions are logged and reported.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#2-get_all_sp_sites","title":"2. <code>get_all_sp_sites</code>","text":"<p>Fetches all root-level SharePoint sites in the tenant using the Microsoft Graph API: <pre><code>def get_all_sp_sites():\n    url = f\"{AZURE_GRAPH_V1}sites?search=*\"\n    return execute_odata_query_get(url)\n</code></pre> - Purpose: Returns a list of all root SharePoint sites. - API Used: List SharePoint Sites</p>"},{"location":"sharepoint-site-library-enumeration/#3-fetch_all_sites_including_subsites","title":"3. <code>fetch_all_sites_including_subsites</code>","text":"<p>Recursively discovers all subsites for each root site: <pre><code>def fetch_all_sites_including_subsites(sharepoint_root_sites):\n    all_sites = []\n    for site in sharepoint_root_sites:\n        logger.info(f\"Started site {site['webUrl']}\")\n        all_sites.append({\"id\": site[\"id\"], \"webUrl\": site[\"webUrl\"]})\n        sharepoint_subsites = get_sp_subsites(site[\"id\"])\n        if len(sharepoint_subsites) &gt; 0:\n            for subsite in sharepoint_subsites:\n                all_sites.append({\"id\": subsite[\"id\"], \"webUrl\": subsite[\"webUrl\"]})\n    return all_sites\n</code></pre> - Purpose: Ensures every site and subsite is included in the inventory. - API Used: List Subsites</p>"},{"location":"sharepoint-site-library-enumeration/#4-process_document_library","title":"4. <code>process_document_library</code>","text":"<p>Processes each document library (drive) for a site: <pre><code>def process_document_library(site_id, drive_id, drive_name, all_sites):\n    data = []\n    logger.info(f\"Started Document Library -- {drive_name}\")\n    start_time = time.perf_counter()\n    site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives/{drive_id}/root/delta{DOCUMENT_LIB_SELECT_QUERY}\"\n    search_results = execute_odata_query_get(site_url)\n    for item in search_results:\n        entry = {\n            \"site_id\": site_id,\n            \"webUrl\": next(site[\"webUrl\"] for site in all_sites if site[\"id\"] == site_id),\n            \"drive_id\": drive_id,\n            \"document_id\": item[\"id\"],\n            \"name\": item[\"name\"],\n            \"lastModifiedDateTime\": parse_iso_date(item.get(\"lastModifiedDateTime\")),\n            \"size\": item.get(\"size\") if \"file\" in item else \"\"\n        }\n        data.append(entry)\n    write_data_to_csv(data, SP_WITHOUT_VERSION_CSV_FILE_PATH)\n    elapsed_time = time.perf_counter() - start_time\n    logger.info(f\"Document Library '{drive_name}' took {elapsed_time:.2f} seconds to process.\")\n</code></pre> - Purpose:   - Queries all files in the document library using the Graph API delta endpoint.   - Collects metadata for each file.   - Writes results to a CSV for further processing or database import.   - Logs processing time for performance monitoring.</p>"},{"location":"sharepoint-site-library-enumeration/#supporting-utilities-full-implementations","title":"Supporting Utilities (Full Implementations)","text":""},{"location":"sharepoint-site-library-enumeration/#execute_odata_query_geturl","title":"<code>execute_odata_query_get(url)</code>","text":"<p>Handles authenticated GET requests to the Microsoft Graph API, including error handling and token refresh. <pre><code>def execute_odata_query_get(url):\n    try:\n        token = get_access_token_API_Access_AAD()\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json().get(\"value\", [])\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#parse_iso_datedate_str","title":"<code>parse_iso_date(date_str)</code>","text":"<p>Converts ISO 8601 date strings to Python datetime objects for easier manipulation and formatting. <pre><code>def parse_iso_date(date_str: str):\n    if not date_str:\n        return None\n    date_str = date_str.rstrip('Z')\n    formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\"]\n    for fmt in formats:\n        try:\n            return datetime.strptime(date_str, fmt).date()\n        except ValueError:\n            continue\n    return None\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#write_data_to_csvdata-file_path","title":"<code>write_data_to_csv(data, file_path)</code>","text":"<p>Appends data to a CSV file, writing headers if the file does not exist. <pre><code>def write_data_to_csv(data, file_path):\n    file_exists = os.path.isfile(file_path)\n    with open(file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n        fieldnames = [\"site_id\", \"webUrl\", \"drive_id\", \"document_id\", \"name\", \"lastModifiedDateTime\", \"size\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerows(data)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handle_global_exceptionfunctionname-exceptionobject","title":"<code>handle_global_exception(functionName, exceptionObject)</code>","text":"<p>Logs and emails details of any exception that occurs. <pre><code>def handle_global_exception(functionName, exceptionObject):\n    emailBody = f\"Function Name: {functionName}; Exception Description: {exceptionObject}\"\n    send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n               subject='Exception occured in code', \n               plain_message=emailBody)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#get_access_token_api_access_aadscopesnone","title":"<code>get_access_token_API_Access_AAD(scopes=None)</code>","text":"<p>Obtains an access token for Microsoft Graph API using MSAL or Azure Identity. (Example implementation:) <pre><code>def get_access_token_API_Access_AAD(scopes=None):\n    if scopes is None:\n        scopes = ['https://graph.microsoft.com/.default']\n    app = ConfidentialClientApplication(\n        client_id=AZURE_CONFIDENTIAL_APP_ID,\n        authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n        client_credential=AZURE_CONFIDENTIAL_SECRET\n    )\n    result = app.acquire_token_for_client(scopes=scopes)\n    return result[\"access_token\"]\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handling-large-sharepoint-environments","title":"Handling Large SharePoint Environments","text":"<p>Important: Large tenants with many sites, subsites, and document libraries can have tens or hundreds of thousands of files. Processing all of them can take significant time and resources.</p>"},{"location":"sharepoint-site-library-enumeration/#best-practices-for-large-document-libraries","title":"Best Practices for Large Document Libraries","text":"<ul> <li>Run in Azure: For large environments, it is highly recommended to run this inventory code in an Azure Container Instance or Azure VM. This ensures:</li> <li>Sufficient compute and memory resources.</li> <li>Proximity to Microsoft 365 services for faster API calls.</li> <li>Ability to scale or schedule the job as needed.</li> <li>Batch Processing: The code is designed to process and write data in batches, minimizing memory usage and allowing for partial progress in case of interruptions.</li> <li>Progress Notifications: The function sends email notifications after each document library and when all sites are complete, so you can monitor long-running jobs.</li> <li>Error Handling: All exceptions are logged and reported, ensuring that issues with individual sites or libraries do not halt the entire process.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#example-end-to-end-inventory-flow","title":"Example: End-to-End Inventory Flow","text":"<ol> <li>Discover Sites:</li> <li><code>get_all_sp_sites()</code> \u2192 returns all root sites.</li> <li>Expand to Subsites:</li> <li><code>fetch_all_sites_including_subsites()</code> \u2192 returns all sites and subsites.</li> <li>Process Each Library:</li> <li>For each site, enumerate all document libraries and call <code>process_document_library()</code>.</li> <li>Write Results:</li> <li>Metadata for each file is written to a CSV file for further analysis or database import.</li> </ol>"},{"location":"sharepoint-site-library-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API: List SharePoint Sites</li> <li>Microsoft Graph API: List Drive Items</li> <li>Azure Container Instances Documentation</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#conclusion","title":"Conclusion","text":"<p>The <code>get_all_files_from_sp</code> function and its supporting helpers provide a robust, scalable way to inventory all files and folders across a Microsoft 365 tenant's SharePoint environment. For large tenants, running this code in an Azure container or VM is strongly recommended to ensure reliability and performance.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/","title":"Copying Files from SharePoint to Azure File Share at Scale","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#overview","title":"Overview","text":"<p>This article provides a comprehensive, company-agnostic guide for copying large volumes of files from SharePoint Online document libraries into Azure File Shares. The solution is designed for high-throughput, scalable execution (e.g., as an Azure Container Instance), and is suitable for enterprise-scale migrations, backups, or data archiving. The approach leverages multi-threading for performance and handles large files (30\u201350 GB+) efficiently by streaming and chunked uploads.</p> <p>Key Features: - Secure, certificate-based authentication to Microsoft Graph and Azure - Multi-threaded file copy for high throughput - Chunked upload for large files - Robust error handling and progress tracking - All secrets managed via Azure Key Vault</p> <p>Note: This article assumes you have already extracted the list of files and their metadata from SharePoint. For details on how to enumerate SharePoint files and extract metadata, see Extracting SharePoint Document Library Metadata.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Azure File Share and connection string</li> <li>Azure AD App Registration with certificate-based authentication</li> <li>Azure Key Vault for secret management</li> <li>Extracted metadata for all SharePoint files to be copied (see stub above)</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#solution-architecture","title":"Solution Architecture","text":"<ol> <li>Metadata Extraction: Retrieve all file metadata from SharePoint (site ID, drive ID, item ID, file path, size, timestamps, etc.) and store in a database or CSV. (See stub above)</li> <li>File Copy Process: For each file, download from SharePoint using Microsoft Graph and upload to Azure File Share, preserving directory structure and metadata.</li> <li>Multi-threading: Use a thread pool to process multiple files in parallel for maximum throughput.</li> <li>Chunked Upload: For large files, stream and upload in chunks to avoid memory issues and support files up to 100s of GB.</li> <li>Progress Tracking: Log and track progress for monitoring and troubleshooting.</li> </ol>"},{"location":"sharepoint-site-library-to-azure-fileshare/#full-python-code-example","title":"Full Python Code Example","text":"<p>Below is a complete, production-ready script for the file copy process. All company-specific values have been removed. Replace stub values and secret names as appropriate for your environment.</p> <pre><code>\"\"\"\nThis script copies files from SharePoint Online to Azure File Share using multi-threading and chunked uploads.\n- Designed for high-volume, large-file scenarios (30\u201350 GB+)\n- All secrets are retrieved from Azure Key Vault\n- Can be run as an Azure Container Instance (ACI) or VM\n\"\"\"\nimport threading\nimport requests\nfrom queue import Queue\nfrom urllib.parse import unquote\nimport os\nfrom azure.storage.fileshare import ShareFileClient, ShareDirectoryClient\nfrom datetime import datetime, timedelta, timezone, time\nfrom msal import ConfidentialClientApplication\n\nfrom gdepcommon.logger import setup_logger\nfrom gdepcommon.utils import (\n    get_azure_kv_sceret,\n    sql_dbconnection,\n    PFX_CERTIFICATE_NAME,\n    PFX_CERTIFICATE_NAME_TP\n)\nfrom gdepazure.common import (\n    AZURE_CONFIDENTIAL_APP_ID,\n    AZURE_TENANT_ID,\n    AZURE_AUTHORITY_BASE_URL,\n    AZURE_GRAPH_DEFAULT_RESOURCE\n)\n\n# Thread and chunk parameters\nTHREAD_COUNT = 10  # Tune based on environment\nCHUNK_SIZE = 4 * 1024 * 1024  # 4 MB\n\n# Shared progress state\ntotal_files = 0\nprocessed_files = 0\nlock = threading.Lock()\n\n# Token management\naccess_token = None\ntoken_expiry_time = None\n\ndef refresh_access_token(logger):\n    \"\"\"Refreshes the Microsoft Graph access token using certificate-based auth.\"\"\"\n    global access_token, token_expiry_time\n    try:\n        logger.info(\"Refreshing access token...\")\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n        app = ConfidentialClientApplication(\n            client_id=AZURE_CONFIDENTIAL_APP_ID,\n            authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n        result = app.acquire_token_for_client(scopes=AZURE_GRAPH_DEFAULT_RESOURCE)\n        access_token = result[\"access_token\"]\n        expires_in = result[\"expires_in\"]\n        token_expiry_time = datetime.now(tz=timezone.utc) + timedelta(seconds=expires_in - 60)\n        logger.info(\"Access token refreshed successfully.\")\n    except Exception as e:\n        logger.error(f\"Failed to refresh access token: {e}\")\n        raise Exception(\"Access token refresh failed.\")\n\ndef get_access_token(logger):\n    \"\"\"Returns a valid access token, refreshing if expired.\"\"\"\n    global access_token, token_expiry_time\n    if not access_token or datetime.now(tz=timezone.utc) &gt;= token_expiry_time:\n        for attempt in range(3):\n            try:\n                refresh_access_token(logger)\n                break\n            except Exception as e:\n                if attempt &lt; 2:\n                    logger.warning(f\"Retrying token refresh (attempt {attempt + 1}/3)...\")\n                else:\n                    raise e\n    return access_token\n\ndef ensure_directory_path_exists(azure_conn_str, share_name, directory_path, cache=None):\n    \"\"\"Ensures the full directory path exists in Azure File Share.\"\"\"\n    if cache is None:\n        cache = set()\n    parts = directory_path.strip('/').split('/')\n    current_path = ''\n    for part in parts:\n        current_path = f\"{current_path}/{part}\" if current_path else part\n        if current_path in cache:\n            continue\n        dir_client = ShareDirectoryClient.from_connection_string(\n            conn_str=azure_conn_str,\n            share_name=share_name,\n            directory_path=current_path\n        )\n        try:\n            dir_client.create_directory()\n            cache.add(current_path)\n        except Exception as ex:\n            if \"ResourceAlreadyExists\" in str(ex):\n                cache.add(current_path)\n            else:\n                raise\n\ndef copy_file_from_sp_to_azure(site_id, drive_id, item_id, azure_file_client, total_file_size_in_bytes, created_date=None, modified_date=None, logger=None):\n    \"\"\"Streams a file from SharePoint and uploads to Azure File Share in chunks.\"\"\"\n    access_token = get_access_token(logger)\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/items/{item_id}/content\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    retries = 3\n    for attempt in range(retries):\n        try:\n            logger.info(f\"Starting file copy for item_id: {item_id}\")\n            with requests.get(url, headers=headers, stream=True) as response:\n                response.raise_for_status()\n                azure_file_client.create_file(size=total_file_size_in_bytes)\n                offset = 0\n                for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n                    azure_file_client.upload_range(data=chunk, offset=offset, length=len(chunk))\n                    offset += len(chunk)\n                    # Log progress for large files (&gt;5GB) every 10%\n                    if total_file_size_in_bytes &gt; 5 * 1024 * 1024 * 1024:\n                        percent_complete = (offset / total_file_size_in_bytes) * 100\n                        if int(offset / CHUNK_SIZE) % int((total_file_size_in_bytes / CHUNK_SIZE) * 0.05) == 0:\n                            logger.info(f\"File {item_id}: {percent_complete:.2f}% complete\")\n            # Set file properties for created/modified dates\n            if created_date or modified_date:\n                file_properties = {}\n                if created_date:\n                    created_datetime = datetime.combine(created_date, time())\n                    file_properties['file_creation_time'] = created_datetime\n                if modified_date:\n                    modified_datetime = datetime.combine(modified_date, time())\n                    file_properties['file_last_write_time'] = modified_datetime\n                from azure.storage.fileshare import ContentSettings\n                content_settings = ContentSettings(content_type=\"application/octet-stream\")\n                azure_file_client.set_http_headers(file_attributes=\"none\", content_settings=content_settings, **file_properties)\n            logger.info(f\"Successfully copied file for item_id: {item_id}\")\n            return\n        except requests.exceptions.RequestException as e:\n            if attempt &lt; retries - 1:\n                logger.warning(f\"Retrying file copy for item_id {item_id} (attempt {attempt + 1}/{retries})...\")\n            else:\n                logger.error(f\"Failed to copy file for item_id: {item_id}. Error: {e}\")\n                raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during file copy for item_id {item_id}: {e}\")\n\ndef worker(queue, azure_conn_str, share_name, created_dirs, logger):\n    \"\"\"Thread worker function: processes files from the queue.\"\"\"\n    global processed_files\n    while not queue.empty():\n        try:\n            file_record = queue.get()\n            site_id = file_record['site_id']\n            drive_id = file_record['drive_id']\n            item_id = file_record['item_id']\n            total_file_size_in_bytes = file_record['length']\n            created_date = file_record['created_date']\n            modified_date = file_record['modified_date']\n            decoded_path = file_record['decoded_path']\n            # Ensure directory exists\n            azure_directory_path = os.path.dirname(decoded_path)\n            if azure_directory_path:\n                ensure_directory_path_exists(azure_conn_str, share_name, azure_directory_path, created_dirs)\n            file_client = ShareFileClient.from_connection_string(\n                conn_str=azure_conn_str,\n                share_name=share_name,\n                file_path=decoded_path\n            )\n            copy_file_from_sp_to_azure(site_id, drive_id, item_id, file_client, total_file_size_in_bytes, created_date, modified_date, logger)\n            # Mark as copied in DB\n            with sql_dbconnection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"UPDATE [dbo].[Fact_Document_Library_Details] SET [copied] = 1 WHERE [unique_id] = ?\", file_record['unique_id'])\n                conn.commit()\n            with lock:\n                global processed_files\n                processed_files += 1\n                overall_progress = (processed_files / total_files) * 100\n                logger.info(f\"Overall Progress: {overall_progress:.2f}% ({processed_files}/{total_files} files complete)\")\n            queue.task_done()\n        except Exception as e:\n            logger.error(f\"Error processing file: {e}\")\n\ndef main(logger):\n    \"\"\"\n    Main entry point: loads file metadata, initializes threads, and starts the copy process.\n    - Loads Azure File Share connection string from Key Vault\n    - Loads file metadata (site_id, drive_id, item_id, file path, size, timestamps, etc.)\n    - Spawns worker threads to process the file queue\n    - Tracks and logs progress\n    \"\"\"\n    global total_files\n    try:\n        azure_conn_str = get_azure_kv_sceret('your-azure-file-connection-string-secret')\n        share_name = \"your-azure-file-share-name\"\n        site_id = \"your-sharepoint-site-id\"\n        drive_id = \"your-sharepoint-drive-id\"\n        created_dirs = set()\n        # Fetch files to copy (replace with your DB or CSV logic)\n        with sql_dbconnection() as sqlConnection:\n            cursor = sqlConnection.cursor()\n            cursor.execute(\"SELECT * FROM [dbo].[Fact_Document_Library_Details] WHERE [type] = 'file' AND [copied] = 0\")\n            results = cursor.fetchall()\n        total_files = len(results)\n        if total_files == 0:\n            logger.info(\"No files to process.\")\n            return\n        queue = Queue()\n        for row in results:\n            sp_relative_url = row.server_relative_url\n            decoded_path = unquote(sp_relative_url[len(\"/sites/YourSite/YourLibrary\"):].lstrip('/'))\n            queue.put({\n                'site_id': site_id,\n                'drive_id': drive_id,\n                'item_id': row.unique_id,\n                'unique_id': row.unique_id,\n                'length': row.length,\n                'created_date': row.time_created,\n                'modified_date': row.time_last_modified,\n                'decoded_path': decoded_path\n            })\n        threads = []\n        for _ in range(THREAD_COUNT):\n            thread = threading.Thread(target=worker, args=(queue, azure_conn_str, share_name, created_dirs, logger))\n            thread.start()\n            threads.append(thread)\n        for thread in threads:\n            thread.join()\n        logger.info(\"All files have been processed successfully.\")\n    except Exception as e:\n        logger.error(f\"Error in main function: {e}\")\n\nif __name__ == \"__main__\":\n    logger = setup_logger(\"sp2azfileshare\", \"/mnt/azure/logs/sp2azfileshare.log\")\n    main(logger)\n</code></pre>"},{"location":"sharepoint-site-library-to-azure-fileshare/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#1-authentication-and-secret-management","title":"1. Authentication and Secret Management","text":"<ul> <li>All secrets (Azure connection string, certificate thumbprint, etc.) are retrieved from Azure Key Vault using a utility function (<code>get_azure_kv_sceret</code>).</li> <li>Microsoft Graph authentication uses certificate-based credentials for security and automation.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#2-multi-threaded-file-copy","title":"2. Multi-Threaded File Copy","text":"<ul> <li>The script uses a thread pool (<code>THREAD_COUNT</code>) and a <code>Queue</code> to distribute file copy tasks across multiple threads.</li> <li>Each thread processes files independently, ensuring high throughput and efficient use of resources.</li> <li>Thread-safe progress tracking is implemented using a <code>Lock</code>.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#3-chunked-upload-for-large-files","title":"3. Chunked Upload for Large Files","text":"<ul> <li>Files are streamed from SharePoint and uploaded to Azure File Share in 4 MB chunks.</li> <li>This approach supports very large files (30\u201350 GB+) without excessive memory usage.</li> <li>Progress for large files is logged every 10% (configurable).</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#4-directory-structure-and-metadata-preservation","title":"4. Directory Structure and Metadata Preservation","text":"<ul> <li>The script ensures that the full directory path exists in Azure File Share before uploading each file.</li> <li>File creation and modification timestamps are preserved if available.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#5-database-integration-and-idempotency","title":"5. Database Integration and Idempotency","text":"<ul> <li>The script marks each file as copied in the database after successful upload, ensuring idempotency and resumability.</li> <li>You can adapt this logic to use a CSV or other metadata store as needed.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#scaling-and-running-in-azure","title":"Scaling and Running in Azure","text":"<ul> <li>This script is designed to run as an Azure Container Instance (ACI), but can also be run on VMs or Kubernetes.</li> <li>Tune <code>THREAD_COUNT</code> based on available CPU and network bandwidth.</li> <li>For very large migrations, consider splitting the workload across multiple containers or jobs.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#related-articles","title":"Related Articles","text":"<ul> <li>Extracting SharePoint Document Library Metadata for Automation</li> <li>Automating Secure Secret Management with Azure Key Vault</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#conclusion","title":"Conclusion","text":"<p>By following this guide and using the provided code, you can efficiently and securely copy massive volumes of files from SharePoint Online to Azure File Share, with full support for large files, multi-threaded performance, and robust error handling. All sensitive information is managed via Azure Key Vault, ensuring compliance and security for enterprise automation scenarios.</p>"},{"location":"sharepoint-sites-enumeration/","title":"How to Retrieve All SharePoint Sites in Your Microsoft 365 Tenant","text":""},{"location":"sharepoint-sites-enumeration/#introduction","title":"Introduction","text":"<p>Retrieving a complete list of SharePoint sites in your Microsoft 365 (M365) tenant is essential for IT automation, reporting, and governance. This article provides a detailed, company-agnostic, step-by-step guide to programmatically enumerate all SharePoint sites using Python and the Microsoft Graph API. All code samples are generic and ready to use in any tenant.</p>"},{"location":"sharepoint-sites-enumeration/#prerequisites","title":"Prerequisites","text":""},{"location":"sharepoint-sites-enumeration/#1-azure-entra-application-registration","title":"1. Azure Entra Application Registration","text":"<ul> <li>Register an application in Azure Entra (Azure AD).</li> <li>Assign the following Microsoft Graph API permissions:</li> <li><code>Sites.Read.All</code> (Application permission)</li> <li><code>Sites.ReadWrite.All</code> (if you need to write/update)</li> <li>Grant admin consent for these permissions.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#2-certificate-based-authentication","title":"2. Certificate-Based Authentication","text":"<ul> <li>Upload a certificate to your Azure Entra application.</li> <li>Use the certificate thumbprint and private key for authentication.</li> <li>For a detailed guide and code on certificate-based authentication, see: Certificate Auth for Microsoft Graph API</li> </ul>"},{"location":"sharepoint-sites-enumeration/#3-python-environment","title":"3. Python Environment","text":"<ul> <li>Install the required packages:   <pre><code>pip install requests msal\n</code></pre></li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-1-authenticate-and-get-an-access-token","title":"Step 1: Authenticate and Get an Access Token","text":"<p>You need to authenticate as your Azure Entra application and obtain an access token for Microsoft Graph. This is best done using certificate-based authentication for security.</p> <p>Below is a full, reusable function for certificate-based authentication. (Replace the placeholders with your actual values.)</p> <pre><code>import msal\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    # Replace these with your app's values\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n</code></pre> <p>See this blog post for a full explanation and troubleshooting tips for certificate-based authentication.</p>"},{"location":"sharepoint-sites-enumeration/#step-2-query-the-microsoft-graph-api-for-sharepoint-sites","title":"Step 2: Query the Microsoft Graph API for SharePoint Sites","text":"<p>The Microsoft Graph API endpoint to list all sites is:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites?search=*\n</code></pre> <p>This returns a paginated list of root SharePoint sites in your tenant.</p>"},{"location":"sharepoint-sites-enumeration/#helper-function-execute-odata-query","title":"Helper Function: Execute OData Query","text":"<pre><code>import requests\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#retrieve-all-sites-with-pagination","title":"Retrieve All Sites (with Pagination)","text":"<pre><code>def get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation","title":"Explanation:","text":"<ul> <li><code>get_all_sp_sites</code> starts with the root search URL.</li> <li>It uses the access token for authentication.</li> <li>It loops through all pages using the <code>@odata.nextLink</code> property for pagination.</li> <li>All sites are collected in the <code>sites</code> list.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-3-retrieve-subsites-for-each-site","title":"Step 3: Retrieve Subsites for Each Site","text":"<p>To enumerate subsites for a given site, use:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{site-id}/sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#function-to-get-subsites","title":"Function to Get Subsites","text":"<pre><code>def get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation_1","title":"Explanation:","text":"<ul> <li>For each site, call <code>get_sp_subsites(site_id)</code> to get its direct subsites.</li> <li>You can recursively call this function to build a full site tree.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-4-full-example-enumerate-all-sites-and-subsites","title":"Step 4: Full Example - Enumerate All Sites and Subsites","text":"<p>Here is a complete script you can copy, edit, and run in your own environment:</p> <pre><code>import msal\nimport requests\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n\ndef get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n\ndef enumerate_all_sites_and_subsites():\n    all_sites = get_all_sp_sites()\n    all_sites_with_subsites = []\n    for site in all_sites:\n        site_id = site['id']\n        subsites = get_sp_subsites(site_id)\n        site['subsites'] = subsites\n        all_sites_with_subsites.append(site)\n    return all_sites_with_subsites\n\nif __name__ == \"__main__\":\n    all_sites = enumerate_all_sites_and_subsites()\n    print(json.dumps(all_sites, indent=2))\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#step-by-step-code-walkthrough","title":"Step-by-Step Code Walkthrough","text":"<ol> <li>get_access_token_API_Access_AAD: Authenticates using your Azure Entra app and certificate, returning a valid access token for Microsoft Graph.</li> <li>execute_odata_query_get: Sends a GET request to the specified Microsoft Graph endpoint using the access token, returning the parsed JSON response.</li> <li>get_all_sp_sites: Uses the <code>/sites?search=*</code> endpoint to retrieve all root SharePoint sites, handling pagination.</li> <li>get_sp_subsites: For each site, retrieves its direct subsites.</li> <li>enumerate_all_sites_and_subsites: Combines the above to build a list of all sites and their subsites.</li> <li>Main block: Runs the enumeration and prints the result as formatted JSON.</li> </ol>"},{"location":"sharepoint-sites-enumeration/#required-permissions-recap","title":"Required Permissions Recap","text":"<ul> <li><code>Sites.Read.All</code> (Application permission, admin consent required)</li> <li>The Azure Entra app must be granted consent by a tenant admin</li> <li>The app must authenticate using a certificate or secret (certificate recommended)</li> </ul>"},{"location":"sharepoint-sites-enumeration/#troubleshooting-and-tips","title":"Troubleshooting and Tips","text":"<ul> <li>If you get a 403 error, check that your app registration has admin consent for <code>Sites.Read.All</code>.</li> <li>If you get a 401 error, check your certificate and app credentials.</li> <li>The <code>search=*</code> parameter is required to enumerate all sites, not just the root site.</li> <li>For large tenants, always handle pagination using <code>@odata.nextLink</code>.</li> <li>You can extend the code to recursively enumerate subsites to any depth.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API - List sites</li> <li>Microsoft Graph API - List subsites</li> <li>Microsoft Graph permissions reference</li> <li>Register an application with the Microsoft identity platform</li> <li>Certificate credentials for application authentication</li> <li>MSAL for Python documentation</li> <li>Microsoft Graph Explorer</li> </ul>"},{"location":"sharepoint-sites-enumeration/#summary","title":"Summary","text":"<ul> <li>Register an Azure Entra application and grant it <code>Sites.Read.All</code> permission</li> <li>Authenticate using a certificate (see this blog post)</li> <li>Use the Microsoft Graph API <code>/sites?search=*</code> endpoint to enumerate all SharePoint sites</li> <li>Use <code>/sites/{site-id}/sites</code> to enumerate subsites</li> <li>Handle pagination using <code>@odata.nextLink</code></li> </ul> <p>This approach is secure, scalable, and works in any Microsoft 365 tenant. You can now automate SharePoint site inventory, reporting, or governance tasks in your own environment.</p>"},{"location":"spo-o365_excel_automation/","title":"Programmatically Editing Excel Files on SharePoint Online with Python","text":""},{"location":"spo-o365_excel_automation/#introduction","title":"Introduction","text":"<p>Editing Excel files stored on SharePoint Online (SPO) programmatically is a powerful capability for automating business workflows, generating reports, and integrating data from web applications. Unlike traditional methods that rely on Microsoft Excel, COM automation, or macros, the Microsoft Graph API enables direct, secure, and scalable access to Excel workbooks in the cloud\u2014without requiring Excel to be installed or running on the server.</p> <p>This article demonstrates how to: - Open and edit an XLSX file stored on SharePoint Online using Python and the Microsoft Graph API - Inject data into worksheets, triggering automatic updates to formulas, tables, and charts - Loop through worksheets and extract refreshed charts as image files - Build a web interface for user input</p>"},{"location":"spo-o365_excel_automation/#authentication-and-permissions","title":"Authentication and Permissions","text":"<p>For authentication setup and certificate-based SPN connection, see my previous blog articles: - Certificate Based Authentication</p> <p>Required Permissions for SPN: - <code>Files.ReadWrite.All</code> (edit/view files) - <code>Sites.ReadWrite.All</code> (access SPO sites) - <code>User.Read</code> (basic profile) - <code>offline_access</code> (refresh tokens)</p> <p>Once authenticated, you can use the access token to interact with the Excel file on SPO.</p>"},{"location":"spo-o365_excel_automation/#connecting-to-the-excel-file-on-spo","title":"Connecting to the Excel File on SPO","text":"<p>The following code sets up the workbook endpoint and session management:</p> <pre><code>DRIVE_ID = '...'\nITEM_ID = '...'\nGRAPH_BASE = f\"{AZURE_GRAPH_V1}drives/{DRIVE_ID}/items/{ITEM_ID}/workbook\"\n\ndef headers(token, session_id=None):\n    h = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n    if session_id:\n        h[\"workbook-session-id\"] = session_id\n    return h\n</code></pre>"},{"location":"spo-o365_excel_automation/#creating-and-closing-workbook-sessions","title":"Creating and Closing Workbook Sessions","text":"<p>Sessions ensure your changes are saved and visible to all users. Always close the session after editing.</p> <pre><code>def create_session(token, persist=True):\n    url = f\"{GRAPH_BASE}/createSession\"\n    body = {\"persistChanges\": persist}\n    r = requests.post(url, headers=headers(token), json=body)\n    r.raise_for_status()\n    return r.json()[\"id\"]\n\ndef close_session(token, session_id):\n    url = f\"{GRAPH_BASE}/closeSession\"\n    try:\n        r = requests.post(url, headers=headers(token, session_id))\n        if r.status_code == 204:\n            print(\"\u2705 Workbook session closed successfully.\")\n        else:\n            print(f\"\u26a0\ufe0f  Could not close session cleanly: {r.status_code} {r.text}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  Error closing session: {e}\")\n</code></pre>"},{"location":"spo-o365_excel_automation/#listing-worksheets-and-charts","title":"Listing Worksheets and Charts","text":"<p>You can enumerate all worksheets and charts in the workbook:</p> <pre><code>def list_worksheets(token, session_id):\n    url = f\"{GRAPH_BASE}/worksheets\"\n    r = requests.get(url, headers=headers(token, session_id))\n    r.raise_for_status()\n    return r.json().get(\"value\", [])\n\ndef list_charts(token, session_id, sheet_name):\n    url = f\"{GRAPH_BASE}/worksheets('{sheet_name}')/charts\"\n    r = requests.get(url, headers=headers(token, session_id))\n    r.raise_for_status()\n    return r.json().get(\"value\", [])\n</code></pre>"},{"location":"spo-o365_excel_automation/#injecting-data-into-excel-worksheets","title":"Injecting Data into Excel Worksheets","text":"<p>You can update any cell in the workbook using the Graph API. This triggers recalculation of formulas, tables, and charts automatically. Here is a full example from <code>common.py</code> showing how to inject multiple parameters and pump data:</p> <pre><code>def update_workbook(token, session_id, file_to_process):\n    # ...load JSON data...\n    global_params = ['jobPressure', 'driverPower', 'fleetSize', 'pumpUsage', 'engineRPM']\n    for param in global_params:\n        if param in data:\n            param_data = data[param]\n            value = param_data['value']\n            worksheet = param_data['worksheet']\n            cell = param_data['cell']\n            update_cell(token, session_id, worksheet, cell, value)\n\n    # Process pumps array\n    if 'pumps' in data:\n        for idx, pump in enumerate(data['pumps']):\n            # Update pump name\n            name_data = pump['name']\n            update_cell(token, session_id, name_data['worksheet'], name_data['cell'], name_data['value'])\n            # Update pump diameter\n            diameter_data = pump['diameter']\n            update_cell(token, session_id, diameter_data['worksheet'], diameter_data['cell'], diameter_data['value'])\n</code></pre> <p>Key Points: - You do not need to open Excel or use macros. - All formulas, VLOOKUPs, and charts update automatically after cell edits. - Only <code>.xlsx</code> files are supported; macro-enabled files (<code>.xlsm</code>) are not editable via Graph API.</p>"},{"location":"spo-o365_excel_automation/#looping-through-worksheets-and-extracting-charts","title":"Looping Through Worksheets and Extracting Charts","text":"<p>After updating the workbook, you can loop through all worksheets and extract charts as images:</p> <pre><code>worksheets = list_worksheets(token, session_id)\nfor ws in worksheets:\n    ws_name = ws[\"name\"]\n    charts = list_charts(token, session_id, ws_name)\n    if not charts:\n        print(f\"No charts found on worksheet '{ws_name}'\")\n        continue\n    print(f\"Charts on '{ws_name}': {[c['name'] for c in charts]}\")\n    for chart in charts:\n        chart_name = chart[\"name\"]\n        try:\n            img_bytes = get_chart_image(token, session_id, ws_name, chart_name)\n            fname = os.path.join(PROCESSED_PATH, f\"{graph_files_prefix}_{ws_name}_{chart_name.replace(' ', '_')}.png\")\n            with open(fname, \"wb\") as f:\n                f.write(img_bytes)\n            print(f\"   \u2714 Saved chart '{chart_name}' as {fname}\")\n        except Exception as e:\n            print(f\"   \u26a0 Failed to get chart '{chart_name}': {e}\")\n</code></pre> <p>Chart Extraction Function:</p> <pre><code>def get_chart_image(token, session_id, sheet, chart_name):\n    cname = quote(chart_name, safe=\"\")\n    url = f\"{GRAPH_BASE}/worksheets('{sheet}')/charts('{cname}')/image(width=0,height=0,fittingMode='fit')\"\n    r = requests.get(url, headers=headers(token, session_id))\n    r.raise_for_status()\n    b64 = r.json()[\"value\"]\n    return base64.b64decode(b64)\n</code></pre>"},{"location":"spo-o365_excel_automation/#end-to-end-workflow-example","title":"End-to-End Workflow Example","text":"<p>Here is a simplified workflow for processing pump comparison requests:</p> <pre><code>def process_pump_comparison_requests():\n    unprocessed_files = fetch_unprocessed_pump_comparisons_requests()\n    for unprocessed_file in unprocessed_files:\n        token = get_access_token_API_Access_AAD()\n        session_id = create_session(token)\n        graph_files_prefix, email_recipients, email_recipients_bcc = update_workbook(token, session_id, unprocessed_file)\n        OUTPUT_PDF = os.path.join(PROCESSED_PATH, f\"{graph_files_prefix}_Pump_Comparison.pdf\")\n        worksheets = list_worksheets(token, session_id)\n        print(\"Worksheets found:\", [ws[\"name\"] for ws in worksheets])\n        # Loop through worksheets and extract charts\n        for ws in worksheets:\n            ws_name = ws[\"name\"]\n            charts = list_charts(token, session_id, ws_name)\n            if not charts:\n                print(f\"No charts found on worksheet '{ws_name}'\")\n                continue\n            print(f\"Charts on '{ws_name}': {[c['name'] for c in charts]}\")\n            for chart in charts:\n                chart_name = chart[\"name\"]\n                try:\n                    img_bytes = get_chart_image(token, session_id, ws_name, chart_name)\n                    fname = os.path.join(PROCESSED_PATH, f\"{graph_files_prefix}_{ws_name}_{chart_name.replace(' ', '_')}.png\")\n                    with open(fname, \"wb\") as f:\n                        f.write(img_bytes)\n                    print(f\"   \u2714 Saved chart '{chart_name}' as {fname}\")\n                except Exception as e:\n                    print(f\"   \u26a0 Failed to get chart '{chart_name}': {e}\")\n        close_session(token, session_id)\n</code></pre>"},{"location":"spo-o365_excel_automation/#visualizing-results-generating-pdfs-with-refreshed-charts","title":"Visualizing Results: Generating PDFs with Refreshed Charts","text":"<p>After updating the Excel file and extracting charts, you can use ReportLab to generate visually appealing PDF reports. The code in <code>common.py</code> demonstrates how to: - Render worksheet data as tables - Insert chart images - Format and style the PDF output</p> <p>Example: PDF Generation</p> <pre><code>from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image\nfrom reportlab.lib import colors\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\n    OUTPUT_PDF,\n    pagesize=landscape(letter),\n    leftMargin=6,\n    rightMargin=6,\n    topMargin=6,\n    bottomMargin=6\n)\nelements = []\nstyles = getSampleStyleSheet()\n# Add worksheet data as table, add charts as images\n# ...see common.py for full implementation...\n</code></pre>"},{"location":"spo-o365_excel_automation/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ul> <li>Macro-Enabled Files: The Graph API cannot edit <code>.xlsm</code> files. Macros are not executed or updated.</li> <li>Security: All operations require Azure AD authentication and appropriate permissions.</li> <li>Concurrency: Use workbook sessions to avoid conflicts and ensure changes are saved.</li> </ul> <p>Reference: Why can't macro-enabled Excel files be edited via Graph API?</p>"},{"location":"spo-o365_excel_automation/#conclusion","title":"Conclusion","text":"<p>By leveraging the Microsoft Graph API, you can automate Excel file editing and reporting on SharePoint Online\u2014without relying on Excel, COM automation, or macros. This approach is secure, scalable, and works across platforms.</p> <p>Key Takeaways: - Use Graph API for direct, cloud-native Excel automation - Only <code>.xlsx</code> files are supported (no macros) - All formulas, tables, and charts update automatically - Extract charts as images for reporting - Integrate with web applications for seamless user input</p>"},{"location":"spo-o365_excel_automation/#further-reading","title":"Further Reading","text":"<ul> <li>Microsoft Graph Excel API Overview</li> <li>Python Requests Library</li> <li>ReportLab PDF Toolkit</li> </ul>"},{"location":"teams-user-number-assignment/","title":"Retrieving Teams Phone Number Assignments","text":""},{"location":"teams-user-number-assignment/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve Microsoft Teams phone number assignments for users in Entra (Azure AD), combining Python and PowerShell. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"teams-user-number-assignment/#why-use-powershell-for-teams-phone-assignments","title":"Why Use PowerShell for Teams Phone Assignments?","text":"<p>Some Microsoft Teams telephony data, such as direct phone number assignments, is not always available via the Microsoft Graph API or may require additional permissions and modules. PowerShell modules (such as <code>MicrosoftTeams</code> or <code>SkypeOnlineConnector</code>) provide richer access to Teams telephony configuration and are often used in enterprise automation for this purpose.</p>"},{"location":"teams-user-number-assignment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>subprocess</code> (standard library)</li> <li><code>requests</code></li> <li>PowerShell installed on the system (with the required Teams modules)</li> <li>An Azure AD application (service principal) with permissions to read user information</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"teams-user-number-assignment/#step-1-retrieve-teams-phone-number-assignments","title":"Step 1: Retrieve Teams Phone Number Assignments","text":""},{"location":"teams-user-number-assignment/#function-get_teams_phone_number_assignments","title":"Function: <code>get_teams_phone_number_assignments</code>","text":"<p>This function combines user data from Entra (Azure AD) with Teams phone number assignments, which are typically retrieved via a PowerShell script/module.</p> <pre><code>def get_teams_phone_number_assignments(aad_users):\n    try:\n        user_dict = {user['id']: user for user in aad_users}  # Dictionary for faster lookups\n        phone_assignments = get_teams_phone_numbers()  # Calls PowerShell to get assignments\n        teams_phone_assignments = []\n        for assignment in phone_assignments:\n            user_id = assignment['AssignedPstnTargetId']\n            if user_id in user_dict:\n                formatted_assignment = {\n                    'assigned_to_entra_account': 'yes',\n                    'entra_user': user_dict[user_id].get('displayName','') or '',\n                    'teams_telephoneNumber': assignment.get('TelephoneNumber','') or '',\n                    'entra_businessPhones': user_dict[user_id].get('businessPhones','') or '',\n                    'entra_mobilePhone': user_dict[user_id].get('mobilePhone','') or '',\n                    'entra_accountEnabled': user_dict[user_id].get('accountEnabled','') or '',\n                    'entra_employeeType': user_dict[user_id].get('employeeType','') or '',\n                    'entra_city': user_dict[user_id].get('city','') or '',\n                    'entra_officeLocation': user_dict[user_id].get('officeLocation','') or '',\n                    'teams_assignmentCategory': assignment.get('AssignmentCategory','') or '',\n                    'teams_city': assignment.get('City','') or '',\n                    'teams_isosubdivision': assignment.get('IsoSubdivision','') or '',\n                    'teams_numbertype': assignment.get('NumberType','') or '',\n                    'teams_pstnassignmentstatus': assignment.get('PstnAssignmentStatus','') or '',\n                }\n            else:\n                formatted_assignment = {\n                    'assigned_to_entra_account': 'no',\n                    'entra_user': '',\n                    'teams_telephoneNumber': assignment.get('TelephoneNumber','') or '',\n                    'entra_businessPhones': '',\n                    'entra_mobilePhone': '',\n                    'entra_accountEnabled': '',\n                    'entra_employeeType': '',\n                    'entra_city': '',\n                    'entra_officeLocation': '',\n                    'teams_assignmentCategory': assignment.get('AssignmentCategory','') or '',\n                    'teams_city': assignment.get('City','') or '',\n                    'teams_isosubdivision': assignment.get('IsoSubdivision','') or '',\n                    'teams_numbertype': assignment.get('NumberType','') or '',\n                    'teams_pstnassignmentstatus': assignment.get('PstnAssignmentStatus','') or '',\n                }\n            teams_phone_assignments.append(formatted_assignment)\n        return teams_phone_assignments\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>Explanation: - Builds a dictionary of users for fast lookup. - Calls <code>get_teams_phone_numbers</code> to retrieve phone assignments (see next step). - Combines user and phone assignment data into a unified list for reporting or export.</p>"},{"location":"teams-user-number-assignment/#step-2-retrieve-teams-phone-numbers-via-powershell","title":"Step 2: Retrieve Teams Phone Numbers via PowerShell","text":""},{"location":"teams-user-number-assignment/#function-get_teams_phone_numbers","title":"Function: <code>get_teams_phone_numbers</code>","text":"<p>This function (not shown in full here) typically uses Python's <code>subprocess</code> module to invoke a PowerShell script or command that retrieves Teams phone number assignments. The PowerShell script should output data in a format that Python can parse (e.g., JSON or CSV).</p> <p>Example (conceptual):</p> <pre><code>import subprocess\nimport json\n\ndef get_teams_phone_numbers():\n    # Example PowerShell command to get Teams phone assignments as JSON\n    ps_command = [\n        'pwsh', '-Command',\n        'Import-Module MicrosoftTeams; Get-CsPhoneNumberAssignment | ConvertTo-Json'\n    ]\n    result = subprocess.run(ps_command, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"PowerShell error: {result.stderr}\")\n    return json.loads(result.stdout)\n</code></pre> <p>Explanation: - Uses PowerShell to access Teams telephony data not available via Graph API. - Returns a list of phone assignment dictionaries for further processing in Python.</p>"},{"location":"teams-user-number-assignment/#step-3-export-or-store-the-results","title":"Step 3: Export or Store the Results","text":"<p>After combining the data, you can export the results to CSV or store them in a database for reporting or compliance purposes.</p>"},{"location":"teams-user-number-assignment/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can programmatically retrieve and correlate Teams phone number assignments with Entra (Azure AD) user data, using a combination of Python and PowerShell. This enables automated reporting, compliance, and inventory workflows for Teams telephony in your organization.</p> <p>For more details, see the Microsoft Teams PowerShell documentation and Microsoft Graph API documentation.</p>"},{"location":"tenantallowblocklist/","title":"Investigating a False Positive in O365 Tenant Allow/Block List: A Complete Technical Analysis","text":""},{"location":"tenantallowblocklist/#executive-summary","title":"Executive Summary","text":"<p>This article provides a detailed technical walkthrough of investigating and resolving an O365 Tenant Allow/Block List false positive that occurred when a user accidentally reported an internal email as phishing. We'll explore the investigation process, explain the unexpected behavior between internal and external email delivery, and demonstrate how to properly diagnose and resolve such issues in a hybrid ProofPoint Essentials + O365 environment.</p>"},{"location":"tenantallowblocklist/#environment-overview","title":"Environment Overview","text":"<p>Email Security Architecture: - External Email Flow: Internet \u2192 ProofPoint Essentials \u2192 O365 \u2192 End User - Internal Email Flow: Internal User \u2192 O365 \u2192 End User - Security Stack: ProofPoint Essentials (primary external filter) + O365 Defender for Office 365</p> <p>Key Systems: - Microsoft 365 Defender Portal: <code>https://security.microsoft.com</code> - Exchange Online PowerShell - ProofPoint Essentials Portal - Microsoft Purview Compliance Portal: <code>https://compliance.microsoft.com</code></p>"},{"location":"tenantallowblocklist/#the-incident-timeline","title":"The Incident Timeline","text":""},{"location":"tenantallowblocklist/#initial-discovery","title":"Initial Discovery","text":"<p>Time: 9:00 AM CST, July 17, 2025 Issue: User <code>john.doe@yourdomain.com</code> could not receive internal emails Symptoms: Internal emails bouncing, external emails delivering normally</p>"},{"location":"tenantallowblocklist/#investigation-phase-1-identifying-the-root-cause","title":"Investigation Phase 1: Identifying the Root Cause","text":"<p>The first step was to check the O365 Tenant Allow/Block List for any entries related to the affected user.</p> <p>Navigation: Microsoft 365 Defender Portal \u2192 Email &amp; collaboration \u2192 Policies &amp; rules \u2192 Threat policies \u2192 Tenant Allow/Block List</p> <pre><code># Connect to Exchange Online\nConnect-ExchangeOnline -Device\n\n# Check current Tenant Allow/Block List entries\nGet-TenantAllowBlockListItems -ListType Sender | Select-Object Value, Action, CreatedDateTime, LastModifiedDateTime, Notes, CreatedBy, ModifiedBy\n</code></pre> <p>Key Finding: The user <code>john.doe@yourdomain.com</code> was found on the block list with: - CreatedDateTime: July 17, 2025 2:07:06 PM UTC (8:07:06 AM CST) - ModifiedBy: [Blank] - indicating system-generated entry - Action: Block</p>"},{"location":"tenantallowblocklist/#investigation-phase-2-audit-log-analysis","title":"Investigation Phase 2: Audit Log Analysis","text":"<p>To understand how the user ended up on the block list, we needed to examine the audit logs.</p> <pre><code># Search for Tenant Allow/Block List activities on July 17th\nSearch-UnifiedAuditLog -StartDate \"2025-07-17 00:00:00\" -EndDate \"2025-07-17 23:59:59\" -Operations \"New-TenantAllowBlockListItems\" -ResultSize 1000\n</code></pre> <p>Critical Discovery: The audit log revealed:</p> <pre><code>{\n  \"CreationTime\": \"2025-07-17T14:07:06\",\n  \"Operation\": \"New-TenantAllowBlockListItems\",\n  \"UserId\": \"NT AUTHORITY\\\\SYSTEM (Microsoft.Exchange.AdminApi.NetCore)\",\n  \"Parameters\": [\n    {\"Name\": \"ListSubType\", \"Value\": \"Submission\"},\n    {\"Name\": \"Block\", \"Value\": \"True\"},\n    {\"Name\": \"Entries\", \"Value\": \"john.doe@yourdomain.com\"},\n    {\"Name\": \"SubmissionID\", \"Value\": \"8c3cd19d-e1fb-4095-fa0a-08ddc53afdd2\"},\n    {\"Name\": \"ListType\", \"Value\": \"Sender\"},\n    {\"Name\": \"ExpirationDate\", \"Value\": \"8/15/2025 6:30:00 PM\"},\n    {\"Name\": \"SubmissionUserId\", \"Value\": \"admin.user@yourdomain.com\"}\n  ]\n}\n</code></pre> <p>Root Cause Identified:  - Trigger: User <code>admin.user@yourdomain.com</code> reported an email as phishing using Outlook's \"Report Message\" feature - System Response: Microsoft's automated system analyzed the submission and incorrectly added the recipient to the block list instead of evaluating the sender - ListSubType: \"Submission\": Confirms this was from a user report, not manual admin action</p>"},{"location":"tenantallowblocklist/#investigation-phase-3-understanding-the-anomalous-behavior","title":"Investigation Phase 3: Understanding the Anomalous Behavior","text":"<p>The most puzzling aspect was why internal emails were blocked while external emails continued to deliver normally. This behavior seemed inconsistent with how the Tenant Allow/Block List should function.</p>"},{"location":"tenantallowblocklist/#message-trace-analysis","title":"Message Trace Analysis","text":"<pre><code># Check for failed internal emails\nGet-MessageTrace -RecipientAddress \"john.doe@yourdomain.com\" -StartDate \"2025-07-17 00:00:00\" -EndDate \"2025-07-17 23:59:59\" -Status \"Failed\" | Select-Object Received, SenderAddress, Subject, Status, MessageTraceId\n</code></pre> <p>Results: Multiple internal emails failed after 8:07 AM CST:</p> <pre><code>Received             SenderAddress                    Subject                Status\n--------             -------------                    -------                ------\n7/17/2025 9:35:35 PM internal.user@yourdomain.com    Testing - ignore       Failed\n7/17/2025 9:30:58 PM manager@yourdomain.com          Daily Stats           Failed\n7/17/2025 9:29:08 PM hr@yourdomain.com               Test Email            Failed\n</code></pre>"},{"location":"tenantallowblocklist/#detailed-failure-analysis","title":"Detailed Failure Analysis","text":"<pre><code># Get detailed trace for specific failed message\nGet-MessageTraceDetail -MessageTraceId \"990b654e-bcad-489d-66b1-08ddc579ddd4\" -RecipientAddress \"john.doe@yourdomain.com\"\n</code></pre> <p>Critical Error Message: <pre><code>Event: Fail\nReason: [{LED=550 5.7.703 Your message can't be delivered because messages to john.doe@yourdomain.com...\n</code></pre></p> <p>Error Code Analysis: <code>550 5.7.703</code> is the specific error code for Tenant Allow/Block List blocking, confirming the block list was the cause.</p>"},{"location":"tenantallowblocklist/#external-email-analysis","title":"External Email Analysis","text":"<pre><code># Check if external emails were delivered successfully after the block\nGet-MessageTrace -RecipientAddress \"john.doe@yourdomain.com\" -StartDate \"2025-07-17 08:07:00\" -EndDate \"2025-07-17 23:59:59\" -Status \"Delivered\" | Where-Object {$_.SenderAddress -notlike \"*@yourdomain.com\"}\n</code></pre> <p>Surprising Results: External emails were successfully delivered:</p> <pre><code>Received             SenderAddress                        Subject\n--------             -------------                        -------\n7/17/2025 11:34:38 PM noreply@vendor.com                 Flight Details\n7/17/2025 10:48:58 PM shop@retailer.com                  Promotional Email\n7/17/2025 1:39:06 PM  partner@external.com               Business Communication\n</code></pre>"},{"location":"tenantallowblocklist/#investigation-phase-4-proofpoint-connector-analysis","title":"Investigation Phase 4: ProofPoint Connector Analysis","text":"<p>The anomalous behavior led us to examine the ProofPoint integration with O365.</p> <pre><code># Check for ProofPoint connectors\nGet-InboundConnector | Where-Object {$_.Name -like \"*proof*\" -or $_.ConnectorSource -like \"*proof*\"}\n</code></pre> <p>Discovery:  <pre><code>Name                         SenderDomains SenderIPAddresses           Enabled\n----                         ------------- -----------------           -------\nProofpoint Inbound connector {smtp:*;1}    {67.231.149.0/24, ...}     True\n</code></pre></p> <pre><code># Get detailed connector configuration\nGet-InboundConnector \"Proofpoint Inbound connector\" | Format-List\n</code></pre> <p>Key Configuration Properties: - SenderDomains: <code>{smtp:*;1}</code> - Accepts from any domain - SenderIPAddresses: Restricted to ProofPoint IP ranges - RestrictDomainsToIPAddresses: True - Only accepts from trusted IPs - CloudServicesMailEnabled: True - Enables certain bypass capabilities</p>"},{"location":"tenantallowblocklist/#technical-explanation-why-the-behavior-occurred","title":"Technical Explanation: Why the Behavior Occurred","text":""},{"location":"tenantallowblocklist/#email-flow-architecture-analysis","title":"Email Flow Architecture Analysis","text":"<p>Internal Email Flow: <pre><code>Internal Sender \u2192 O365 Mail Flow Rules \u2192 Tenant Allow/Block List Check \u2192 Recipient\n                                              \u2193\n                                         BLOCKED (550 5.7.703)\n</code></pre></p> <p>External Email Flow: <pre><code>External Sender \u2192 ProofPoint Essentials \u2192 Trusted Inbound Connector \u2192 Recipient\n                                                    \u2193\n                                              BYPASSES Tenant Allow/Block List\n</code></pre></p>"},{"location":"tenantallowblocklist/#the-technical-explanation","title":"The Technical Explanation","text":"<p>The ProofPoint Inbound connector is configured as a trusted source that bypasses standard O365 mail flow rules, including the Tenant Allow/Block List. This is intentional design because:</p> <ol> <li>ProofPoint serves as the primary external threat filter</li> <li>Trusted connectors have elevated permissions to bypass certain O365 security checks</li> <li>Internal emails still flow through standard O365 routing where security policies apply</li> </ol> <p>This explains why: - \u2705 External emails continued to deliver (via ProofPoint's trusted connector) - \u274c Internal emails were blocked (via standard O365 mail flow)</p>"},{"location":"tenantallowblocklist/#resolution-steps","title":"Resolution Steps","text":""},{"location":"tenantallowblocklist/#step-1-immediate-remediation","title":"Step 1: Immediate Remediation","text":"<pre><code># Remove the user from the block list\nRemove-TenantAllowBlockListItems -ListType Sender -Entries \"john.doe@yourdomain.com\"\n</code></pre>"},{"location":"tenantallowblocklist/#step-2-comprehensive-impact-assessment","title":"Step 2: Comprehensive Impact Assessment","text":"<pre><code># Generate comprehensive report of all blocked emails\n$blockedEmails = Get-MessageTrace -RecipientAddress \"john.doe@yourdomain.com\" -StartDate \"2025-07-17 08:07:00\" -EndDate \"2025-07-17 23:59:59\" -Status \"Failed\" | Where-Object {$_.SenderAddress -like \"*@yourdomain.com\"}\n\n# Export for documentation\n$blockedEmails | Select-Object @{Name=\"Time_CST\";Expression={$_.Received.AddHours(-5)}}, SenderAddress, Subject, Status, MessageTraceId | Export-Csv -Path \"C:\\temp\\BlockedEmails_Impact_Assessment.csv\" -NoTypeInformation\n\nWrite-Host \"Total internal emails blocked: $($blockedEmails.Count)\"\n</code></pre>"},{"location":"tenantallowblocklist/#step-3-verification","title":"Step 3: Verification","text":"<pre><code># Verify removal from block list\nGet-TenantAllowBlockListItems -ListType Sender | Where-Object {$_.Value -like \"*john.doe*\"}\n</code></pre>"},{"location":"tenantallowblocklist/#lessons-learned-and-preventive-measures","title":"Lessons Learned and Preventive Measures","text":""},{"location":"tenantallowblocklist/#technical-insights","title":"Technical Insights","text":"<ol> <li>User Reports Can Cause False Positives: The \"Report Message\" feature can incorrectly target recipients instead of malicious senders</li> <li>Trusted Connectors Bypass Security Rules: ProofPoint's trusted connector configuration explains the asymmetric behavior</li> <li>Audit Logs Are Critical: The <code>ListSubType: \"Submission\"</code> parameter was key to identifying the root cause</li> </ol>"},{"location":"tenantallowblocklist/#recommended-preventive-actions","title":"Recommended Preventive Actions","text":"<ol> <li>User Training: Educate users on proper use of \"Report Message\" functionality</li> <li>Monitoring: Implement alerts for Tenant Allow/Block List changes</li> <li>Review Process: Consider implementing approval workflows for user-reported submissions</li> </ol>"},{"location":"tenantallowblocklist/#powershell-monitoring-script","title":"PowerShell Monitoring Script","text":"<pre><code># Daily monitoring script for Tenant Allow/Block List changes\n$yesterday = (Get-Date).AddDays(-1)\n$today = Get-Date\n\n$changes = Search-UnifiedAuditLog -StartDate $yesterday -EndDate $today -Operations \"New-TenantAllowBlockListItems\",\"Set-TenantAllowBlockListItems\",\"Remove-TenantAllowBlockListItems\" -ResultSize 5000\n\nif ($changes) {\n    Write-Host \"Tenant Allow/Block List changes detected:\" -ForegroundColor Yellow\n    $changes | ForEach-Object {\n        $details = $_.AuditData | ConvertFrom-Json\n        Write-Host \"Time: $($_.CreationDate) | Operation: $($_.Operations) | User: $($_.UserIds)\" -ForegroundColor Cyan\n    }\n} else {\n    Write-Host \"No Tenant Allow/Block List changes in the last 24 hours.\" -ForegroundColor Green\n}\n</code></pre>"},{"location":"tenantallowblocklist/#conclusion","title":"Conclusion","text":"<p>This incident highlighted the importance of understanding complex email security architectures and the interactions between different security layers. The false positive was quickly identified and resolved through systematic investigation using PowerShell and audit logs.</p> <p>Key Takeaways: - Audit logs provide crucial forensic information for security incidents - Trusted connectors can create asymmetric email flow behavior - User education is essential for preventing false positive security reports - Systematic investigation methodology is critical for complex email issues</p> <p>The resolution restored normal email flow while maintaining the security benefits of both ProofPoint Essentials and O365 Defender integration.</p> <p>This article demonstrates real-world troubleshooting techniques for O365 and ProofPoint environments. All domain names and user identities have been anonymized for security purposes.</p>"},{"location":"ukg-api-employee-verification/","title":"Validating Employee Data Consistency Between UKG and Your HR System","text":""},{"location":"ukg-api-employee-verification/#introduction","title":"Introduction","text":"<p>When integrating HR data between systems such as ADP (or any HRIS) and UKG Dimensions, it's critical to ensure that the data loaded into UKG matches the source-of-truth in your HR system. This is especially important when using middleware (like Dell Boomi) for automated data loads. This article provides a step-by-step, company-agnostic guide to:</p> <ul> <li>Connect securely to the UKG API</li> <li>Retrieve employee data from UKG</li> <li>Retrieve employee data from your HR system (e.g., ADP)</li> <li>Compare the two datasets for validation</li> <li>Report discrepancies for remediation</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment.</p>"},{"location":"ukg-api-employee-verification/#why-validate-data-after-integration","title":"Why Validate Data After Integration?","text":"<p>Automated integrations (e.g., via Dell Boomi) can occasionally result in mismatches due to mapping errors, transformation issues, or upstream data changes. Validating data post-load ensures:</p> <ul> <li>Data integrity between systems</li> <li>Early detection of integration or mapping issues</li> <li>Compliance with audit requirements</li> <li>Improved trust in downstream business processes</li> </ul>"},{"location":"ukg-api-employee-verification/#solution-overview","title":"Solution Overview","text":"<ol> <li>Connect to UKG API: Use secure credentials (ideally from Azure Key Vault or similar) to authenticate and retrieve employee data from UKG.</li> <li>Connect to HR System: Query your HR system (e.g., ADP) for the same set of employees.</li> <li>Compare Data: Match employees by a unique identifier (e.g., email or employee ID) and compare key fields.</li> <li>Report Results: Output a report of discrepancies for review and correction.</li> </ol>"},{"location":"ukg-api-employee-verification/#step-1-securely-connect-to-the-ukg-api","title":"Step 1: Securely Connect to the UKG API","text":"<p>UKG Dimensions provides a REST API for programmatic access. Authentication typically uses OAuth2 with client credentials. Credentials should be stored securely (e.g., Azure Key Vault).</p> <pre><code>import requests\nimport urllib.parse\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\ndef get_ukg_environment_credentials(environment):\n    if environment == 'PROD':\n        return (\n            get_azure_kv_sceret('ukg-base-uri'),\n            get_azure_kv_sceret('ukg-api-username'),\n            get_azure_kv_sceret('ukg-api-password'),\n            get_azure_kv_sceret('ukg-api-key'),\n            get_azure_kv_sceret('ukg-api-client-id'),\n            get_azure_kv_sceret('ukg-api-client-secret')\n        )\n    else:\n        # Use your non-production secrets\n        ...\n\ndef get_token_apikey_and_uri(environment):\n    base_uri, username, password, api_key, client_id, client_secret = get_ukg_environment_credentials(environment)\n    access_token_uri = base_uri + 'api/authentication/access_token'\n    payload = (\n        f\"username={urllib.parse.quote(username)}&amp;\"\n        f\"password={urllib.parse.quote(password)}&amp;\"\n        f\"client_id={urllib.parse.quote(client_id)}&amp;\"\n        f\"client_secret={urllib.parse.quote(client_secret)}&amp;\"\n        \"grant_type=password&amp;auth_chain=OAuthLdapService\"\n    )\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'appkey': api_key\n    }\n    response = requests.post(access_token_uri, headers=headers, data=payload)\n    response.raise_for_status()\n    response_json = response.json()\n    return response_json[\"access_token\"], base_uri, api_key\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The function requests an OAuth2 access token from UKG. - The token is used for all subsequent API calls.</p>"},{"location":"ukg-api-employee-verification/#step-2-retrieve-employee-data-from-ukg","title":"Step 2: Retrieve Employee Data from UKG","text":"<p>Once authenticated, you can call the UKG API to retrieve employee details. The following function demonstrates how to fetch all employees and their details:</p> <pre><code>import requests\n\ndef get_all_employees(environment):\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    employees = []\n    # Example endpoint for listing employees (adjust as needed for your UKG tenant)\n    url = base_uri + 'api/v1/commons/persons'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    employees = response.json().get('persons', [])\n    return employees\n</code></pre> <p>Explanation: - Calls the UKG API endpoint to list all employees. - Returns a list of employee records as dictionaries.</p>"},{"location":"ukg-api-employee-verification/#step-3-retrieve-employee-data-from-your-hr-system-eg-adp","title":"Step 3: Retrieve Employee Data from Your HR System (e.g., ADP)","text":"<p>Assuming you have a database or API access to your HR system, you can retrieve employee data for comparison. Here is a sample function for fetching from a SQL database:</p> <pre><code>def fetch_users_from_hr():\n    sql_query = \"\"\"\n        SELECT email, position_id, status, associate_id, last_name, first_name, location, pay_rate_code, reports_to\n        FROM hr_employees\n    \"\"\"\n    # Replace with your actual DB query logic\n    results = execute_Select_SQL_statement(sql_query)[0]\n    return {\n        row[0].lower(): {\n            \"position_id\": row[1],\n            \"status\": row[2],\n            \"associate_id\": row[3],\n            \"last_name\": row[4],\n            \"first_name\": row[5],\n            \"location\": row[6],\n            \"pay_rate_code\": row[7],\n            \"reports_to\": row[8]\n        } for row in results\n    }\n</code></pre> <p>Explanation: - Queries the HR system for employee data. - Returns a dictionary keyed by email for easy lookup.</p>"},{"location":"ukg-api-employee-verification/#step-4-compare-and-validate-employee-data","title":"Step 4: Compare and Validate Employee Data","text":"<p>Now, compare the two datasets and report any discrepancies. Here is a function that does this and saves the results to a CSV file:</p> <pre><code>def compare_ukg_and_hr_employees(ukg_employees, hr_employees):\n    processed_details = []\n    for employee in ukg_employees:\n        username = employee.get('user', {}).get('userAccount', {}).get('userName', None)\n        if username and username.lower() in hr_employees:\n            hr_user = hr_employees[username.lower()]\n            # Compare fields as needed\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': hr_user.get('status'),\n                # Add more fields as needed\n            })\n        else:\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': 'Not Found',\n            })\n    # Save to CSV for review\n    save_list_to_csv(processed_details, 'ukg_hr_comparison.csv')\n</code></pre> <p>Explanation: - For each UKG employee, attempts to find a match in the HR system by email/username. - Compares relevant fields and records the results. - Outputs a CSV file for review.</p>"},{"location":"ukg-api-employee-verification/#step-5-full-example-putting-it-all-together","title":"Step 5: Full Example \u2013 Putting It All Together","text":"<p>Here is a complete example that ties all the steps together:</p> <pre><code>def main(environment='PROD'):\n    ukg_employees = get_all_employees(environment)\n    hr_employees = fetch_users_from_hr()\n    compare_ukg_and_hr_employees(ukg_employees, hr_employees)\n    print(\"Comparison complete. Results saved to ukg_hr_comparison.csv.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-api-employee-verification/#benefits-of-post-load-validation","title":"Benefits of Post-Load Validation","text":"<ul> <li>Data Quality: Ensures that the data loaded into UKG matches your HR system.</li> <li>Early Issue Detection: Quickly identifies mapping or integration errors.</li> <li>Audit Readiness: Provides evidence of data integrity for compliance.</li> <li>Continuous Improvement: Enables ongoing monitoring and process improvement.</li> </ul>"},{"location":"ukg-api-employee-verification/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the validation of employee data between UKG and your HR system, regardless of your integration platform. This approach is scalable, secure, and adaptable to any enterprise environment.</p> <p>For further enhancements, consider automating the process to run after each integration cycle and integrating with your alerting or ticketing system for proactive remediation.</p>"},{"location":"ukg-api-integration-and-dataview/","title":"Automating UKG Dimensions Integrations and DataView Exports with Python","text":""},{"location":"ukg-api-integration-and-dataview/#introduction","title":"Introduction","text":"<p>UKG Dimensions (formerly Kronos) provides powerful APIs for automating data extraction and integration tasks. This article demonstrates how to:</p> <ul> <li>Programmatically execute a predefined integration (such as a payroll export) in UKG Dimensions and retrieve the output file.</li> <li>Programmatically extract data from an existing DataView using a Hyperfind query.</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment. This guide is company-agnostic and can be adapted to your own UKG tenant.</p>"},{"location":"ukg-api-integration-and-dataview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to UKG Dimensions APIs (with appropriate permissions)</li> <li>A predefined integration (e.g., Payroll Export) already set up in your UKG tenant</li> <li>An existing DataView in UKG Dimensions</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Secure storage for API credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#1-executing-a-predefined-integration-in-ukg-dimensions","title":"1. Executing a Predefined Integration in UKG Dimensions","text":"<p>UKG Dimensions allows you to define integrations (such as payroll exports) via the UI. These integrations can be triggered and monitored via API, and the resulting files can be downloaded programmatically.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_payroll_hours","title":"Python Function: <code>fetch_and_store_payroll_hours</code>","text":"<pre><code>def fetch_and_store_payroll_hours(environment, week_start, week_end, week_start_datetime, week_end_datetime):\n    import uuid, json, time, csv\n    from io import StringIO\n    # ... import your secret and DB utilities ...\n    unique_id = 'Automation-' + str(uuid.uuid4())\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    # 1. Trigger the integration\n    dimensions_api_uri = base_uri + 'api/v1/platform/integrations/4/execute'\n    payload = json.dumps({\n        \"integrationParameters\": [\n            {\"name\": \"Symbolic Period\", \"value\": {'symbolicPeriod': {'id': '0'}, 'startDate': week_start + '.000Z', 'endDate': week_end + '.000Z'}},\n            {\"name\": \"Summary File Name\", \"value\": \"AutomationPayrollSummaryExport.csv\"},\n            {\"name\": \"Hyperfind ID\", \"value\": {'hyperfind': {'id': '1304'}}},\n            {\"name\": \"Ignore Sign Off\", \"value\": False},\n            {\"name\": \"File Name\", \"value\": \"automationpayrollexport.csv\"}\n        ],\n        \"name\": unique_id\n    })\n    response = requests.post(dimensions_api_uri, headers=headers, data=payload).json()\n    # 2. Poll for completion\n    execution_id = response['id']\n    status_url = base_uri + f'api/v1/platform/integration_executions/{execution_id}'\n    while True:\n        status_response = requests.get(status_url, headers=headers).json()\n        if status_response['status'] == 'Completed':\n            break\n        time.sleep(60)  # Wait before polling again\n    # 3. Download the output file\n    file_url = status_url + '/file'\n    params = {'file_name': 'automationpayrollexport.csv'}\n    file_response = requests.get(file_url, headers=headers, params=params)\n    data_file = StringIO(file_response.text)\n    csv_reader = csv.DictReader(data_file)\n    data_list = list(csv_reader)\n    # ... process and store data as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation","title":"Explanation","text":"<ul> <li>Trigger Integration: The function sends a POST request to the integration execution endpoint, passing required parameters (dates, file names, hyperfind, etc.).</li> <li>Poll for Completion: The function polls the execution status endpoint until the integration is complete.</li> <li>Download Output: Once complete, the output file is downloaded and parsed as CSV.</li> <li>Processing: The data can then be processed or loaded into a database as needed.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 Integrations</p>"},{"location":"ukg-api-integration-and-dataview/#2-extracting-data-from-a-dataview-using-a-hyperfind-query","title":"2. Extracting Data from a DataView Using a Hyperfind Query","text":"<p>DataViews in UKG Dimensions allow you to define custom reports. You can extract data from a DataView programmatically using the API and a Hyperfind query to filter employees.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_hours_using_dataview","title":"Python Function: <code>fetch_and_store_hours_using_dataview</code>","text":"<pre><code>def fetch_and_store_hours_using_dataview(environment, week_start_datetime, week_end_datetime):\n    import json, time, csv, os\n    # ... import your secret and DB utilities ...\n    pay_code_translation = { 'Regular': 'REG', 'Overtime 1.5': 'OT', 'Doubletime': 'DBL', ... }\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    export_url = base_uri + 'api/v1/commons/exports/async'\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    payload = json.dumps({\n        \"name\": \"UKG DV Export Pay Code\",\n        \"payLoad\": {\n            \"from\": {\n                \"view\": 0,\n                \"employeeSet\": {\n                    \"hyperfind\": {\"id\": \"1304\"},\n                    \"dateRange\": {\n                        \"startDate\": week_start_datetime.strftime(\"%Y-%m-%d\"),\n                        \"endDate\": week_end_datetime.strftime(\"%Y-%m-%d\"),\n                    }\n                },\n                \"viewPresentation\": \"People\"\n            },\n            \"select\": [\n                {\"key\": \"PEOPLE_PERSON_NUMBER\", \"alias\": \"Employee ID\", ...},\n                {\"key\": \"CORE_PAYCODE\", \"alias\": \"Pay Code Name\", ...},\n                {\"key\": \"TIMECARD_TRANS_ACTUAL_HOURS\", \"alias\": \"Actual Hours\", ...},\n                # ... more fields ...\n            ],\n            \"groupBy\": [],\n            \"where\": [],\n        },\n        \"type\": \"DATA\"\n    })\n    # 1. Trigger DataView export\n    response = requests.post(export_url, headers=headers, data=payload)\n    execution_key = response.json()['executionKey']\n    # 2. Wait for export to complete\n    time.sleep(60)\n    # 3. Download the CSV file\n    csv_export_url = base_uri + f'api/v1/commons/exports/{execution_key}/file'\n    response = requests.get(csv_export_url, headers=headers)\n    temp_file_path = '/tmp/paycodeextracttempdetail.csv'\n    with open(temp_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(response.text)\n    # ... process CSV as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_1","title":"Explanation","text":"<ul> <li>Trigger DataView Export: Sends a POST request to the DataView export endpoint with the required payload (including Hyperfind and date range).</li> <li>Wait for Completion: Waits for the export to complete (can be improved with polling).</li> <li>Download CSV: Downloads the resulting CSV file for further processing.</li> <li>Processing: The CSV can be parsed and loaded into a database or used for reporting.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 DataViews</p>"},{"location":"ukg-api-integration-and-dataview/#3-orchestrating-the-process-process_and_validate_payroll_hours","title":"3. Orchestrating the Process: <code>process_and_validate_payroll_hours</code>","text":"<p>This function coordinates the two previous steps, automating the extraction of both payroll export and DataView data for a given period. It uses the <code>fetch_period</code> function to retrieve the start and end dates for both the prior and current pay periods, and then passes these as parameters to the extraction functions.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-process_and_validate_payroll_hours-with-prior_period-and-current_period-parameters","title":"Python Function: <code>process_and_validate_payroll_hours</code> (with <code>prior_period</code> and <code>current_period</code> parameters)","text":"<pre><code>def process_and_validate_payroll_hours(environment='PROD'):\n    \"\"\"\n    Main function to process and validate payroll hours.\n    Steps:\n    1. Fetches the prior period data and stores payroll hours.\n    2. Fetches and stores hours using dataview for the prior period.\n    3. Fetches the current period data and stores hours using dataview.\n    \"\"\"\n    # Get prior period (returns tuple: start_date, end_date, start_datetime, end_datetime)\n    prior_period = fetch_period('Previous', environment)\n    # Extract and store payroll hours for prior period\n    fetch_and_store_payroll_hours(environment, *prior_period)\n    # Extract and store DataView hours for prior period\n    fetch_and_store_hours_using_dataview(environment, prior_period[2], prior_period[3])\n\n    # Get current period\n    current_period = fetch_period('Current', environment)\n    # Extract and store DataView hours for current period\n    fetch_and_store_hours_using_dataview(environment, current_period[2], current_period[3])\n\n    # Optionally, add validation or reporting here\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#about-prior_period-and-current_period","title":"About <code>prior_period</code> and <code>current_period</code>","text":"<p>The <code>fetch_period</code> function returns a tuple for each period:</p> <ul> <li><code>start_date</code> (str): Start date of the pay period (e.g., '2025-06-01')</li> <li><code>end_date</code> (str): End date of the pay period (e.g., '2025-06-15')</li> <li><code>start_datetime</code> (datetime): Start date as a Python <code>datetime</code> object</li> <li><code>end_datetime</code> (datetime): End date as a Python <code>datetime</code> object</li> </ul> <p>These are used as parameters for the extraction functions:</p> <ul> <li><code>fetch_and_store_payroll_hours(environment, start_date, end_date, start_datetime, end_datetime)</code></li> <li><code>fetch_and_store_hours_using_dataview(environment, start_datetime, end_datetime)</code></li> </ul> <p>This approach ensures that all data extraction is aligned to the correct pay periods, and makes it easy to extend the process for additional periods or custom ranges.</p>"},{"location":"ukg-api-integration-and-dataview/#example-usage","title":"Example Usage","text":"<pre><code>if __name__ == \"__main__\":\n    process_and_validate_payroll_hours(environment=\"PROD\")\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_2","title":"Explanation","text":"<ul> <li>Fetch Periods: Retrieves the date ranges for the previous and current pay periods using <code>fetch_period</code>.</li> <li>Extract Data: Calls the two extraction functions for each period, passing the correct parameters.</li> <li>Validation: (Optional) You can add logic to compare and validate the extracted data.</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#conclusion","title":"Conclusion","text":"<p>By leveraging the UKG Dimensions API, you can automate the execution of predefined integrations and the extraction of DataView data. This enables robust, repeatable, and auditable data flows for payroll, compliance, and analytics.</p> <p>For more details, consult the official UKG Dimensions API Documentation or your UKG support representative.</p>"},{"location":"ukg-sftp-file-transfer/","title":"Secure File Transfer with UKG Dimensions SFTP","text":""},{"location":"ukg-sftp-file-transfer/#introduction","title":"Introduction","text":"<p>UKG Dimensions (Kronos) provides SFTP endpoints for secure file exchange. For additional security, files are often encrypted (e.g., with PGP/GPG) before upload and must be decrypted after download. This article demonstrates how to:</p> <ul> <li>Connect to a UKG SFTP server using Python</li> <li>Download and decrypt files from UKG</li> <li>Encrypt and upload files to UKG</li> <li>Use Azure Storage (or any local mount) as your working directory</li> <li>Securely manage all credentials and keys using Azure Key Vault</li> <li>Import and manage GPG keys for file encryption/decryption</li> </ul> <p>We use Python libraries such as <code>pysftp</code>, <code>paramiko</code>, <code>gnupg</code>, and Azure SDKs to accomplish these tasks.</p>"},{"location":"ukg-sftp-file-transfer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>The following Python packages:</li> <li><code>pysftp</code> (SFTP client)</li> <li><code>paramiko</code> (SSH key handling)</li> <li><code>python-gnupg</code> (PGP encryption/decryption)</li> <li><code>azure-identity</code>, <code>azure-keyvault-secrets</code> (Azure Key Vault access)</li> <li>Access to your UKG SFTP credentials and keys (public/private, passphrase)</li> <li>GPG/PGP keys for file encryption/decryption</li> <li>A local directory (e.g., Azure Storage mount) for file staging</li> <li>Azure Key Vault for Secrets: Store all sensitive credentials (SFTP username, private key, passphrase, GPG passphrase, etc.) in Azure Key Vault and retrieve them securely at runtime. This avoids hardcoding secrets in your code or environment variables.</li> </ul> <p>Install dependencies: <pre><code>pip install pysftp paramiko python-gnupg azure-identity azure-keyvault-secrets\n</code></pre></p>"},{"location":"ukg-sftp-file-transfer/#key-constants-and-their-secure-retrieval","title":"Key Constants and Their Secure Retrieval","text":"<p>All SFTP credentials and keys are securely retrieved from Azure Key Vault using the <code>get_azure_kv_sceret</code> function. Here are the main constants and how they are constructed:</p> <ul> <li>SFTP_UKG_DATA_HOST_NAME: The SFTP server hostname (e.g., <code>'your-ukg-sftp-host.com'</code>).</li> <li>SFTP_UKG_DATA_USER_NAME: The SFTP username.</li> <li>SFTP_UKG_PUBLIC_KEY: The SFTP server's public key, retrieved and decoded as bytes:   <pre><code>SFTP_UKG_PUBLIC_KEY = bytes(get_azure_kv_sceret('ukg-sftp-host-public-key'), encoding='utf-8')\n</code></pre></li> <li>SFTP_UKG_PRIVATE_KEY: The private key for SFTP authentication, retrieved as a base64-encoded string from Key Vault, then decoded to a PEM string:   <pre><code>SFTP_UKG_PRIVATE_KEY = base64.b64decode(get_azure_kv_sceret('ukg-sftp-host-private-key')).decode('utf-8')\n</code></pre> Why base64.b64decode? <p>When storing sensitive files like private keys in Azure Key Vault, it is common to first encode them using Base64. This ensures the key is stored as a plain string (since Key Vault secrets are always strings) and avoids issues with special characters or line breaks. When retrieving the key, you must decode it back to its original binary (or PEM) format using <code>base64.b64decode</code>. This allows you to safely store and retrieve binary data (like private keys) in a text-only secret store.</p> </li> <li>SFTP_UKG_PRIVATE_KEY_PASSPHRASE: The passphrase for the private key, also retrieved from Key Vault.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#how-get_azure_kv_sceret-works","title":"How <code>get_azure_kv_sceret</code> Works","text":"<p>This function retrieves secrets from Azure Key Vault using the Azure SDK. It authenticates using environment variables for client ID, client secret, and tenant ID, then fetches the secret value by name:</p> <pre><code>def get_azure_kv_sceret(name):\n    secret = None\n    try:\n        vault_url = \"https://&lt;your-key-vault-name&gt;.vault.azure.net/\"\n        client_id = os.environ.get(\"application_interface_clientid\")\n        client_secret = os.environ.get(\"application_interface_clientsecret\")\n        tenant_id = os.environ.get(\"application_interface_tenantid\")\n        credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)\n        secret_client = SecretClient(vault_url=vault_url, credential=credential)\n        retrieved_secret = secret_client.get_secret(name)\n        secret = retrieved_secret.value\n    except Exception as e:\n        print(\"Error:\", str(e))\n    finally:\n        return secret\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#importing-and-managing-gpg-keys-for-ukg-file-encryptiondecryption","title":"Importing and Managing GPG Keys for UKG File Encryption/Decryption","text":"<p>When your container is first provisioned, you should import the GPG keys required for file encryption and decryption. The following function, typically run at container startup, retrieves the GPG public and private keys from Azure Key Vault, decodes them, saves them to disk, and imports them into the GPG keyring:</p> <pre><code>def download_and_import_gpg_keys():\n    try:\n        ukg_sftp_public_encrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-public-encrypt')).decode('utf-8')\n        ukg_sftp_private_decrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-private-decrypt')).decode('utf-8')\n        # Save public key\n        public_key_file = \"certs/public_ukg_encrypt.asc\"\n        with open(public_key_file, \"w\") as file:\n            file.write(ukg_sftp_public_encrypt)\n        # Save private key\n        private_key_file = \"certs/private_ukg_decrypt.asc\"\n        with open(private_key_file, \"w\") as file:\n            file.write(ukg_sftp_private_decrypt)\n        # Import the public key\n        subprocess.run([\"gpg\", \"--batch\", \"--yes\", \"--import\", public_key_file], check=True)\n        # Import the private key with passphrase\n        subprocess.run([\n            \"gpg\", \"--batch\", \"--yes\", \"--pinentry-mode=loopback\",\n            \"--passphrase\", os.environ[\"ukg_encrypt_passphrase\"],\n            \"--import\", private_key_file\n        ], check=True)\n        os.remove(public_key_file)\n        os.remove(private_key_file)\n    except Exception as e:\n        print(f\"Failed to import GPG keys: {e}\")\n</code></pre> <p>When to run this: - Run this function once at container startup (or as part of your provisioning script) to ensure the GPG keys are available for all encryption/decryption operations in your UKG SFTP workflows. - This ensures that all subsequent file transfers (upload/download) can use GPG seamlessly at the OS level.</p>"},{"location":"ukg-sftp-file-transfer/#sftp-connection-function-with-explanation","title":"SFTP Connection Function (with Explanation)","text":"<p>The following function establishes a secure SFTP connection to the UKG server using all the above constants. It supports both production and non-production environments:</p> <pre><code>import pysftp\nimport paramiko\nimport base64\nimport io\nimport warnings\n\ndef get_sftp_connection(environment: str) -&gt; Optional[pysftp.Connection]:\n    \"\"\"\n    Establish an SFTP connection to the UKG server.\n    Returns a pysftp.Connection object if successful, otherwise None.\n    \"\"\"\n    localConnection: Optional[pysftp.Connection] = None\n    try:\n        hostname = (SFTP_UKG_DATA_HOST_NAME if environment == 'PROD' else SFTP_UKG_DATA_HOST_NAME_NON_PROD)\n        username = (SFTP_UKG_DATA_USER_NAME if environment == 'PROD' else SFTP_UKG_DATA_USER_NAME_NON_PROD)\n        hostkey = (SFTP_UKG_PUBLIC_KEY if environment == 'PROD' else SFTP_UKG_PUBLIC_KEY_NON_PROD)\n        warnings.filterwarnings('ignore', '.*Failed to load HostKeys.*')\n        hostKey = paramiko.RSAKey(data=base64.decodebytes(hostkey))\n        cnopts = pysftp.CnOpts()\n        cnopts.hostkeys.add(hostname, 'ssh-rsa', hostKey)\n        # Convert private key string to file-like object\n        with io.StringIO(SFTP_UKG_PRIVATE_KEY) as private_key_file:\n            private_key = paramiko.RSAKey.from_private_key(private_key_file, password=SFTP_UKG_PRIVATE_KEY_PASSPHRASE)\n            localConnection = pysftp.Connection(host=hostname, username=username, private_key=private_key, cnopts=cnopts)\n    except Exception as e:\n        print(f\"SFTP connection failed: {e}\")\n    return localConnection\n</code></pre> <p>This function: - Retrieves all connection parameters and keys from Azure Key Vault. - Decodes and loads the SFTP server's public key for host verification. - Loads the private key and passphrase for authentication. - Returns a live SFTP connection object for use in upload/download operations.</p>"},{"location":"ukg-sftp-file-transfer/#downloading-and-decrypting-files-from-ukg","title":"Downloading and Decrypting Files from UKG","text":"<p>UKG may deliver files encrypted with PGP/GPG. Use <code>python-gnupg</code> to decrypt after download.</p> <pre><code>import gnupg\n\ngpg = gnupg.GPG()\nLOCAL_DOWNLOAD_DIR = '/mnt/azure/UKG/Download/'  # Example: Azure Storage mount\nREMOTE_UKG_FOLDER = './Outbound/'\n\ndef download_and_decrypt_files():\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(REMOTE_UKG_FOLDER)\n        for filename in sftp.listdir():\n            if filename.endswith('.gpg'):\n                local_path = os.path.join(LOCAL_DOWNLOAD_DIR, filename)\n                sftp.get(filename, local_path)\n                print(f\"Downloaded: {filename}\")\n                # Decrypt the file\n                with open(local_path, 'rb') as f:\n                    decrypted_data = gpg.decrypt_file(f, passphrase=os.environ[\"ukg_encrypt_passphrase\"])\n                if decrypted_data.ok:\n                    decrypted_path = local_path.replace('.gpg', '')\n                    with open(decrypted_path, 'w', encoding='utf-8') as out:\n                        out.write(str(decrypted_data))\n                    print(f\"Decrypted: {decrypted_path}\")\n                else:\n                    print(f\"Decryption failed for {filename}: {decrypted_data.status}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#encrypting-and-uploading-files-to-ukg","title":"Encrypting and Uploading Files to UKG","text":"<p>Before uploading, files must be encrypted with UKG's public key.</p> <pre><code>def encrypt_and_upload_file(local_file, remote_folder='./Inbound/'):\n    with open(local_file, 'rb') as f:\n        encrypted_data = gpg.encrypt_file(\n            f,\n            recipients=['UKG_PUBLIC_KEY_NAME'],  # Replace with UKG's GPG key name\n            always_trust=True\n        )\n    if not encrypted_data.ok:\n        print(f\"Encryption failed: {encrypted_data.status}\")\n        return\n    encrypted_file = local_file + '.gpg'\n    with open(encrypted_file, 'wb') as ef:\n        ef.write(encrypted_data.data)\n    print(f\"Encrypted: {encrypted_file}\")\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(remote_folder)\n        sftp.put(encrypted_file)\n        print(f\"Uploaded: {encrypted_file} to {remote_folder}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#putting-it-all-together","title":"Putting It All Together","text":"<p>You can automate the full workflow:</p> <pre><code>def main():\n    download_and_import_gpg_keys()  # Ensure GPG keys are imported at container startup\n    download_and_decrypt_files()\n    file_to_upload = '/mnt/azure/UKG/Upload/myfile.csv'\n    encrypt_and_upload_file(file_to_upload)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#key-points-and-best-practices","title":"Key Points and Best Practices","text":"<ul> <li>Key Management: Never hardcode sensitive keys or passphrases in your code. Use environment variables or a secure vault.</li> <li>File Cleanup: Remove decrypted/encrypted files after processing if not needed.</li> <li>Error Handling: Add robust error handling for production use.</li> <li>Azure Storage: If using Azure Files, ensure your container mounts the share with correct permissions.</li> <li>Security: Only trust files from known sources and validate signatures if possible.</li> <li>GPG Key Import: Always import GPG keys at the OS level before running any file encryption/decryption operations.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#references","title":"References","text":"<ul> <li>pysftp Documentation</li> <li>python-gnupg Documentation</li> <li>UKG Dimensions Integration Guides</li> <li>Azure Key Vault Documentation</li> </ul>"}]}