{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Resume","text":""},{"location":"#azure","title":"Azure","text":"<ul> <li>Certificate Based Authentication</li> <li>Download Subscription Bill</li> <li>Download Azure Resource Listing</li> <li>Query Recovery Services Vault</li> <li>Entra Users, User Groups and License assignments</li> <li>Entra User Devices</li> <li>Orchestrating Scheduled Jobs (Container Instances)</li> <li>DevOps Build Container</li> <li>DevOps Deploy Container IaC</li> <li>DevOps Deploy Container Github Actions</li> <li>Building and Publishing a Custom Image - Part 1</li> <li>Deploying Azure Virtual Desktop (AVD) Desktops - Part 2</li> <li>Cleanup Obsolete FSLogix Profiles</li> <li>Business Continuity Program / Disaster Recovery - Part 1</li> <li>Business Continuity Program / Disaster Recovery - Part 2</li> <li>Business Continuity Program / Disaster Recovery - Git Hub Action</li> <li>Restore VM from RSV Backup - Part 1</li> <li>Restore VM from RSV Backup - Part 2</li> </ul>"},{"location":"#fabric","title":"Fabric","text":"<ul> <li>Warehouse data import</li> </ul>"},{"location":"#cisco-meraki-nagios-xi-manage-engine","title":"Cisco Meraki, Nagios XI &amp; Manage Engine","text":"<ul> <li>Cisco Meraki Unused Devices</li> <li>Cisco Meraki Devices Discovery and Nagios Integration</li> <li>Integrating Monitoring System with ITSM System</li> <li>Deploying Cisco Meraki vMX</li> </ul>"},{"location":"#microsoft-365","title":"Microsoft 365","text":"<ul> <li>Outlook Actionable Adaptive Card - Part 1</li> <li>Outlook Actionable Adaptive Card - Part 2</li> <li>Sharepoint Sites Enumeration</li> <li>Sharepoint Document Library Enumeration</li> <li>Sharepoint Document Library Copy to Azure File Share</li> <li>Get all Teams Phone Assignment(s)</li> </ul>"},{"location":"#sap","title":"SAP","text":"<ul> <li>Setup PyRFC in your Container</li> <li>Concur Expense Report Aggregation</li> <li>Calling SAP RFC Function Modules from Python</li> </ul>"},{"location":"#ukg","title":"UKG","text":"<ul> <li>Secure File Transfer with UKG Dimensions</li> <li>Employee Verification via API</li> <li>Automating Integration Execution and Dataview extract</li> </ul>"},{"location":"#active-directory-domain-services","title":"Active Directory Domain Services","text":"<ul> <li>Create User and assign groups</li> <li>Update User</li> </ul>"},{"location":"#misc","title":"Misc.","text":"<ul> <li>Proof Point User Management</li> <li>Clean GitHub Deployments</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/","title":"Reviewing Consultants via Adaptive Card (Actionable Outlook Messages) \u2013 Part 1","text":"<p>In this article, we\u2019ll walk through a real-world Python implementation for reviewing consultants using Adaptive Cards in Outlook. This solution enables managers to receive an actionable email, review their consultants, and submit decisions directly from their inbox. We'll cover the end-to-end process, focusing on how to build and send an actionable Adaptive Card email using Python and Microsoft Graph.</p> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part1/#overview","title":"Overview","text":"<p>The workflow consists of the following steps:</p> <ol> <li>Fetch consultants grouped by manager using <code>fetch_manager_consultants</code>.</li> <li>Build an Adaptive Card for each manager using <code>create_adaptive_card_outlook</code>.</li> <li>Send the Adaptive Card email using <code>send_adaptive_card_email</code>.</li> </ol> <p>Let\u2019s dive into each step and the code behind it.</p>"},{"location":"adaptive-card-consultant-review-part1/#1-fetching-consultants-grouped-by-manager","title":"1. Fetching Consultants Grouped by Manager","text":"<p>The function <code>fetch_manager_consultants(frequency)</code> retrieves consultants from the database and groups them by their manager\u2019s email.</p> <pre><code>def fetch_manager_consultants(frequency):\n    \"\"\"Fetch consultants grouped by their manager's email.\"\"\"\n    get_consultants_sql_statement = get_consultants_sql(frequency)\n    consultants_data = execute_Select_SQL_statement(get_consultants_sql_statement)[0]\n    manager_to_consultants = {}\n\n    try:\n        for row in consultants_data:\n            manager_email = row[5]\n            consultant_info = {\n                \"in_adp\": row[0],\n                \"name\": row[1],\n                \"last_logon\": row[2],\n                \"email\": row[3],\n                \"last_password_change\": row[4],\n                \"hire_date\": row[6],\n            }\n            manager_to_consultants.setdefault(manager_email, []).append(consultant_info)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, Exception)\n\n    return manager_to_consultants\n</code></pre> <ul> <li>Key Points:</li> <li>The function queries the database for consultant data.</li> <li>It organizes consultants by their manager\u2019s email, returning a dictionary mapping each manager to their consultants.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#2-building-the-adaptive-card","title":"2. Building the Adaptive Card","text":"<p>The function <code>create_adaptive_card_outlook(manager_email, consultants)</code> constructs an Adaptive Card JSON payload for Outlook. This card allows managers to review each consultant and select an action (keep active or deactivate).</p> <pre><code>def create_adaptive_card_outlook(manager_email, consultants):\n    \"\"\"Create an Adaptive Card with consultant details and actions.\"\"\"\n    try:\n        manager_name = manager_email.split('@')[0].split('.')[0]  # Extract manager's first name\n        inputs = []\n        action_data = {}\n\n        for consultant in consultants:\n            consultant_id = consultant[\"email\"].replace(\"@\", \"_\").replace(\".\", \"_\")\n            last_logon = consultant['last_logon'] or 'N/A'\n            hire_date = consultant['hire_date'] or 'N/A'\n\n            inputs.extend([\n                {\n                    \"type\": \"TextBlock\",\n                    \"wrap\": True,\n                    \"weight\": \"Bolder\",\n                    \"color\": \"Warning\",\n                    \"spacing\": \"Medium\",\n                    \"text\": \"****\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"padding\": \"None\",\n                    \"spacing\": \"None\",\n                    \"items\": [\n                        {\n                            \"type\": \"TextBlock\",\n                            \"text\": f\"**{consultant['name']}** ({consultant['email']}), with Last Logon: {last_logon} and Hire Date: {hire_date}\",\n                            \"weight\": \"Bolder\",\n                            \"wrap\": True\n                        },\n                        {\n                            \"type\": \"Input.ChoiceSet\",\n                            \"id\": f\"decision_{consultant_id}\",\n                            \"isMultiSelect\": False,\n                            \"value\": \"keep\",\n                            \"choices\": [\n                                {\"title\": \"Keep Active\", \"value\": \"keep\"},\n                                {\"title\": \"Deactivate\", \"value\": \"deactivate\"}\n                            ],\n                            \"style\": \"expanded\",\n                            \"spacing\": \"None\",\n                        }\n                    ]\n                }\n            ])\n            action_data[consultant_id] = {\n                \"decision\": f\"{{{{decision_{consultant_id}.value}}}}\",\n                \"email\": consultant[\"email\"],\n                \"manageremail\": manager_email,\n                \"managername\": manager_name,\n            }\n\n        inputs.append({\n            \"type\": \"TextBlock\",\n            \"wrap\": True,\n            \"weight\": \"Bolder\",\n            \"color\": \"Warning\",\n            \"spacing\": \"Medium\",\n            \"text\": \"****\"\n        })\n\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.0\",\n            \"originator\": ORGINATOR_ID,\n            \"body\": [\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Consultant Review\",\n                    \"weight\": \"bolder\",\n                    \"size\": \"extraLarge\",\n                    \"color\": \"attention\",\n                    \"separator\": True,\n                    \"horizontalAlignment\": \"center\",\n                    \"spacing\": \"small\",\n                    \"wrap\": True\n                },\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": (\n                        f\"Hello {manager_name}, please review the details of your consultants and select the appropriate action. \"\n                        \"Some consultants may not be in the HR system as they were set up directly as Guest accounts, so their hire date will show as N/A. \"\n                        \"Please review all consultants and provide feedback so that appropriate action can be taken if they no longer require network access.\"\n                    ),\n                    \"wrap\": True,\n                    \"color\": \"Default\",\n                    \"spacing\": \"Medium\",\n                    \"weight\": \"Bolder\"\n                }\n            ] + inputs,\n            \"actions\": [\n                {\n                    \"type\": \"Action.Http\",\n                    \"title\": \"Submit Consultant Actions\",\n                    \"headers\": [\n                        {\"name\": \"Content-Type\", \"value\": \"application/json\"},\n                        {\"name\": \"Authorization\", \"value\": \"\"}\n                    ],\n                    \"method\": \"POST\",\n                    \"url\": \"https://api.example.com/consultant-review-confirmation\",\n                    \"body\": \"\"\n                }\n            ],\n            \"style\": \"default\"\n        }\n\n        # Prepare the action data for the body\n        action_data_str = json.dumps(action_data)\n        adaptive_card['actions'][0]['body'] = action_data_str\n\n        email_payload = {\n            \"message\": {\n            \"subject\": \"Consultant Review - Action Required\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n            \"bccRecipients\": [{\"emailAddress\": {\"address\": \"audit@example.com\"}}]\n            }\n        }\n\n        return email_payload\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        return None\n</code></pre> <ul> <li>Key Points:</li> <li>The card is dynamically built for each manager and their consultants.</li> <li>Each consultant has a choice set for the manager to select \"Keep Active\" or \"Deactivate\".</li> <li>The card is embedded in the email as a <code>&lt;script type='application/adaptivecard+json'&gt;...&lt;/script&gt;</code> block, which is required for actionable messages in Outlook.</li> <li>The card uses an <code>Action.Http</code> action to POST the manager\u2019s decisions to a specified endpoint.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#3-sending-the-adaptive-card-email","title":"3. Sending the Adaptive Card Email","text":"<p>The function <code>send_adaptive_card_email(email_payload)</code> sends the constructed Adaptive Card email using the Microsoft Graph API.</p> <pre><code>def send_adaptive_card_email(email_payload):\n    \"\"\"Send an email with an embedded Adaptive Card using Microsoft Graph API.\"\"\"\n    try:\n        user_id = \"your-user-guid\"\n        graph_api_url = f\"https://graph.microsoft.com/v1.0/users/{user_id}/sendMail\"\n        access_token = get_access_token_API_Access_AAD()\n\n        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n\n        response = requests.post(graph_api_url, json=email_payload, headers=headers)\n\n        if response.status_code == 202:\n            print(\"Email sent successfully!\")\n        else:\n            print(f\"Failed to send email: {response.status_code}, {response.text}\")\n\n    except requests.exceptions.RequestException as req_error:\n        handle_global_exception(sys._getframe().f_code.co_name, req_error)\n        print(f\"Request error occurred: {req_error}\")\n\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n        print(f\"An unexpected error occurred: {error}\")\n</code></pre> <ul> <li>Key Points:</li> <li>The function authenticates using an Azure AD access token.</li> <li>It sends the email via the Microsoft Graph <code>/sendMail</code> endpoint.</li> <li>The Adaptive Card is delivered as an actionable message in Outlook.</li> </ul>"},{"location":"adaptive-card-consultant-review-part1/#end-to-end-example","title":"End-to-End Example","text":"<p>Here\u2019s how you might orchestrate the process:</p> <pre><code>def process_consultants(frequency):\n    manager_to_consultants = fetch_manager_consultants(frequency)\n    for manager_email, consultants in manager_to_consultants.items():\n        email_payload = create_adaptive_card_outlook(manager_email, consultants)\n        if email_payload:\n            send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part1/#conclusion","title":"Conclusion","text":"<p>This article (Part 1) demonstrated how to:</p> <ul> <li>Fetch consultants grouped by manager.</li> <li>Build an Adaptive Card for actionable review in Outlook.</li> <li>Send the Adaptive Card email using Microsoft Graph.</li> </ul> <p>In Part 2, we\u2019ll cover how to process the manager\u2019s response when they submit the Adaptive Card. Read Part 2 \u2192</p>"},{"location":"adaptive-card-consultant-review-part2/","title":"Company-Agnostic Adaptive Card Consultant Review Blog (Part 2, Deep Dive)","text":""},{"location":"adaptive-card-consultant-review-part2/#introduction","title":"Introduction","text":"<p>In Part 1, we covered how to send actionable Adaptive Card emails for consultant review. In this Part 2, we focus on the backend: how to securely receive, verify, and process the manager's response when the Adaptive Card is submitted. This article recursively examines each function involved in the request processing chain, providing a complete, end-to-end understanding of how an incoming Adaptive Card request is handled\u2014with all relevant Python code included and all company-specific references replaced with generic placeholders (e.g., <code>mycompany.com</code>).</p>"},{"location":"adaptive-card-consultant-review-part2/#1-endpoint-consultant-review-confirmation","title":"1. Endpoint: <code>/consultant-review-confirmation</code>","text":"<p>When a manager submits the Adaptive Card, the card's action posts the data to the <code>/consultant-review-confirmation</code> endpoint:</p> <pre><code>@app.route('/consultant-review-confirmation', methods=['POST'])\ndef consultant_review_confirmation():\n    try:\n        payload, client_ip, error_response, status_code = process_request_headers_and_payload(request)\n        if error_response:\n            return error_response, status_code\n        process_adaptive_card_payload(payload, client_ip)\n        return jsonify({\"status\": \"success\", \"message\": \"Actions processed successfully\"}), 200\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>This route does two things: 1. Verifies the request and extracts the payload using <code>process_request_headers_and_payload</code>. 2. Processes the submitted data using <code>process_adaptive_card_payload</code>.</p>"},{"location":"adaptive-card-consultant-review-part2/#2-deep-dive-process_request_headers_and_payload","title":"2. Deep Dive: <code>process_request_headers_and_payload</code>","text":"<p>This function is responsible for: - Extracting and logging request headers. - Validating the JWT Bearer token in the <code>Action-Authorization</code> header. - Decoding the token and verifying its authenticity. - Extracting the JSON payload from the request.</p> <pre><code>def process_request_headers_and_payload(request):\n    headers = dict(request.headers)\n    logger.info(f\"Request headers: {headers}\")\n    action_auth_header = headers.get(\"Action-Authorization\", \"\")\n    client_ip = headers.get(\"X-Forwarded-For\", \"\")\n    logger.info(f\"Incoming request from IP: {client_ip}\")\n    logger.info(f\"Action Authorization: {action_auth_header}\")\n    if not action_auth_header.startswith(\"Bearer \"):\n        logger.error(f\"Missing or invalid Bearer token in Action-Authorization header from {client_ip}\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Missing Bearer token\"}), 401\n    token = action_auth_header.split(\" \", 1)[1]\n    log_jwt_payload(token)\n    public_key = fetch_public_key(token)\n    if not public_key:\n        logger.error(\"Public key not found!\")\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    if not validate_token(token, public_key):\n        return None, None, jsonify({\"error\": \"Unauthorized - Invalid Bearer token\"}), 401\n    payload = request.get_json()\n    logger.info(f\"Payload: {payload}\")\n    return payload, client_ip, None, None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#21-log_jwt_payloadtoken","title":"2.1. <code>log_jwt_payload(token)</code>","text":"<p>Logs the decoded JWT payload (without verifying the signature) for debugging and traceability.</p> <pre><code>def log_jwt_payload(token):\n    \"\"\"Logs the decoded JWT payload without verification.\"\"\"\n    payload = jwt.decode(token, options={\"verify_signature\": False})\n    for key, value in payload.items():\n        logger.info(f\"{key}: {value}\")\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#22-fetch_public_keytoken","title":"2.2. <code>fetch_public_key(token)</code>","text":"<p>Extracts the key ID (<code>kid</code>) from the JWT header, fetches the public keys from the identity provider's JWKS endpoint, and finds the matching key for signature verification.</p> <pre><code>def fetch_public_key(token):\n    \"\"\"Fetches the public key for the given token.\"\"\"\n    try:\n        header = jwt.get_unverified_header(token)\n        key_id = header.get(\"kid\")\n        jwks_url = 'https://substrate.office.com/sts/common/discovery/keys'  # Replace with your IdP's JWKS endpoint if needed\n        jwks = requests.get(jwks_url).json()\n        for key in jwks[\"keys\"]:\n            if key[\"kid\"] == key_id:\n                return RSAAlgorithm.from_jwk(json.dumps(key))\n    except Exception as e:\n        raise Exception(f\"Error fetching public key: {e}\")\n    return None\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#23-validate_tokentoken-public_key","title":"2.3. <code>validate_token(token, public_key)</code>","text":"<p>Decodes and verifies the JWT signature using the public key, checks the token's issuer and audience, and raises an error if the token is expired or invalid.</p> <pre><code>def validate_token(token, public_key):\n    \"\"\"Validates the JWT token using the public key.\"\"\"\n    try:\n        decoded_token = jwt.decode(\n            token, public_key, algorithms=[\"RS256\"], audience=\"https://api.mycompany.com\"\n        )\n        if decoded_token.get(\"iss\") != \"https://substrate.office.com/sts/\":  # Replace with your IdP's issuer if needed\n            raise Exception(\"Invalid issuer!\")\n        return True\n    except jwt.ExpiredSignatureError:\n        raise Exception(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        raise Exception(\"Invalid token!\")\n</code></pre> <p>Summary: Only requests with a valid JWT token (issued by your identity provider) are accepted. The payload is only processed if authentication passes. All actions are logged for traceability.</p>"},{"location":"adaptive-card-consultant-review-part2/#3-deep-dive-process_adaptive_card_payload","title":"3. Deep Dive: <code>process_adaptive_card_payload</code>","text":"<p>This function is responsible for: - Iterating through the submitted consultant actions. - Extracting manager and consultant details from the payload. - Taking the appropriate action (e.g., sending confirmation emails, saving to disk, triggering downstream automation).</p> <pre><code>def process_adaptive_card_payload(payload, client_ip):\n    for consultant_id, values in payload.items():\n        manager_email = values.get(\"manageremail\")\n        manager_name = values.get(\"managername\")\n        # ... process each consultant's action ...\n    send_email_to_manager(payload, manager_email, manager_name)\n    save_payload_to_disk(payload, manager_email)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#31-send_email_to_managerpayload-manager_email-manager_name","title":"3.1. <code>send_email_to_manager(payload, manager_email, manager_name)</code>","text":"<p>Builds an HTML summary of the manager's actions for all consultants and sends a confirmation email to the manager with a table of decisions (keep/deactivate).</p> <pre><code>def send_email_to_manager(payload, manager_email, manager_name):\n    \"\"\"Sends an HTML formatted email to the manager.\"\"\"\n    try:\n        subject = \"Consultant Review Actions Summary\"\n        body = f\"\"\"\n        &lt;html&gt;\n        &lt;body style=\\\"font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;p&gt;Dear {manager_name},&lt;/p&gt;\n            &lt;p&gt;Thank you for reviewing the consultants. Below is a summary of your actions:&lt;/p&gt;\n            &lt;table border=\\\"1\\\" style=\\\"border-collapse: collapse; width: 100%; font-family:verdana,courier,serif; font-size: 13px;\\\"&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Consultant Email&lt;/th&gt;\n                &lt;th&gt;Decision&lt;/th&gt;\n            &lt;/tr&gt;\n        \"\"\"\n        for consultant_id, values in payload.items():\n            body += f\"&lt;tr&gt;&lt;td&gt;{values.get('email')}&lt;/td&gt;&lt;td&gt;{values.get('decision')}&lt;/td&gt;&lt;/tr&gt;\"\n        body += \"\"\"\n            &lt;/table&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n        send_email(recipients=[manager_email], subject=subject, html_message=body)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#32-save_payload_to_diskpayload-manager_email","title":"3.2. <code>save_payload_to_disk(payload, manager_email)</code>","text":"<p>Serializes the entire payload to a JSON file and saves it to a mounted share or persistent storage for auditing and further processing.</p> <pre><code>def save_payload_to_disk(payload, manager_email):\n    \"\"\"Saves the entire payload to the mounted share as a single JSON file.\"\"\"\n    try:\n        import os, json, datetime\n        filename = f\"{manager_email}_consultant_review_{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}.json\"\n        path = os.path.join(UNPROCESSED_PATH, filename)\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(payload, f, ensure_ascii=False, indent=2)\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#4-downstream-automation-process_deactived_consultants","title":"4. Downstream Automation: <code>process_deactived_consultants</code>","text":"<p>This function is typically run on a schedule to process all submitted consultant reviews: - Loads all unprocessed review files from disk. - For each consultant marked for deactivation, adds them to a deactivation list. - Sends a summary email to HR and IT for further action. - Moves processed files to an archive location.</p> <pre><code>def process_deactived_consultants():\n    deactivate_list = []\n    manager_consultants_files = fetch_and_ignore_unprocessed_review_files()\n    for file in manager_consultants_files:\n        file_path, file_name = file.rsplit('/', 1)\n        file_time_utc = os.path.getmtime(file)\n        file_time = datetime.fromtimestamp(file_time_utc, pytz.utc).astimezone(pytz.timezone('America/Chicago'))\n        with open(file, 'r') as f:\n            file_content = f.read()\n        consultants_data = json.loads(file_content)\n        for consultant, details in consultants_data.items():\n            if details.get('decision') == 'deactivate':\n                deactivate_list.append({\n                    'manager_email': details.get('manageremail'),\n                    'consultant_email': details.get('email'),\n                    'approval_time': file_time.strftime('%Y-%m-%d %H:%M:%S')\n                })\n    if deactivate_list:\n        send_email_to_hr_and_it(deactivate_list)\n    for file in manager_consultants_files:\n        file_name = os.path.basename(file)\n        processed_file_path = os.path.join(PROCESSED_PATH, file_name)\n        os.rename(file, processed_file_path)\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#5-error-handling-handle_global_exception","title":"5. Error Handling: <code>handle_global_exception</code>","text":"<p>All major functions use <code>handle_global_exception</code> to log and report errors, ensuring that issues are traceable and do not silently fail.</p> <pre><code>def handle_global_exception(function_name, exception_obj):\n    logger.error(f\"Exception in {function_name}: {exception_obj}\")\n    # Optionally, send an alert email or take other action\n</code></pre>"},{"location":"adaptive-card-consultant-review-part2/#6-recap-full-request-processing-chain","title":"6. Recap: Full Request Processing Chain","text":"<ol> <li>Adaptive Card submission posts to <code>/consultant-review-confirmation</code>.</li> <li><code>process_request_headers_and_payload</code> authenticates and extracts the payload.<ul> <li>Calls <code>log_jwt_payload</code>, <code>fetch_public_key</code>, <code>validate_token</code>.</li> </ul> </li> <li><code>process_adaptive_card_payload</code> processes the payload.<ul> <li>Calls <code>send_email_to_manager</code>, <code>save_payload_to_disk</code>.</li> </ul> </li> <li><code>process_deactived_consultants</code> (scheduled) processes all reviews and notifies HR/IT.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#7-example-end-to-end-flow","title":"7. Example: End-to-End Flow","text":"<ol> <li>Manager receives Adaptive Card, reviews consultants, and submits actions.</li> <li>Submission is POSTed to <code>/consultant-review-confirmation</code> with a JWT Bearer token.</li> <li>The backend verifies the token, extracts the payload, and logs all actions.</li> <li>The manager receives a confirmation email summarizing their decisions.</li> <li>The payload is saved for auditing and further automation (e.g., account deactivation).</li> <li>HR/IT are notified of deactivation approvals as needed.</li> </ol>"},{"location":"adaptive-card-consultant-review-part2/#conclusion","title":"Conclusion","text":"<p>By recursively examining each function and providing the full code, you can see how the system securely and reliably processes Adaptive Card submissions. This approach is company-agnostic and can be adapted to any workflow requiring secure, actionable messaging in Outlook.</p> <ul> <li>Always validate and log incoming requests.</li> <li>Process and audit all actions.</li> <li>Automate downstream actions as needed.</li> </ul> <p>This completes the deep dive into the end-to-end workflow for actionable consultant review using Adaptive Cards and Python.</p>"},{"location":"adds-user-creation/","title":"Automating Active Directory User Creation and Group Assignment","text":""},{"location":"adds-user-creation/#introduction","title":"Introduction","text":"<p>Automating user provisioning in Active Directory Domain Services (ADDS) is a common requirement for IT teams managing large organizations. Python, with its rich ecosystem of libraries, makes it possible to programmatically create users and assign them to groups in ADDS. This article provides a detailed walkthrough of a working Python implementation for creating new ADDS users and adding them to groups, using the <code>ldap3</code> library and related tools.</p>"},{"location":"adds-user-creation/#overview-of-the-workflow","title":"Overview of the Workflow","text":"<p>The core function, <code>create_new_users_adds</code>, orchestrates the process of:</p> <ol> <li>Establishing a secure connection to the ADDS server.</li> <li>Creating a new user account with the required attributes.</li> <li>Setting the user's password and enabling the account.</li> <li>Adding the user to one or more ADDS groups.</li> </ol> <p>This workflow is modular, with each step handled by a dedicated function or library call, making it easy to adapt for different environments.</p>"},{"location":"adds-user-creation/#step-1-establishing-a-connection-to-adds","title":"Step 1: Establishing a Connection to ADDS","text":"<p>The function <code>get_adds_Connection</code> uses the <code>ldap3</code> library to connect to the ADDS server over SSL. Credentials are securely retrieved (in this codebase, from Azure Key Vault, but you can use environment variables or other secure stores):</p> <pre><code>server = ldap3.Server(dc_ip, use_ssl=True)\nconn = ldap3.Connection(server, user=LDAP_USER_ID, password=LDAP_USER_PASSWORD)\nif not conn.bind():\n    print('Error in bind', conn.result)\n</code></pre> <p>This returns a connection object used for all subsequent LDAP operations.</p>"},{"location":"adds-user-creation/#step-2-creating-a-new-user-in-adds","title":"Step 2: Creating a New User in ADDS","text":"<p>The function <code>create_adds_user</code> (called within <code>create_new_users_adds</code>) performs the following:</p> <ul> <li> <p>Adds the user object: <pre><code>conn.add(\n    distinguished_name,\n    ['top', 'person', 'organizationalPerson', 'user'],\n    {\n        'givenName': first_name,\n        'sn': last_name,\n        'sAMAccountName': sam_account_name,\n        'userPrincipalName': upn_name,\n        'mail': upn_name\n    }\n)\n</code></pre>   The <code>distinguished_name</code> (DN) specifies the user's location in the directory tree (OU). For generalization, replace any organization-specific OUs with your own structure.</p> </li> <li> <p>Enables the account and sets the password: <pre><code>conn.modify(\n    distinguished_name,\n    {\n        'userAccountControl': [(ldap3.MODIFY_REPLACE, [512])],  # Enable the account\n        'unicodePwd': [(ldap3.MODIFY_REPLACE, [f'\"{default_password}\"'.encode('utf-16-le')])]\n    }\n)\n</code></pre>   The password must be encoded in UTF-16-LE and quoted. The <code>userAccountControl</code> value of 512 enables the account.</p> </li> </ul>"},{"location":"adds-user-creation/#step-3-adding-the-user-to-adds-groups","title":"Step 3: Adding the User to ADDS Groups","text":"<p>After the user is created, the code assigns them to one or more groups using the <code>add_members_to_group</code> function from <code>ldap3.extend.microsoft.addMembersToGroups</code>:</p> <p><pre><code>add_members_to_group(conn, [distinguished_name], group_dns, fix=True)\n</code></pre> - <code>conn</code>: The active LDAP connection. - <code>[distinguished_name]</code>: A list of user DNs to add. - <code>group_dns</code>: A list of group DNs (distinguished names) to which the user should be added. - <code>fix=True</code>: Ensures the function will attempt to fix any inconsistencies in group membership.</p> <p>This function performs the necessary LDAP modifications to add the user as a member of each specified group. It is robust and handles group membership updates according to Microsoft's AD schema.</p>"},{"location":"adds-user-creation/#error-handling-and-best-practices","title":"Error Handling and Best Practices","text":"<ul> <li>Error Handling: Each step is wrapped in try/except blocks, and errors are logged or emailed to administrators. This is critical for production automation.</li> <li>Security: Credentials are not hardcoded. Use secure storage for service accounts and passwords.</li> <li>Generalization: Replace any organization-specific OUs or group names with your own. The logic is portable to any ADDS environment.</li> </ul>"},{"location":"adds-user-creation/#example-creating-and-assigning-a-user","title":"Example: Creating and Assigning a User","text":"<p>Here is a simplified, generalized version of the workflow:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\nfrom ldap3.extend.microsoft.addMembersToGroups import ad_add_members_to_groups as add_members_to_group\n\n# Connect to ADDS\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\n# Create user\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nconn.add(dn, ['top', 'person', 'organizationalPerson', 'user'], {\n    'givenName': 'John',\n    'sn': 'Doe',\n    'sAMAccountName': 'jdoe',\n    'userPrincipalName': 'jdoe@example.com',\n    'mail': 'jdoe@example.com'\n})\n\n# Enable account and set password\nconn.modify(dn, {\n    'userAccountControl': [(MODIFY_REPLACE, [512])],\n    'unicodePwd': [(MODIFY_REPLACE, ['\"YourPassword123!\"'.encode('utf-16-le')])]\n})\n\n# Add to groups\ngroup_dns = ['CN=YourGroup,OU=Groups,DC=example,DC=com']\nadd_members_to_group(conn, [dn], group_dns, fix=True)\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-creation/#conclusion","title":"Conclusion","text":"<p>With Python and the <code>ldap3</code> library, you can fully automate the process of creating users and managing group memberships in Active Directory. This approach is scalable, secure, and adaptable to any ADDS environment. By modularizing each step and handling errors robustly, you can integrate this workflow into larger HR or IT automation pipelines.</p>"},{"location":"adds-user-creation/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"adds-user-update/","title":"Updating Active Directory User Attributes","text":""},{"location":"adds-user-update/#introduction","title":"Introduction","text":"<p>Active Directory Domain Services (ADDS) is the backbone of identity management in many organizations. While user creation and group assignment are common automation tasks, updating user attributes\u2014both standard (delivered) and custom\u2014is equally important for keeping directory data accurate and useful. This article explains, with practical Python code, how to update ADDS user attributes using the <code>ldap3</code> library, focusing on the function <code>update_existing_users_adds</code>.</p>"},{"location":"adds-user-update/#understanding-adds-attributes-delivered-vs-custom","title":"Understanding ADDS Attributes: Delivered vs. Custom","text":"<ul> <li>Delivered (Standard) Attributes:</li> <li>These are built-in attributes provided by Microsoft, such as <code>givenName</code>, <code>sn</code>, <code>title</code>, <code>department</code>, <code>telephoneNumber</code>, etc.</li> <li>They are part of the default AD schema and are widely supported by tools and scripts.</li> <li>Custom Attributes:</li> <li>Organizations can extend the AD schema to include custom attributes (e.g., <code>extensionAttribute1</code>, <code>departmentNumber</code>).</li> <li>These are used for business-specific data not covered by standard attributes.</li> </ul> <p>Both types can be updated using the same LDAP operations.</p>"},{"location":"adds-user-update/#the-python-approach-using-ldap3","title":"The Python Approach: Using ldap3","text":"<p>The <code>ldap3</code> library provides a high-level, Pythonic interface for interacting with ADDS. The function <code>update_existing_users_adds</code> demonstrates how to:</p> <ol> <li>Build a dictionary of user attributes to update (both standard and custom).</li> <li>Connect to ADDS securely.</li> <li>Use the <code>modify</code> method to update attributes for each user.</li> <li>Handle errors and notify administrators if updates fail.</li> </ol>"},{"location":"adds-user-update/#step-by-step-updating-user-attributes","title":"Step-by-Step: Updating User Attributes","text":""},{"location":"adds-user-update/#1-prepare-the-attribute-dictionary","title":"1. Prepare the Attribute Dictionary","text":"<p>For each user, a dictionary is built with the attributes to update. This can include both delivered and custom attributes:</p> <pre><code>item = {\n    'displayName': display_name,           # Standard\n    'givenName': first_name,               # Standard\n    'sn': last_name,                       # Standard\n    'title': title,                        # Standard\n    'department': department,              # Standard\n    'employeeType': employee_type,         # Standard\n    'extensionAttribute1': is_mgmt_position, # Custom\n    'manager': manager_dn,                 # Standard (DN of manager)\n    # ... add more as needed ...\n}\n</code></pre>"},{"location":"adds-user-update/#2-connect-to-adds","title":"2. Connect to ADDS","text":"<pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n</code></pre>"},{"location":"adds-user-update/#3-update-attributes-with-modify","title":"3. Update Attributes with <code>modify</code>","text":"<p>The <code>modify</code> method is used to update one or more attributes for a user. The changes dictionary maps attribute names to a tuple specifying the operation (e.g., <code>MODIFY_REPLACE</code>) and the new value(s):</p> <p><pre><code>changes = {key: (MODIFY_REPLACE, [value]) for key, value in item.items() if value}\nconn.modify(dn=distinguished_name, changes=changes)\n</code></pre> - <code>dn</code>: The distinguished name of the user to update. - <code>changes</code>: A dictionary of attribute updates.</p>"},{"location":"adds-user-update/#4-error-handling-and-notification","title":"4. Error Handling and Notification","text":"<p>After each modify operation, the result is checked. If the update fails, an email notification is sent to administrators:</p> <pre><code>if conn.result['result'] != 0:\n    send_email(\n        recipients=['admin@example.com'],\n        subject=f'Error while updating user {distinguished_name}',\n        plain_message=f\"An error occurred while modifying user: {conn.result}\",\n    )\n</code></pre>"},{"location":"adds-user-update/#example-updating-a-users-attributes","title":"Example: Updating a User's Attributes","text":"<p>Here is a simplified, generalized example:</p> <pre><code>from ldap3 import Server, Connection, MODIFY_REPLACE\n\nserver = Server('your_dc_ip', use_ssl=True)\nconn = Connection(server, user='your_user', password='your_password')\nconn.bind()\n\ndn = 'CN=John Doe,OU=Users,DC=example,DC=com'\nchanges = {\n    'title': (MODIFY_REPLACE, ['Senior Engineer']),\n    'department': (MODIFY_REPLACE, ['Engineering']),\n    'extensionAttribute1': (MODIFY_REPLACE, ['Project Lead'])\n}\nconn.modify(dn=dn, changes=changes)\n\nif conn.result['result'] != 0:\n    print(f\"Error updating user: {conn.result}\")\n\nconn.unbind()\n</code></pre>"},{"location":"adds-user-update/#best-practices","title":"Best Practices","text":"<ul> <li>Batch Updates: You can update multiple attributes in a single <code>modify</code> call for efficiency.</li> <li>Custom Attributes: Ensure custom attributes exist in your AD schema before attempting to update them.</li> <li>Error Handling: Always check the result of LDAP operations and log or notify on failure.</li> <li>Security: Never hardcode credentials; use secure storage.</li> </ul>"},{"location":"adds-user-update/#conclusion","title":"Conclusion","text":"<p>Updating user attributes in ADDS with Python and <code>ldap3</code> is straightforward and powerful. Whether you are updating standard or custom attributes, the process is the same. By following the approach in <code>update_existing_users_adds</code>, you can automate directory maintenance and ensure your AD data stays current and accurate.</p>"},{"location":"adds-user-update/#references","title":"References","text":"<ul> <li>ldap3 Documentation</li> <li>Microsoft ADDS Schema</li> <li>Python ADDS Automation Examples</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/","title":"Cleaning Up Obsolete FSLogix Profiles in Azure","text":"<p>Obsolete FSLogix profile containers can consume significant storage and increase costs in Azure environments. This article explains how to identify and delete outdated profiles using a Bash script, helping you save space and reduce expenses. The approach is multi-step: first, list and analyze profiles, then safely delete those that are no longer needed. This process is ideal for automation and can be run in a container for portability and security.</p>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#why-clean-up-obsolete-profiles","title":"Why Clean Up Obsolete Profiles?","text":"<ul> <li>Cost Savings: Old FSLogix profile containers can accumulate and consume large amounts of Azure Files storage, leading to unnecessary costs.</li> <li>Performance: Removing unused profiles can improve performance and reduce clutter.</li> <li>Compliance: Regular cleanup helps maintain a tidy, compliant environment.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#multi-step-approach","title":"Multi-Step Approach","text":"<ol> <li>List and Analyze Profiles: Identify which profiles are old and candidates for deletion.</li> <li>Delete Obsolete Profiles: Remove only those that are confirmed to be outdated.</li> </ol> <p>This article covers the first step\u2014identifying obsolete profiles. (You can extend the script to perform deletions after review.)</p>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#example-script-identify-obsolete-fslogix-profiles","title":"Example Script: Identify Obsolete FSLogix Profiles","text":"<p>Below is a Bash script that lists FSLogix profile containers in an Azure Files share, checks their last modified date, and logs those that haven't been updated in a specified number of days. All sensitive values are masked for security.</p> <pre><code>#!/bin/bash\n\n# Define the output log files\ndeleteLogFile=\"step1-verbose.log\"\nsizeLogFile=\"step1-filesize.log\"\ndeleteFile=\"step1-input4step2.txt\"\n\n# Variables (replace with your own values)\nresourceGroupName=\"&lt;your-resource-group&gt;\"\nstorageAccountName=\"&lt;your-storage-account&gt;\"\nfileShareName=\"&lt;your-file-share&gt;\"\ndaysThreshold=30\nmaxProfiles=500  # Limit the number of profiles to process\n\n# Determine the script directory\nscriptDir=\"$(dirname \\\"$(realpath \\\"$0\\\")\\\")\"\ndeleteFile=\"$scriptDir/$deleteFile\"\nsizeFile=\"$scriptDir/$sizeLogFile\"\ndeleteLogFile=\"$scriptDir/$deleteLogFile\"\n\n# Delete existing files if they exist\nif [ -f \"$deleteFile\" ]; then\n    echo \"Deleting existing delete file: $deleteFile\"\n    rm \"$deleteFile\"\nfi\n\nif [ -f \"$deleteLogFile\" ]; then\n    echo \"Deleting existing delete log file: $deleteLogFile\"\n    rm \"$deleteLogFile\"\nfi\n\nif [ -f \"$sizeFile\" ]; then\n    echo \"Deleting existing size log file: $sizeFile\"\n    rm \"$sizeFile\"\nfi\n\n# Redirect all echo output to the delete log file\nexec &gt; \"$deleteLogFile\" 2&gt;&amp;1\n\necho \"Starting script...\"\n\necho \"Variables set: resourceGroupName=$resourceGroupName, storageAccountName=$storageAccountName, fileShareName=$fileShareName, daysThreshold=$daysThreshold, deleteFile=$deleteFile, sizeFile=$sizeFile\"\n\n# Get the storage account key\necho \"Fetching storage account key...\"\nstorageAccountKey=$(az storage account keys list --resource-group $resourceGroupName --account-name $storageAccountName --query '[0].value' --output tsv)\nif [ $? -ne 0 ]; then\n    echo \"Failed to fetch storage account key.\"\n    exit 1\nfi\necho \"Storage account key fetched successfully.\"\n\n# Get the current date in seconds since epoch and human-readable format\ncurrentDate=$(date +%s)\nhumanReadableCurrentDate=$(date -d @$currentDate +\"%Y-%m-%d %H:%M:%S\")\necho \"Current date (epoch): $currentDate\"\necho \"Current date (human-readable): $humanReadableCurrentDate\"\n\n# Calculate the threshold date (30 days ago)\nthresholdDate=$((currentDate - daysThreshold * 24 * 60 * 60))\nhumanReadableThresholdDate=$(date -d @$thresholdDate +\"%Y-%m-%d %H:%M:%S\")\necho \"Threshold date (epoch): $thresholdDate\"\necho \"Threshold date (human-readable): $humanReadableThresholdDate\"\n\n# Initialize arrays to store profile directories, sizes, last modified dates, and delete candidates\noldProfiles=()\nprofileSizes=()\ndeleteCandidates=()\n\n# List all directories in the file share\necho \"Listing profile directories...\"\nprofileDirs=$(az storage file list --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --output tsv --query '[].name')\nif [ $? -ne 0 ]; then\n    echo \"Failed to list profile directories.\"\n    exit 1\nfi\n\n# Loop through each profile directory\nprofileCount=0\nfor profileDir in $profileDirs; do\n    if [ $profileCount -ge $maxProfiles ]; then\n        echo \"Processed $maxProfiles profiles. Exiting loop.\"\n        break\n    fi\n\n    echo \"Processing profile directory: $profileDir\"\n\n    # List files in the profile directory\n    files=$(az storage file list --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --path $profileDir --output tsv --query '[].name')\n    if [ $? -ne 0 ]; then\n        echo \"Failed to list files in $profileDir.\"\n        continue\n    fi\n\n    for file in $files; do\n        if [[ $file == *.vhdx ]]; then\n            echo \"Processing file: $file\"\n\n            # Get the properties of the .vhdx file\n            fileProperties=$(az storage file show --account-name $storageAccountName --account-key $storageAccountKey --share-name $fileShareName --path $profileDir/$file --query '{properties: properties}' --output json)\n            if [ $? -ne 0 ]; then\n                echo \"Failed to get properties for $file.\"\n                continue\n            fi\n\n            # Get file size\n            fileSize=$(echo \"$fileProperties\" | jq -r '.properties.contentLength')\n            if [ $? -ne 0 ]; then\n                echo \"Failed to get file size for $file.\"\n                continue\n            fi\n            echo \"File size: $fileSize bytes\"\n\n            # Convert file size from bytes to GB\n            fileSizeGB=$(echo \"scale=2; $fileSize / (1024 * 1024 * 1024)\" | bc)\n            echo \"File size: $fileSizeGB GB\"\n\n            # Add profile directory and size to the arrays\n            profileSizes+=(\"$fileSizeGB $profileDir\")\n\n            # Get the last modified date of the file\n            lastModified=$(echo \"$fileProperties\" | jq -r '.properties.lastModified')\n            lastModifiedDate=$(date -d \"$lastModified\" +%s)\n            if [ $? -ne 0 ]; then\n                echo \"Failed to convert last modified date.\"\n                continue\n            fi\n            echo \"Last modified date (epoch): $lastModifiedDate\"\n\n            # Check if the last modified date is older than the threshold\n            if [[ $lastModifiedDate -lt $thresholdDate ]]; then\n                echo \"Profile $profileDir has not been updated in the last $daysThreshold days. Last modified: $lastModified\"\n\n                # Convert last modified date to human-readable format\n                humanReadableLastModifiedDate=$(date -d \"$lastModified\" +\"%Y-%m-%d %H:%M:%S\")\n\n                # Add profile directory to the delete candidates array\n                deleteCandidates+=(\"$profileDir - Last Modified: $humanReadableLastModifiedDate\")\n            fi\n        fi\n    done\n    profileCount=$((profileCount + 1))\ndone\n\n# Sort and write the profile sizes to the size log file\nif [ ${#profileSizes[@]} -ne 0 ]; then\n    echo \"Writing profile sizes to $sizeFile...\"\n    {\n        echo \"Profile directories and their .vhdx file sizes (in GB):\"\n        echo \"-------------------------------------\"\n        for i in \"${profileSizes[@]}\"; do\n            echo \"$i\"\n        done | sort -nr -k1 &gt; \"$sizeFile\"\n    }\n    echo \"Done writing to $sizeFile.\"\nelse\n    echo \"No profiles found with .vhdx files.\"\nfi\n\n# Write the outdated profiles to the delete file\nif [ ${#deleteCandidates[@]} -ne 0 ]; then\n    echo \"Writing outdated profiles to $deleteFile...\"\n    {\n        echo \"Profiles that haven't been updated in the last $daysThreshold days:\"\n        echo \"Threshold Date (human-readable): $humanReadableThresholdDate\"\n        echo \"Current Date (human-readable): $humanReadableCurrentDate\"\n        echo \"-------------------------------------\"\n        for i in \"${deleteCandidates[@]}\"; do\n            echo \"$i\"\n        done\n    } &gt; \"$deleteFile\"\n    echo \"Done writing to $deleteFile.\"\nelse\n    echo \"No profiles found that are outdated.\"\nfi\n\necho \"Script completed.\"\n</code></pre>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Setup and Variable Initialization:</li> <li>The script sets up log file names and variables for the resource group, storage account, and file share (all masked for security).</li> <li> <p>It determines the script directory and ensures log files are fresh for each run.</p> </li> <li> <p>Fetch Storage Account Key:</p> </li> <li> <p>Uses the Azure CLI to retrieve the storage account key for authentication.</p> </li> <li> <p>Date Calculations:</p> </li> <li> <p>Gets the current date and calculates the threshold date (e.g., 30 days ago) to identify old profiles.</p> </li> <li> <p>List Profile Directories:</p> </li> <li> <p>Uses <code>az storage file list</code> to enumerate all profile directories in the file share.</p> </li> <li> <p>Analyze Each Profile:</p> </li> <li>For each profile directory, lists files and looks for <code>.vhdx</code> files (FSLogix containers).</li> <li>Retrieves file size and last modified date for each <code>.vhdx</code> file.</li> <li> <p>If the file hasn't been updated in the threshold period, adds it to the delete candidates list.</p> </li> <li> <p>Log Results:</p> </li> <li>Writes a sorted list of profile sizes to a log file for review.</li> <li> <p>Writes a list of outdated profiles to a separate file for potential deletion.</p> </li> <li> <p>Output:</p> </li> <li>All output is logged for auditing and review before any deletion is performed.</li> </ol>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#benefits-of-running-in-a-container","title":"Benefits of Running in a Container","text":"<ul> <li>Isolation: Keeps dependencies and credentials isolated from your local environment.</li> <li>Portability: Easily run the script in any environment with Docker or container support.</li> <li>Security: Credentials and logs are contained within the container.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#next-steps-deleting-obsolete-profiles","title":"Next Steps: Deleting Obsolete Profiles","text":"<ul> <li>Review the generated log files to confirm which profiles are safe to delete.</li> <li>You can extend this script to delete profiles by using <code>az storage file delete</code> for each candidate.</li> <li>Always back up or confirm with stakeholders before deleting user data.</li> </ul>"},{"location":"avd-cleanup-obsolete-fsLogix-profiles/#usage-example","title":"Usage Example","text":"<p>This script is typically run as part of a maintenance workflow, either manually or on a schedule. For example, you might: - Run the script in a container or Azure Cloud Shell. - Review the output files (<code>step1-filesize.log</code> and <code>step1-input4step2.txt</code>). - Use a follow-up script to delete the confirmed obsolete profiles.</p> <p>By regularly cleaning up obsolete FSLogix profiles, you can save on Azure storage costs and keep your environment healthy and efficient.</p>"},{"location":"avd-custon-image-compute-gallery-part1/","title":"Part 1: Building and Publishing an Custom Image to Azure Compute Gallery","text":"<p>This article provides a comprehensive, step-by-step guide to creating a custom Azure Virtual Desktop (AVD) image using Infrastructure as Code (IaC), and publishing it to the Azure Compute Gallery. This is the foundation for deploying consistent, secure, and up-to-date AVD environments. For deploying AVD desktops from this image, see Part 2.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#overview","title":"Overview","text":"<p>The process involves: - Cleaning up old images in the Azure Compute Gallery - Determining the latest Microsoft 365 image SKU - Creating an image template using Bicep and Azure CLI - Building the image and publishing it to the Compute Gallery</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-1-clean-out-old-images-in-azure-compute-gallery","title":"Step 1: Clean Out Old Images in Azure Compute Gallery","text":"<ol> <li>Navigate to your Azure Compute Gallery (e.g., <code>GDEP_Azure_Compute_Gallery</code>).</li> <li>Identify and delete old images (sort by published date descending).</li> <li>Keep only the most recent image (from last month) for rollback purposes.</li> <li>Log in to Azure CLI as an administrator:</li> </ol> <pre><code>az login\n</code></pre> <p>Note: Use an account with sufficient permissions (e.g., Contributor or higher on the resource group). Masked example: <code>user.name@domain.com</code>.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-2-determine-the-latest-microsoft-365-image-sku","title":"Step 2: Determine the Latest Microsoft 365 Image SKU","text":"<p>To ensure your template uses the latest Microsoft 365 image, run:</p> <pre><code>az vm image list-skus --location eastus --publisher MicrosoftWindowsDesktop --offer office-365 --output table\n</code></pre> <ul> <li>This command lists all available SKUs for Microsoft 365 images in the East US region.</li> <li>Update your Bicep or parameter file to reference the latest SKU as needed.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-3-create-the-image-template-using-bicep","title":"Step 3: Create the Image Template Using Bicep","text":"<p>You can define your image template as code using a Bicep file (e.g., <code>avd.bicep</code>).</p>"},{"location":"avd-custon-image-compute-gallery-part1/#example-bicep-snippet","title":"Example Bicep Snippet","text":"<pre><code>param location string = 'eastus'\nparam imageTemplateName string = 'GDEPAVDWin11Template'\nparam galleryName string = 'GDEP_Azure_Compute_Gallery'\nparam resourceGroupName string = 'rg-gdep-peus-avd'\nparam sourceImagePublisher string = 'MicrosoftWindowsDesktop'\nparam sourceImageOffer string = 'office-365'\nparam sourceImageSku string = 'latest-sku-here'\nparam storageAccountUrl string = 'https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software/'\n\n// ...other parameters as needed...\n\nresource imageTemplate 'Microsoft.VirtualMachineImages/imageTemplates@2022-02-14' = {\n  name: imageTemplateName\n  location: location\n  properties: {\n    source: {\n      type: 'PlatformImage'\n      publisher: sourceImagePublisher\n      offer: sourceImageOffer\n      sku: sourceImageSku\n      version: 'latest'\n    }\n    customize: [\n      {\n        type: 'PowerShell'\n        name: 'InstallSoftware'\n        scriptUri: '${storageAccountUrl}install.ps1'\n      }\n      // Add more customization steps as needed\n    ]\n    distribute: [\n      {\n        type: 'SharedImage'\n        galleryImageId: '/subscriptions/&lt;sub-id&gt;/resourceGroups/${resourceGroupName}/providers/Microsoft.Compute/galleries/${galleryName}/images/GDEPAVDWin11Image'\n        runOutputName: 'GDEPAVDWin11ImageOutput'\n        artifactTags: {\n          source: 'avd-image-builder'\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#parameter-file-example-avdpjson","title":"Parameter File Example (<code>avdp.json</code>)","text":"<pre><code>{\n  \"location\": { \"value\": \"eastus\" },\n  \"imageTemplateName\": { \"value\": \"GDEPAVDWin11Template\" },\n  \"galleryName\": { \"value\": \"GDEP_Azure_Compute_Gallery\" },\n  \"resourceGroupName\": { \"value\": \"rg-gdep-peus-avd\" },\n  \"sourceImagePublisher\": { \"value\": \"MicrosoftWindowsDesktop\" },\n  \"sourceImageOffer\": { \"value\": \"office-365\" },\n  \"sourceImageSku\": { \"value\": \"latest-sku-here\" },\n  \"storageAccountUrl\": { \"value\": \"https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software/\" }\n}\n</code></pre> <p>Parameter Explanations: - <code>location</code>: Azure region for deployment. - <code>imageTemplateName</code>: Name for the image template resource. - <code>galleryName</code>: Name of the Azure Compute Gallery. - <code>resourceGroupName</code>: Resource group for the template and gallery. - <code>sourceImagePublisher</code>, <code>sourceImageOffer</code>, <code>sourceImageSku</code>: Define the base image. - <code>storageAccountUrl</code>: URL to a non-secure storage account containing install scripts and software (mask this in documentation).</p>"},{"location":"avd-custon-image-compute-gallery-part1/#step-4-deploy-the-image-template","title":"Step 4: Deploy the Image Template","text":"<p>Run the following command to deploy the image template:</p> <pre><code>az deployment group create --name GDEPAVDImage --resource-group rg-gdep-peus-avd --template-file ./compute/img/avd/avd.bicep --parameters @./compute/vm/avd/avdp.json\n</code></pre> <ul> <li>This command creates the image template in the specified resource group.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-5-build-the-image-and-publish-to-compute-gallery","title":"Step 5: Build the Image and Publish to Compute Gallery","text":"<p>After the template is created, build the image and publish it:</p> <pre><code>az image builder run --resource-group rg-gdep-peus-avd --name GDEPAVDWin11Template --no-wait\n</code></pre> <ul> <li>This command starts the image build process and publishes the resulting image to the Azure Compute Gallery.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#step-6-automate-software-installation-and-configuration-with-avdps1","title":"Step 6: Automate Software Installation and Configuration with avd.ps1","text":"<p>The <code>avd.ps1</code> PowerShell script is used during image creation to automate the installation and configuration of all required software and settings on the AVD image. Below is a detailed breakdown of its structure and key functions, with code examples and explanations for each part.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#key-functions-in-avdps1","title":"Key Functions in avd.ps1","text":""},{"location":"avd-custon-image-compute-gallery-part1/#1-invokestartprocess","title":"1. <code>InvokeStartProcess</code>","text":"<p>Runs a command (such as an installer) in a specified working directory with arguments, waits for completion, and logs the result.</p> <pre><code>Function InvokeStartProcess {\n    Param([string]$Command,[string]$WorkingDirectory,[string]$Arguments)\n    try {\n        $InstallProcess = Start-Process -FilePath $Command `\n                                        -WorkingDirectory $WorkingDirectory `\n                                        -ArgumentList $Arguments `\n                                        -Wait `\n                                        -Passthru\n        Write-Log \"Installed successfully  using command \", $Command, $Arguments -join\n    } catch {\n        Write-Log \"Error While running command \", $Command, $Arguments -join\n        $InstallProcess.Kill()\n    }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#2-downloadfilefromsa","title":"2. <code>DownloadfilefromSA</code>","text":"<p>Downloads a file from a given URI to a destination filename and logs the result.</p> <pre><code>Function DownloadfilefromSA {\n    Param([string]$SourceURI,[string]$DestinationFileName)\n    try {\n        Invoke-WebRequest $SourceURI -OutFile $DestinationFileName        \n        Write-Log \"Downloaded successfully file \", $DestinationFileName -join\n    } catch {\n        Write-Log \"Error While Downloading file \", $DestinationFileName -join\n        $InstallProcess.Kill()\n    }\n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#3-write-log","title":"3. <code>Write-Log</code>","text":"<p>Writes a message to a log file and outputs it to the console.</p> <pre><code>Function Write-Log {\n    Param($message)\n    Write-Output \"$(get-date -format 'yyyyMMdd HH:mm:ss') $message\" | Out-File -Encoding utf8 $logFile -Append\n    Write-Output $message  \n}\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#4-installandconfiguresoftware","title":"4. <code>InstallandConfigureSoftware</code>","text":"<p>This is the main function that orchestrates all software installations, registry changes, and configuration steps. It uses Chocolatey (<code>choco</code>) to install a wide range of software and handles additional configuration for FSLogix, SAP, and more.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#chocolatey-installs-with-code","title":"Chocolatey Installs (with code):","text":"<pre><code>choco install googlechrome -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install notepadplusplus -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install 7zip.install -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install visioviewer -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install vscode -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install git -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install powerbi -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install winscp -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install python -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install wireshark -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install sql-server-management-studio -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\nchoco install putty -y --no-progress --limit-output --ignore-checksums &gt;&gt; $ChocoFileName\n</code></pre> <p>Explanation: - Each <code>choco install</code> command installs a specific application silently and logs the output. - Chocolatey is a package manager for Windows, making it easy to automate software installation in your image builds. - You can add or remove packages as needed for your own environment.</p>"},{"location":"avd-custon-image-compute-gallery-part1/#additional-steps-in-installandconfiguresoftware","title":"Additional Steps in <code>InstallandConfigureSoftware</code>:","text":"<ul> <li>Configures FSLogix profile containers via registry.</li> <li>Applies local GPO and Office registry settings.</li> <li>Installs SAP, BPC Excel Add-In, and other business software.</li> <li>Creates desktop shortcuts for SAP, Notepad++, and a graceful logoff.</li> <li>Installs endpoint protection (Crowdstrike) and other utilities.</li> <li>All actions are logged for troubleshooting.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#example-downloading-and-installing-software","title":"Example: Downloading and Installing Software","text":"<pre><code>$StorageAccountDownloadURI = \"https://&lt;maskedstorageaccount&gt;.blob.core.windows.net/software4avd/\"\n$SoftwarefolderName = \"C:\\\\Software\\\\\"\n\n# Download a file\n$file2Download = \"ComputerGPO.cmd\"\n$fileURL2DownloadFrom = $StorageAccountDownloadURI + $file2Download\nInvoke-WebRequest $fileURL2DownloadFrom -OutFile $SoftwarefolderName\\$file2Download\n\n# Install Chrome using Chocolatey\nchoco install googlechrome -y --no-progress --limit-output --ignore-checksums\n</code></pre>"},{"location":"avd-custon-image-compute-gallery-part1/#security-note","title":"Security Note","text":"<ul> <li>The storage account used for downloads should be read-only and not contain sensitive data.</li> <li>All user accounts and credentials should be masked in documentation.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#additional-notes","title":"Additional Notes","text":"<ul> <li>All scripts (e.g., <code>avd.ps1</code>) should be reviewed for security and idempotency.</li> <li>Use service principals or user accounts with Contributor or higher permissions for these operations. Mask actual usernames in documentation.</li> </ul>"},{"location":"avd-custon-image-compute-gallery-part1/#related-articles","title":"Related Articles","text":"<ul> <li>Part 2: Deploying AVD Desktops</li> </ul>"},{"location":"avd-publish-part2/","title":"Part 2: Deploying Azure Virtual Desktop (AVD) Desktops","text":"<p>This article provides a detailed, step-by-step guide to deploying AVD VMs from a custom image in the Azure Compute Gallery, joining them to a domain, and ensuring a successful deployment. For the image creation process, see Part 1: Building and Publishing an AVD Image.</p>"},{"location":"avd-publish-part2/#overview","title":"Overview","text":"<p>The process involves: - Reviewing and updating parameter files for deployment - Deploying AVD VMs using Bicep and Azure CLI - Joining VMs to the domain using a privileged service account - Post-deployment validation and best practices</p>"},{"location":"avd-publish-part2/#step-1-review-and-update-parameter-file","title":"Step 1: Review and Update Parameter File","text":"<p>Before deploying, ensure your parameter file (e.g., <code>avdp.json</code>) is up to date with the correct image reference, VM size, network settings, and domain join information.</p>"},{"location":"avd-publish-part2/#example-parameter-file-avdpjson","title":"Example Parameter File (<code>avdp.json</code>)","text":"<pre><code>{\n  \"resource_group\": { \"value\": \"rg-gdep-peus-avd-pools\" },\n  \"nic_subnet_resourcegroup\": { \"value\": \"rg-gdep-peus-vnets\" },\n  \"nic_vnet_name\": { \"value\": \"GDEP_VNET_PROD\" },\n  \"nic_subnet_name\": { \"value\": \"Prod_AVD_FS_Subnet\" },\n  \"vm_name\": { \"value\": \"AZGDEPENG\" },\n  \"adminusername\": { \"value\": \"avdadmin\" },\n  \"hostpoolname\": { \"value\": \"GDEP_Engineering\" },\n  \"adminPassword\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"new-vm-password\"\n    }\n  },\n  \"adadminusername\": { \"value\": \"service-account@yourdomain.com\" },\n  \"adadminPassword\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"ad-join-password\"\n    }\n  },\n  \"hostpoolregistrationkey\": {\n    \"reference\": {\n      \"keyVault\": {\n        \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/&lt;vault-name&gt;\"\n      },\n      \"secretName\": \"avd-host-pool-reg-key\"\n    }\n  },\n  \"vmCount\": { \"value\": 1 }\n}\n</code></pre> <p>Parameter Explanations: - <code>resource_group</code>, <code>nic_subnet_resourcegroup</code>, <code>nic_vnet_name</code>, <code>nic_subnet_name</code>: Networking and resource group settings. - <code>vm_name</code>: Prefix for VM names. - <code>adminusername</code>/<code>adminPassword</code>: Local admin credentials for the VM (use Key Vault for secrets). - <code>adadminusername</code>/<code>adadminPassword</code>: Service account with delegated rights to join computers to the domain. Do not use personal admin accounts; use a dedicated service account. - <code>hostpoolname</code>, <code>hostpoolregistrationkey</code>: AVD host pool registration details. - <code>vmCount</code>: Number of VMs to deploy.</p>"},{"location":"avd-publish-part2/#step-2-full-bicep-code-for-avd-deployment","title":"Step 2: Full Bicep Code for AVD Deployment","text":"<p>Below is the full Bicep code for deploying AVD VMs, joining them to the domain, and registering them with the AVD host pool.</p> <pre><code>param location string = resourceGroup().location\nparam resource_group string\nparam nic_subnet_resourcegroup string\nparam nic_vnet_name string\nparam nic_subnet_name string\nparam vm_name string\nparam adminusername string\nparam adadminusername string\nparam hostpoolregistrationkey string\nparam hostpoolname string  \nparam vmCount int\n\n@secure()\nparam adminPassword string\n@secure()\nparam adadminPassword string\n\nvar applicationtag = 'Virtual Desktop'\nvar environmetag = 'Production'\nvar domain = 'yourdomain.com'\nvar ou2add2 = 'OU=Computers,OU=Standard,OU=Virtual Desktops,OU=Azure,OU=GD,DC=yourdomain,DC=com'\n\n@description('Create NICs for each VM')\nmodule nicgdepavd '../../../networking/nic/nic.bicep' = [for i in range(0, vmCount):{\n  name: '${deployment().name}-nic${i}'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: 'nicgdepp${location}${vm_name}${i}'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      ipConfigurations: [\n        {\n          name: 'ipconfig'\n          properties: {\n            privateIPAllocationMethod: 'Dynamic'\n            subnet: {\n              id: resourceId(\n                '${nic_subnet_resourcegroup}',\n                'Microsoft.Network/virtualNetworks/subnets',\n                '${nic_vnet_name}',\n                '${nic_subnet_name}'\n              )\n            }\n          }\n        }\n      ]\n    }\n  }\n}]\n\n@description('Create the AVD VM(s)')\nmodule vmgdepavd '../vm.bicep' = [for i in range(0, vmCount): {\n  name: '${deployment().name}-${i}'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: '${vm_name}-${i}'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      hardwareProfile: { vmSize: 'Standard_NV16as_v4' }\n      storageProfile: {\n        imageReference: {\n          id: resourceId('Microsoft.Compute/galleries/images', 'GDEP_Azure_Compute_Gallery', 'Win11AVD')\n        }\n        osDisk: {\n          createOption: 'FromImage'\n          diskSizeGB: 512\n          managedDisk: { storageAccountType: 'Premium_LRS' }\n        }\n      }\n      osProfile: {\n        computerName: '${vm_name}-${i}'\n        adminUsername: adminusername\n        adminPassword: adminPassword\n        windowsConfiguration: {\n          provisionVMAgent: true\n          enableAutomaticUpdates: true\n        }\n      }\n      networkProfile: {\n        networkInterfaces: [\n          {\n            id: nicgdepavd[i].outputs.id\n            properties: { deleteOption: 'Delete' }\n          }\n        ]\n      }\n      diagnosticsProfile: { bootDiagnostics: { enabled: true } }\n      licenseType: 'Windows_Client'\n    }\n  }\n}]\n\n@description('Add Custom Script Extension for Kofax')\nresource extkofaxcustomscript 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/CustomScriptExtension'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    publisher: 'Microsoft.Compute'\n    type: 'CustomScriptExtension'\n    typeHandlerVersion: '1.9'\n    autoUpgradeMinorVersion: true\n    settings: {\n      commandToExecute: 'powershell.exe -ExecutionPolicy Unrestricted -Command \"Invoke-WebRequest -Uri \\'https://storeusgdepsoftware.blob.core.windows.net/software4avd/avdkofax.ps1\\' -OutFile \\'C:\\\\software\\\\avdkofax.ps1\\'; Set-Location -Path \\'C:\\\\software\\'; .\\\\avdkofax.ps1\"'\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n  ]\n}]\n\n@description('Join VM(s) to the domain')\nresource extadd2adcustomscript 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/joindomain'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    autoUpgradeMinorVersion: true\n    publisher: 'Microsoft.Compute'\n    type: 'JsonADDomainExtension'\n    typeHandlerVersion: '1.3'\n    settings: {\n      name: domain\n      ouPath: ou2add2\n      user: adadminusername\n      restart: true\n      options: '3'\n    }\n    protectedSettings: {\n      password: adadminPassword\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n    extkofaxcustomscript[i]\n  ]\n}]\n\n@description('Register VM(s) with AVD Host Pool')\nresource vmExtension 'Microsoft.Compute/virtualMachines/extensions@2024-03-01' = [for i in range(0, vmCount): {\n  name: '${vm_name}-${i}/Microsoft.PowerShell.DSC'\n  location: location\n  tags: {\n    Environment: environmetag\n    Application: applicationtag\n  }\n  properties: {\n    autoUpgradeMinorVersion: true\n    publisher: 'Microsoft.Powershell'\n    type: 'DSC'\n    typeHandlerVersion: '2.73'\n    settings: {\n      modulesUrl: 'https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_1.0.02714.342.zip'\n      configurationFunction: 'Configuration.ps1\\\\AddSessionHost'\n      properties: {\n        hostPoolName: hostpoolname\n        registrationInfoToken: hostpoolregistrationkey\n        aadJoin: false\n        UseAgentDownloadEndpoint: true\n      }\n    }\n  }\n  dependsOn: [\n    vmgdepavd[i]\n    extkofaxcustomscript[i]\n    extadd2adcustomscript[i]\n  ]\n}]\n</code></pre>"},{"location":"avd-publish-part2/#step-3-deploy-the-avd-vms-using-azure-cli","title":"Step 3: Deploy the AVD VM(s) Using Azure CLI","text":"<p>Use the following command to deploy the VM(s):</p> <pre><code>az deployment group create --name gdepiacavd --resource-group rg-gdep-peus-avd-pools --template-file ./compute/vm/avd/avd.bicep --parameters @./compute/vm/avd/avdp.json\n</code></pre> <ul> <li>This command deploys the VM(s) using the custom image and parameters.</li> </ul>"},{"location":"avd-publish-part2/#step-4-domain-join-and-permissions","title":"Step 4: Domain Join and Permissions","text":"<ul> <li>The <code>adadminusername</code> must be a service account with delegated permissions to join computers to the specified OU in Active Directory.</li> <li>Do not use personal admin accounts; use a dedicated, least-privilege service account.</li> <li>The deployment will use the provided credentials to join the VM to the domain and place it in the correct OU.</li> </ul>"},{"location":"avd-publish-part2/#step-5-post-deployment-validation","title":"Step 5: Post-Deployment Validation","text":"<ul> <li>Log in to the new VM(s) as the local admin account.</li> <li>Verify log files (e.g., in <code>C:\\Software</code>) to ensure a clean build.</li> <li>Expand the disk to 512 GB if required.</li> <li>(Optional) Install additional software as needed.</li> <li>For validation desktops, use the correct host pool name and ensure only authorized users have access.</li> </ul>"},{"location":"avd-publish-part2/#security-and-best-practices","title":"Security and Best Practices","text":"<ul> <li>Always use secure methods (Key Vault references) for passwords and sensitive data.</li> <li>Review all scripts and templates for security and compliance.</li> <li>Mask all sensitive information in documentation.</li> <li>Use service accounts for domain join, not personal accounts.</li> </ul>"},{"location":"avd-publish-part2/#related-articles","title":"Related Articles","text":"<ul> <li>Part 1: Building and Publishing an Custom Image</li> </ul>"},{"location":"azure-ad-certificate/","title":"Certificate Based Authentication","text":""},{"location":"azure-ad-certificate/#certificate-based-authentication-for-azure-ad-why-and-how","title":"Certificate-Based Authentication for Azure AD: Why and How","text":""},{"location":"azure-ad-certificate/#creating-a-certificate-for-azure-ad-authentication","title":"Creating a Certificate for Azure AD Authentication","text":"<p>To use certificate-based authentication with Azure Active Directory (Azure AD), you first need to generate a certificate. Certificates provide a secure, manageable, and standards-based way to authenticate applications. A <code>.pfx</code> certificate may be required because Azure AD expects a certificate in Personal Information Exchange (PFX) format when uploading via the portal or for certain SDKs. The <code>.pfx</code> file contains both the public and private keys, protected by a password, and is suitable for import/export scenarios.</p>"},{"location":"azure-ad-certificate/#steps-to-generate-a-certificate-using-openssl","title":"Steps to Generate a Certificate Using OpenSSL","text":"<ol> <li>Generate a Private Key: <pre><code>openssl genrsa -out my-app-auth.key 2048\n</code></pre></li> <li>Create a Certificate Signing Request (CSR): <pre><code>openssl req -new -key my-app-auth.key -out my-app-auth.csr\n</code></pre></li> <li>Generate a Self-Signed Certificate: <pre><code>openssl x509 -req -days 730 -in my-app-auth.csr -signkey my-app-auth.key -out my-app-auth.crt\n</code></pre></li> <li>Export to PFX (if needed for Azure): <pre><code>openssl pkcs12 -export -out my-app-auth.pfx -inkey my-app-auth.key -in my-app-auth.crt\n</code></pre> <p>Note: The <code>.pfx</code> format is required if you want to upload the certificate via the Azure Portal or use it with some SDKs/tools. The <code>.crt</code> file is the public certificate, and the <code>.key</code> file is your private key (keep it secure!).</p> </li> </ol>"},{"location":"azure-ad-certificate/#uploading-the-certificate-to-your-entra-azure-ad-application","title":"Uploading the Certificate to Your Entra (Azure AD) Application","text":"<ol> <li>Go to the Microsoft Entra admin center and select Azure Active Directory.</li> <li>Navigate to App registrations and select your application.</li> <li>In the left menu, click Certificates &amp; secrets.</li> <li>Under Certificates, click Upload certificate.</li> <li>Select your <code>.crt</code> or <code>.pfx</code> file and upload it.</li> <li>After uploading, Azure will display the certificate thumbprint. Save this value for use in your application code.</li> </ol>"},{"location":"azure-ad-certificate/#assigning-permissions-to-the-application","title":"Assigning Permissions to the Application","text":"<p>After uploading the certificate, you must assign the necessary API permissions to your application:</p> <ol> <li>In your application's App registration page, go to API permissions.</li> <li>Click Add a permission and select the required Microsoft APIs (e.g., Microsoft Graph, Azure Service Management, etc.).</li> <li>Choose the appropriate permission type (Application or Delegated) and select the required permissions.</li> <li>Click Add permissions.</li> <li>If required, click Grant admin consent to approve the permissions for your organization.</li> </ol> <p>Note: The application will only be able to access resources for which it has been granted permissions. Make sure to review and assign only the permissions your app needs.</p>"},{"location":"azure-ad-certificate/#why-use-a-certificate-instead-of-an-application-secret","title":"Why Use a Certificate Instead of an Application Secret?","text":""},{"location":"azure-ad-certificate/#1-security","title":"1. Security","text":"<ul> <li>Application secrets are essentially passwords. They are susceptible to accidental exposure (e.g., in code repositories, logs, or configuration files).</li> <li>Certificates use asymmetric cryptography. The private key never leaves your environment, and only the public key is uploaded to Azure AD. This makes certificates much harder to compromise.</li> </ul>"},{"location":"azure-ad-certificate/#2-lifecycle-management","title":"2. Lifecycle Management","text":"<ul> <li>Secrets typically expire every 6-12 months, requiring regular rotation and updates in all dependent systems.</li> <li>Certificates can have longer lifespans (e.g., 1-2 years), and their expiration is easier to track and automate.</li> </ul>"},{"location":"azure-ad-certificate/#3-compliance-and-best-practices","title":"3. Compliance and Best Practices","text":"<ul> <li>Microsoft and most security frameworks recommend certificates for service-to-service authentication.</li> <li>Certificates support better auditing and can be managed centrally (e.g., via Azure Key Vault).</li> </ul>"},{"location":"azure-ad-certificate/#why-use-the-msal-library-and-not-a-specific-azure-sdk","title":"Why Use the MSAL Library (and Not a Specific Azure SDK)?","text":"<p>The MSAL (Microsoft Authentication Library) for Python is a lightweight, flexible library for acquiring tokens from Azure AD. It supports a wide range of authentication scenarios, including certificate-based authentication for confidential clients.</p> <ul> <li>Why MSAL?</li> <li>MSAL is the official library for handling authentication and token acquisition with Azure AD.</li> <li>It is not tied to a specific Azure service, making it ideal for generic authentication scenarios.</li> <li> <p>It supports advanced scenarios like certificate-based authentication, multi-tenant apps, and more.</p> </li> <li> <p>Why Not Use a Specific Azure SDK?</p> </li> <li>Some Azure SDKs (e.g., for Storage, Key Vault, etc.) provide their own authentication mechanisms, but they may not support all advanced scenarios or may require additional dependencies.</li> <li>Using MSAL directly gives you full control over the authentication flow and token management, and is more transparent for troubleshooting and customization.</li> </ul>"},{"location":"azure-ad-certificate/#code-example-certificate-based-authentication-in-python","title":"Code Example: Certificate-Based Authentication in Python","text":"<p>Below is the function used in this project to acquire an Azure AD access token using a certificate:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token_from_azure(client_id, authority, tenant_id, resource_scopes):\n    \"\"\"\n    Retrieves an access token from Azure Active Directory using a confidential client application.\n    This function uses certificate-based authentication to acquire an access token for the specified resource.\n    \"\"\"\n    try:\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n\n        app = ConfidentialClientApplication(\n            client_id=client_id,\n            authority=f\"{authority}{tenant_id}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n\n        result = app.acquire_token_for_client(scopes=resource_scopes)\n        if \"access_token\" in result:\n            return result[\"access_token\"]\n\n    except Exception as exception:\n        handle_global_exception(sys._getframe().f_code.co_name, exception)\n    finally:\n        pass\n</code></pre>"},{"location":"azure-ad-certificate/#key-points","title":"Key Points:","text":"<ul> <li>The private key is read from a secure file (<code>certs/*.key</code>).</li> <li>The certificate thumbprint and private key are passed to MSAL's <code>ConfidentialClientApplication</code>.</li> <li>No secrets or passwords are stored in code or configuration.</li> </ul>"},{"location":"azure-ad-certificate/#conclusion","title":"Conclusion","text":"<p>Certificate-based authentication is the recommended and most secure way to authenticate service applications with Azure AD. It reduces risk, simplifies management, and aligns with industry best practices. Migrating from secrets to certificates is straightforward and well-supported by both Azure and the MSAL Python library.</p>"},{"location":"azure-ad-certificate/#references","title":"References","text":"<ul> <li>MSAL Python Certificate Auth Sample</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> <li>OpenSSL Documentation</li> </ul>"},{"location":"azure-ad-user-devices/","title":"Retrieving Entra (Azure AD) User Device","text":""},{"location":"azure-ad-user-devices/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve a list of all devices registered in Microsoft Entra (Azure AD), including key device attributes and registered user information, using Python and the Microsoft Graph API. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"azure-ad-user-devices/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>requests</code></li> <li><code>msal</code> (for authentication, not shown here)</li> <li>An Azure AD application (service principal) with permissions to read device information</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-ad-user-devices/#constants-and-endpoints","title":"Constants and Endpoints","text":"<pre><code>AZURE_GRAPH_BETA = 'https://graph.microsoft.com/beta/'\nLIST_OF_AAD_DEVICE_ATTRIBUTES = [\n    'approximateLastSignInDateTime', 'deviceId', 'displayName', 'id',\n    'isCompliant', 'isManaged', 'manufacturer', 'model', 'operatingSystem',\n    'operatingSystemVersion', 'deviceOwnership', 'managementType'\n]\n</code></pre>"},{"location":"azure-ad-user-devices/#step-1-retrieve-device-inventory-from-entra-azure-ad","title":"Step 1: Retrieve Device Inventory from Entra (Azure AD)","text":""},{"location":"azure-ad-user-devices/#function-get_list_of_devices","title":"Function: <code>get_list_of_devices</code>","text":"<p>This function retrieves a list of devices from Entra (Azure AD), including key device attributes and registered user information, using the Microsoft Graph API.</p> <pre><code>def get_list_of_devices():\n    devices_list = []  # Initialize list to store device data\n    try:\n        LIST_OF_AAD_DEVICE_ATTRIBUTES = [\n            'approximateLastSignInDateTime', 'deviceId', 'displayName', 'id',\n            'isCompliant', 'isManaged', 'manufacturer', 'model', 'operatingSystem',\n            'operatingSystemVersion', 'deviceOwnership', 'managementType'\n        ]\n        selected_attributes = \",\".join(LIST_OF_AAD_DEVICE_ATTRIBUTES)\n        query_params = f\"$select={selected_attributes}&amp;$expand=registeredUsers\"\n        devices_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}devices?{query_params}\")\n        user_device = []\n        for device in devices_list:\n            registered_user = device.get('registeredUsers', [])\n            user_id = registered_user[0].get('id') if registered_user else None\n            user_ip = None  # Not assigned from the device dictionary, keeping as None\n            user_upn = registered_user[0].get('userPrincipalName') if registered_user else None\n            device_to_add = {\n                'approximateLastSignInDateTime': parse_iso_date(device.get('approximateLastSignInDateTime')),\n                'deviceId': device.get('deviceId'),\n                'displayName': device.get('displayName'),\n                'id': device.get('id'),\n                'isCompliant': device.get('isCompliant'),\n                'isManaged': device.get('isManaged'),\n                'manufacturer': device.get('manufacturer'),\n                'model': device.get('model'),\n                'operatingSystem': device.get('operatingSystem'),\n                'operatingSystemVersion': device.get('operatingSystemVersion'),\n                'deviceOwnership': device.get('deviceOwnership'),\n                'user_id': user_id,\n                'user_ip': user_ip,\n                'user_upn': user_upn,\n                'managementType': device.get('managementType'),\n            }\n            user_device.append(device_to_add)\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return user_device\n</code></pre> <p>Explanation: - Builds a query to select device attributes and expand registered user information (<code>registeredUsers</code>). - Calls <code>execute_odata_query_get</code> to make the API request and handle pagination. - For each device, extracts key attributes and the first registered user's ID and UPN (if available). - Returns a list of device dictionaries, each including device and user information.</p>"},{"location":"azure-ad-user-devices/#supporting-function-execute_odata_query_get","title":"Supporting Function: <code>execute_odata_query_get</code>","text":"<p>This function is used to make authenticated, paginated requests to the Microsoft Graph API.</p> <pre><code>def execute_odata_query_get(urltoInvoke, token=''):\n    try:\n        localUserList = []\n        if not token:\n            acesstokenforClientapp = get_access_token_API_Access_AAD()\n        else:\n            acesstokenforClientapp = token\n        continueLooping = True\n        while continueLooping:\n            response = requests.get(\n                url=urltoInvoke,\n                headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n            )\n            if response.status_code == 429:  # Throttling response\n                retry_after = int(response.headers.get(\"Retry-After\", 5))\n                print(f\"Throttled! Retrying after {retry_after} seconds...\")\n                time.sleep(retry_after)\n                continue\n            if response.status_code == 401:  # Token expired or invalid\n                print(\"Token expired or invalid. Fetching a new one...\")\n                acesstokenforClientapp = get_access_token_API_Access_AAD()\n                response = requests.get(\n                    url=urltoInvoke,\n                    headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n                )\n                if response.status_code != 200:\n                    response.raise_for_status()\n            if response.status_code == 403:\n                raise Exception(f\"403 Forbidden: Access denied for URL {urltoInvoke}\")\n            if response.status_code != 200:\n                response.raise_for_status()\n            graph_data = response.json()\n            localUserList.extend(graph_data.get('value', []))\n            if \"@odata.nextLink\" in graph_data:\n                urltoInvoke = graph_data[\"@odata.nextLink\"]\n            else:\n                continueLooping = False\n        return localUserList\n    except Exception as e:\n        if hasattr(e, 'args') and e.args and '403 Forbidden' in str(e.args[0]):\n            raise\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre> <p>Explanation: - Handles authentication, pagination, throttling, and error handling for Microsoft Graph API requests. - Used by all higher-level functions to retrieve data from Entra (Azure AD).  For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p>"},{"location":"azure-ad-user-devices/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can programmatically retrieve a complete inventory of devices from Entra (Azure AD), including device details and registered user information, using Python and the Microsoft Graph API. This enables automated device inventory, compliance, and reporting workflows in your organization.</p> <p>For more details, see the Microsoft Graph API documentation.</p>"},{"location":"azure-ad-user/","title":"Retrieving Entra (Azure AD) Users, Group Membership, and License Assignments","text":""},{"location":"azure-ad-user/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve a list of all users from Microsoft Entra (Azure AD), including their group memberships and license assignments, using Python and the Microsoft Graph API. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"azure-ad-user/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>requests</code></li> <li><code>msal</code> (for authentication, not shown here)</li> <li>An Azure AD application (service principal) with permissions to read users, groups, and licenses</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-ad-user/#constants-and-endpoints","title":"Constants and Endpoints","text":"<pre><code>AZURE_GRAPH_BETA = 'https://graph.microsoft.com/beta/'\nLIST_OF_AAD_USER_ATTRIBUTES = [\n    'displayName', 'accountEnabled', 'userPrincipalName', 'licenseAssignmentStates',\n]\n</code></pre>"},{"location":"azure-ad-user/#step-1-retrieve-users-with-license-assignments-and-group-memberships","title":"Step 1: Retrieve Users with License Assignments and Group Memberships","text":""},{"location":"azure-ad-user/#function-get_users_licenseassignments_and_groups","title":"Function: <code>get_users_licenseassignments_and_groups</code>","text":"<p>This function retrieves a list of users from Entra (Azure AD), including their license assignment states and group memberships, using the Microsoft Graph API.</p> <pre><code>def get_users_licenseassignments_and_groups():\n    user_list = []  # Initialize list to store user data\n    try:\n        LIST_OF_AAD_USER_ATTRIBUTES = [\n            'displayName', 'accountEnabled', 'userPrincipalName', 'licenseAssignmentStates',\n        ]\n        selected_attributes = \",\".join(LIST_OF_AAD_USER_ATTRIBUTES)\n        query_params = f\"$select={selected_attributes}\"\n        query_params += \"&amp;$expand=memberOf\"\n        user_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}users?{query_params}\")\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return user_list\n</code></pre> <p>Explanation: - Builds a query to select user attributes and expand group memberships (<code>memberOf</code>). - Calls <code>execute_odata_query_get</code> to make the API request and handle pagination. - Returns a list of user dictionaries with license and group data.</p>"},{"location":"azure-ad-user/#step-2-retrieve-all-groups","title":"Step 2: Retrieve All Groups","text":""},{"location":"azure-ad-user/#function-get_list_of_groups","title":"Function: <code>get_list_of_groups</code>","text":"<p>This function retrieves all groups from Entra (Azure AD).</p> <pre><code>def get_list_of_groups():\n    group_list = []  # Initialize list to store group data\n    try:\n        group_list = execute_odata_query_get(f\"{AZURE_GRAPH_BETA}groups?\")\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return group_list\n</code></pre> <p>Explanation: - Calls the Microsoft Graph <code>/groups</code> endpoint to retrieve all groups. - Uses <code>execute_odata_query_get</code> for API calls and pagination. - Returns a list of group dictionaries.</p>"},{"location":"azure-ad-user/#step-3-combine-user-license-and-group-data","title":"Step 3: Combine User, License, and Group Data","text":""},{"location":"azure-ad-user/#function-get_users_with_license_and_groups","title":"Function: <code>get_users_with_license_and_groups</code>","text":"<p>This function combines user, license, and group data into a unified list.</p> <pre><code>def get_users_with_license_and_groups():\n    final_list = []  # Final list of dictionaries\n    SKU_MAPPING = {\n        \"ee02fd1b-340e-4a4b-b355-4a514e4c8943\": \"Exchange Online Archiving\",\n        \"05e9a617-0261-4cee-bb44-138d3ef5d965\": \"365 E3\",\n        # ... (other SKU mappings) ...\n    }\n    try:\n        users = get_users_licenseassignments_and_groups()\n        groups = get_list_of_groups()\n        for user in users:\n            # Extract license assignments and group memberships\n            license_assignments = []\n            group_memberships = []\n            # Parse license assignments\n            for license in user.get('licenseAssignmentStates', []):\n                sku_id = license.get('skuId')\n                sku_name = SKU_MAPPING.get(sku_id, sku_id)\n                license_assignments.append({\n                    'sku_id': sku_id,\n                    'sku_name': sku_name,\n                    'assigned_by_group': license.get('assignedByGroup', None),\n                })\n            # Parse group memberships\n            for group in user.get('memberOf', []):\n                group_memberships.append({\n                    'group_id': group.get('id'),\n                    'display_name': group.get('displayName'),\n                })\n            final_list.append({\n                'user_principal_name': user.get('userPrincipalName'),\n                'display_name': user.get('displayName'),\n                'account_enabled': user.get('accountEnabled'),\n                'license_assignments': license_assignments,\n                'group_memberships': group_memberships,\n            })\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n    return final_list\n</code></pre> <p>Explanation: - Calls the previous two functions to get users and groups. - Maps license SKUs to human-readable names. - Extracts and structures license and group membership data for each user. - Returns a list of user dictionaries with all relevant information.</p>"},{"location":"azure-ad-user/#step-4-high-level-orchestration","title":"Step 4: High-Level Orchestration","text":""},{"location":"azure-ad-user/#function-get_list_of_users_with_license_and_groups","title":"Function: <code>get_list_of_users_with_license_and_groups</code>","text":"<p>This function orchestrates the retrieval and structuring of user, license, and group data.</p> <pre><code>def get_list_of_users_with_license_and_groups():\n    user_groups = []\n    user_license = []\n    user_lic_and_groups = get_users_with_license_and_groups()\n    for user in user_lic_and_groups:\n        for membership in user['group_memberships']:\n            user_groups.append({\n                'user_principal_name': user['user_principal_name'],\n                'group_id': membership['group_id'],\n                'group_display_name': membership['display_name'],\n            })\n        for licassignment in user['license_assignments']:\n            user_license.append({\n                'user_principal_name': user['user_principal_name'],\n                'sku_id': licassignment['sku_id'],\n                'sku_name': licassignment['sku_name'],\n                'assigned_by_group': licassignment['assigned_by_group'],\n            })\n    return user_groups, user_license\n</code></pre> <p>Explanation: - Calls <code>get_users_with_license_and_groups</code> to get the unified user data. - Flattens group memberships and license assignments into separate lists for easy processing or storage. - Returns two lists: one for user-group relationships, one for user-license assignments.</p>"},{"location":"azure-ad-user/#supporting-function-execute_odata_query_get","title":"Supporting Function: <code>execute_odata_query_get</code>","text":"<p>This function is used throughout to make authenticated, paginated requests to the Microsoft Graph API.</p> <pre><code>def execute_odata_query_get(urltoInvoke, token=''):\n    try:\n        localUserList = []\n        if not token:\n            acesstokenforClientapp = get_access_token_API_Access_AAD()\n        else:\n            acesstokenforClientapp = token\n        continueLooping = True\n        while continueLooping:\n            response = requests.get(\n                url=urltoInvoke,\n                headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n            )\n            if response.status_code == 429:  # Throttling response\n                retry_after = int(response.headers.get(\"Retry-After\", 5))\n                print(f\"Throttled! Retrying after {retry_after} seconds...\")\n                time.sleep(retry_after)\n                continue\n            if response.status_code == 401:  # Token expired or invalid\n                print(\"Token expired or invalid. Fetching a new one...\")\n                acesstokenforClientapp = get_access_token_API_Access_AAD()\n                response = requests.get(\n                    url=urltoInvoke,\n                    headers={'Authorization': f'Bearer {acesstokenforClientapp}'}\n                )\n                if response.status_code != 200:\n                    response.raise_for_status()\n            if response.status_code == 403:\n                raise Exception(f\"403 Forbidden: Access denied for URL {urltoInvoke}\")\n            if response.status_code != 200:\n                response.raise_for_status()\n            graph_data = response.json()\n            localUserList.extend(graph_data.get('value', []))\n            if \"@odata.nextLink\" in graph_data:\n                urltoInvoke = graph_data[\"@odata.nextLink\"]\n            else:\n                continueLooping = False\n        return localUserList\n    except Exception as e:\n        if hasattr(e, 'args') and e.args and '403 Forbidden' in str(e.args[0]):\n            raise\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n</code></pre> <p>Explanation: - Handles authentication, pagination, throttling, and error handling for Microsoft Graph API requests. - Used by all higher-level functions to retrieve data from Entra (Azure AD).  For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p>"},{"location":"azure-ad-user/#conclusion","title":"Conclusion","text":"<p>By following this step-by-step approach, you can programmatically retrieve all users from Entra (Azure AD), along with their group memberships and license assignments, using Python and the Microsoft Graph API. The modular design allows for easy extension and integration into enterprise automation workflows.</p> <p>For more details, see the Microsoft Graph API documentation.</p>"},{"location":"azure-bcpdr-github-action/","title":"Azure BCP/DR with GitHub Actions: Fully Automated Disaster Recovery","text":"<p>This article demonstrates how to automate the entire Azure Business Continuity/Disaster Recovery (BCP/DR) process using GitHub Actions. If you want to understand the step-by-step process, rationale, and all code involved, first review:</p> <ul> <li>Part 1: Resource Group, Storage, and Network Foundation</li> <li>Part 2: Compute, Firewall, VPN, and Restore</li> </ul> <p>Those articles walk through each step and code block manually. Here, you will see how to run the same process end-to-end with a single click using GitHub Actions.</p>"},{"location":"azure-bcpdr-github-action/#why-automate-with-github-actions","title":"Why Automate with GitHub Actions?","text":"<ul> <li>Consistency: Every DR run is identical and repeatable.</li> <li>Speed: Deploy all infrastructure and restore VMs with minimal manual intervention.</li> <li>Auditability: All actions are logged in GitHub for compliance and troubleshooting.</li> <li>Integration: Easily tie into your existing CI/CD and approval workflows.</li> </ul>"},{"location":"azure-bcpdr-github-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Service Principal with Owner rights on the subscription (see Part 1).</li> <li>Manual creation of the DR deployment resource group (<code>dr-rg-gdep-pwus-deployment</code>) in West US.</li> <li>Valid and up-to-date VM list JSON files.</li> </ul>"},{"location":"azure-bcpdr-github-action/#overview-of-the-automated-process","title":"Overview of the Automated Process","text":"<p>The automation is split into three GitHub Actions workflows:</p> <ol> <li>BCP-DR Infrastructure: Deploys all Azure infrastructure (resource groups, storage, network, firewall, VPN, etc.)</li> <li>BCP-DR Restore VMs: Restores VMs from backup into the DR region.</li> <li>BCP-DR Attach NIC(s): Ensures restored VMs have the correct NICs and IPs.</li> </ol> <p>Each workflow can be triggered manually from the GitHub Actions tab.</p>"},{"location":"azure-bcpdr-github-action/#step-1-deploy-infrastructure-with-github-actions","title":"Step 1: Deploy Infrastructure with GitHub Actions","text":"<p>Workflow File: <code>.github/workflows/bcpdrinfrastructure.yml</code></p> <pre><code>name: 01-BCP-DR Build Infrastructure\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      # ...existing code for deploying Bicep files for all resources (see full YAML above)...\n</code></pre> <p>Explanation: - This workflow deploys all BCP/DR infrastructure using the same Bicep files as in Part 1 and Part 2. - Each step uses the <code>azure/arm-deploy@v1</code> action to deploy a specific Bicep template. - You can select the environment (Development or Production) when running the workflow.</p>"},{"location":"azure-bcpdr-github-action/#step-2-restore-vms-in-the-dr-region","title":"Step 2: Restore VMs in the DR Region","text":"<p>Workflow File: <code>.github/workflows/bcpdrvms.yml</code></p> <pre><code>name: 02-BCP-DR Restore VMs\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure-restore-vms:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      - name: Install PowerShell\n        run: |\n          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n          sudo apt-get update\n          sudo apt-get install -y powershell\n      - name: Install Azure CLI\n        run: |\n          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n      - name: Create the VM(s) based on vmlist1.json\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        run: |\n          pwsh ./scripts/bcpdr/vm/restorevms.ps1 -restorediskonly \"false\" -numberofhours2wait 5 -vmlist './scripts/bcpdr/vm/vmlist.json'\n        env:\n          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}\n</code></pre> <p>Explanation: - This workflow restores VMs from backup using the provided PowerShell script and VM list. - Installs PowerShell Core and Azure CLI as needed. - All restore operations are logged in GitHub Actions.</p>"},{"location":"azure-bcpdr-github-action/#step-3-attach-nics-to-restored-vms","title":"Step 3: Attach NICs to Restored VMs","text":"<p>Workflow File: <code>.github/workflows/bcpdrvmnics.yml</code></p> <pre><code>name: 03-BCP-DR Attach NIC(s)\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-bcp-dr-infrastructure-restore-vmnics:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      - name: Install PowerShell\n        run: |\n          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n          sudo apt-get update\n          sudo apt-get install -y powershell\n      - name: Install Azure CLI\n        run: |\n          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n      - name: Create and Attach NIC(s) to restored VM(s)\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        run: |\n          pwsh ./scripts/bcpdr/vm/attachnics.ps1 -vaultname \"rsv-prod-eus-01\" -vaultresourcegroupname \"rg-gdep-peus-backup\" -vmlist './scripts/bcpdr/vm/vmlist.json'\n        env:\n          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}\n</code></pre> <p>Explanation: - This workflow ensures that all restored VMs have the correct NICs and IP addresses, matching the original environment. - Can be run multiple times to ensure NICs are correct.</p>"},{"location":"azure-bcpdr-github-action/#how-to-run-the-workflows","title":"How to Run the Workflows","text":"<ol> <li>Go to your GitHub repository's Actions tab.</li> <li>Select the workflow you want to run (Infrastructure, Restore VMs, Attach NICs).</li> <li>Click Run workflow, select the environment, and start the workflow.</li> <li>Monitor progress and logs directly in GitHub.</li> </ol>"},{"location":"azure-bcpdr-github-action/#summary","title":"Summary","text":"<ul> <li>This approach automates the entire Azure BCP/DR process described in Part 1 and Part 2.</li> <li>All code, infrastructure, and restore steps are executed via GitHub Actions for speed, repeatability, and auditability.</li> </ul> <p>Ready to automate your DR? Trigger your first workflow in GitHub Actions!</p>"},{"location":"azure-bcpdr-part1/","title":"Azure BCP/DR with Backup &amp; Restore: Part 1 \u2013 Resource Group, Storage, and Network Foundation","text":"<p>This multi-part technical blog series walks you through a practical, cost-effective approach to Business Continuity and Disaster Recovery (BCP/DR) in Azure using backup and restore, rather than Azure Site Recovery. The scenario targets restoring all critical infrastructure from Azure East (primary) to Azure West (DR region). Each step is explained with code and rationale, so you can duplicate this in your own environment.</p> <p>Part 2: Compute, Firewall, and Final Steps</p>"},{"location":"azure-bcpdr-part1/#why-this-approach","title":"Why This Approach?","text":"<ul> <li>Cost-Effective: Backup and restore avoids the ongoing costs of Azure Site Recovery (ASR) replication.</li> <li>Simplicity: You control what is restored and when, with clear, auditable steps.</li> <li>RTO/RPO: Recovery Time Objective (RTO) and Recovery Point Objective (RPO) are acceptable for many workloads, as restore times are predictable and backups are recent.</li> <li>Flexibility: You can restore to any region, in this case from East to West US.</li> </ul>"},{"location":"azure-bcpdr-part1/#step-1-create-the-target-resource-group-manual","title":"Step 1: Create the Target Resource Group (Manual)","text":"<p>This is the only step that must be done manually in the Azure Portal or CLI. It creates the DR resource group in the target region (West US).</p> <ul> <li>Resource Group Name: <code>dr-rg-gdep-pwus-deployment</code></li> <li>Region: West US</li> <li>Tags: Infrastructure, Disaster Recovery</li> </ul> <p>Command: <pre><code>az group create --name dr-rg-gdep-pwus-deployment --location westus --tags \"Purpose=Infrastructure\" \"Type=DisasterRecovery\"\n</code></pre></p>"},{"location":"azure-bcpdr-part1/#step-2-deploy-resource-groups-via-bicep","title":"Step 2: Deploy Resource Groups via Bicep","text":"<p>This step uses a Bicep template to deploy any additional resource groups needed for the DR environment.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-rg --template-file ./rg/rg_main.bicep --resource-group dr-rg-gdep-pwus-deployment\n</code></pre></p> <p>Bicep Code: <code>rg_main.bicep</code> <pre><code>param location string = 'westus'\nvar environmetag = 'Disaster Recovery'\n\nparam resourcegroups2create array = [\n  'dr-rg-gdep-pwus-infrastructure'\n  'dr-rg-gdep-pwus-vnets'\n  'dr-rg-gdep-pwus-fortinet'\n  'dr-rg-gdep-pwus-meraki-sdwan'\n]\n\nmodule rggdepwus './rg.bicep' = [for rggroupname in resourcegroups2create: {\n  name: '${deployment().name}-${rggroupname}'\n  scope:subscription()\n  params: {\n    name: rggroupname\n    location: location\n    tags: {Application:'Infrastructure',Environment: environmetag}\n    }\n}]\n</code></pre></p> <p>Supporting Bicep: <code>rg.bicep</code> <pre><code>targetScope='subscription'\n\nparam name string \nparam location string\nparam tags object\n\nresource rggdepdrwus 'Microsoft.Resources/resourceGroups@2022-09-01' = {\n  name: name\n  location: location\n  tags: tags\n}\noutput resourceGroupName string = rggdepdrwus.name\n</code></pre></p> <p>Explanation: - <code>rg_main.bicep</code> loops through a list of DR resource group names and deploys each using the <code>rg.bicep</code> module. - <code>rg.bicep</code> creates a resource group at the subscription level with the specified name, location, and tags. - This ensures all required DR resource groups are created consistently and tagged for easy management.</p>"},{"location":"azure-bcpdr-part1/#step-3-create-staging-storage-account","title":"Step 3: Create Staging Storage Account","text":"<p>Deploy a storage account for staging backups and other DR artifacts.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-sa-staging --resource-group dr-rg-gdep-pwus-deployment --template-file ./storage/sa/sa_staging.bicep\n</code></pre></p> <p>Bicep Code: <code>sa_staging.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\n\n@description('This Storage Account is used as a Staging account to restore VM(s)')\nmodule sagdepdrwusstaging './sa.bicep' = {\n  name: '${deployment().name}-sa-gdep-pwus-staging'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'storegdeppwusstaging'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      accessTier: 'Hot'\n      allowBlobPublicAccess: false\n      allowCrossTenantReplication: false\n      allowSharedKeyAccess: true\n      defaultToOAuthAuthentication: false\n      dnsEndpointType: 'Standard'\n      encryption: {\n        keySource: 'Microsoft.Storage'\n        requireInfrastructureEncryption: false\n        services: {\n          blob: {\n            enabled: true\n            keyType: 'Account'\n          }\n          file: {\n            enabled: true\n            keyType: 'Account'\n          }\n        }\n      }\n      largeFileSharesState: 'Enabled'\n      minimumTlsVersion: 'TLS1_2'\n      networkAcls: {\n        bypass: 'AzureServices'\n        defaultAction: 'Allow'\n        ipRules: []\n        virtualNetworkRules: []\n      }\n      publicNetworkAccess: 'Enabled'\n      supportsHttpsTrafficOnly: true\n    }\n  }\n}\n</code></pre></p> <p>Supporting Bicep: <code>sa.bicep</code> <pre><code>param location string\nparam name string\nparam tags object\nparam properties object\n\nresource sagdepdrwus 'Microsoft.Storage/storageAccounts@2023-04-01' = {\n  name: name\n  location: location\n  tags: tags\n  kind: 'StorageV2'\n  sku: {\n    name: 'Standard_LRS'\n  }\n  properties: properties\n}\n\nresource sagdepdrwusblobservices 'Microsoft.Storage/storageAccounts/blobServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n    deleteRetentionPolicy: {\n      allowPermanentDelete: false\n      enabled: false\n    }\n  }\n}\n\nresource sagdepdrwusfileservices 'Microsoft.Storage/storageAccounts/fileServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n    protocolSettings: {\n      smb: {}\n    }\n  }\n}\n\nresource sagdepdrwusqueueservices 'Microsoft.Storage/storageAccounts/queueServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n  }\n}\n\nresource sagdepdrwustableservices 'Microsoft.Storage/storageAccounts/tableServices@2023-04-01' = {\n  parent: sagdepdrwus\n  name: 'default'\n  properties: {\n    cors: {\n      corsRules: []\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - <code>sa_staging.bicep</code> deploys a storage account for DR staging in the infrastructure resource group, with secure settings and encryption. - The <code>sa.bicep</code> module provisions the storage account and all required services (blob, file, queue, table). - This storage account is used for storing backup files, scripts, and logs during the DR restore process.</p>"},{"location":"azure-bcpdr-part1/#step-4-create-network-security-groups-nsgs","title":"Step 4: Create Network Security Groups (NSGs)","text":"<p>Deploy NSGs to secure your DR environment.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-nsg --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/nsg/nsg_main.bicep\n</code></pre></p> <p>Bicep Code: <code>nsg_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\n\n@description('Used By Most of our Subnets')\nmodule nsggdepdrwusdefault './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-default'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-default'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {}\n  }\n}\n@description('Used By Meraki Public Subnet')\nmodule nsggdepdrwusmeraki './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-meraki'\n  scope: resourceGroup(infrastructure_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-meraki'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      securityRules: [\n        {\n          name: 'AllowAnyCustom443Inbound'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            description: 'Per Abhishek this may be required when (if) we enable Cisco Anyconnect'\n            protocol: '*'\n            sourcePortRange: '*'\n            destinationPortRange: '*'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 100\n            direction: 'Inbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n        {\n          name: 'AllowAnyCustom443InboundUDP'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            protocol: 'UDP'\n            sourcePortRange: '443'\n            destinationPortRange: '443'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 110\n            direction: 'Inbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n        {\n          name: 'AllowAny'\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n          properties: {\n            protocol: '*'\n            sourcePortRange: '*'\n            destinationPortRange: '*'\n            sourceAddressPrefix: '*'\n            destinationAddressPrefix: '*'\n            access: 'Allow'\n            priority: 120\n            direction: 'Outbound'\n            sourcePortRanges: []\n            destinationPortRanges: []\n            sourceAddressPrefixes: []\n            destinationAddressPrefixes: []\n          }\n        }\n      ]\n    }\n  }\n}\n@description('Used By Fortinet Firewall')\nmodule nsggdepdrwusfortinet './nsg.bicep' = {\n  name: '${deployment().name}-nsg-gdep-pwus-fortinet'\n  scope: resourceGroup(fortinet_rg_name)\n  params: {\n    name: 'nsg-gdep-pwus-fortinet'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      securityRules: [\n        {\n          name: 'AllowAllOutbound'\n          properties: {\n            access: 'Allow'\n            description: 'Allow all out'\n            destinationAddressPrefix: '*'\n            destinationAddressPrefixes: []\n            destinationPortRange: '*'\n            destinationPortRanges: []\n            direction: 'Outbound'\n            priority: 105\n            protocol: '*'\n            sourceAddressPrefix: '*'\n            sourceAddressPrefixes: []\n            sourcePortRange: '*'\n            sourcePortRanges: []\n          }\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n        }\n        {\n          name: 'AllowAllInbound'\n          properties: {\n            access: 'Allow'\n            destinationAddressPrefix: '*'\n            destinationAddressPrefixes: []\n            destinationPortRange: '*'\n            destinationPortRanges: []\n            direction: 'Inbound'\n            priority: 110\n            protocol: '*'\n            sourceAddressPrefix: '*'\n            sourceAddressPrefixes: []\n            sourcePortRange: '*'\n            sourcePortRanges: []\n          }\n          type: 'Microsoft.Network/networkSecurityGroups/securityRules'\n        }\n      ]\n    }\n  }\n}\n</code></pre></p> <p>Supporting Bicep: <code>nsg.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource nsggdepdrwus 'Microsoft.Network/networkSecurityGroups@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\n</code></pre></p> <p>Explanation: - <code>nsg_main.bicep</code> deploys three NSGs: default, Meraki, and Fortinet, each with appropriate rules for their subnet roles. - The <code>nsg.bicep</code> module provisions the NSG with the specified rules and tags. - NSGs are critical for controlling traffic flow and securing your DR network.</p>"},{"location":"azure-bcpdr-part1/#step-5-create-route-tables","title":"Step 5: Create Route Tables","text":"<p>Deploy User Defined Route (UDR) tables for custom routing.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-rt --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/udr/udr_main.bicep\n</code></pre></p> <p>Bicep Code: <code>udr_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\n\n//var infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\nvar meraki_sdwan_rg_name = 'dr-rg-gdep-pwus-meraki-sdwan'\n\n@description('Used to route traffic intended to go to On Premise network from Azure Hub')\nmodule udrgdepdrwushub2onprem './udr.bicep' = {\n  name: '${deployment().name}-route-gdep-pwus-azurehub-onprem'\n  scope: resourceGroup(fortinet_rg_name)\n  params: {\n    name: 'route-gdep-pwus-azurehub-onprem'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      disableBgpRoutePropagation: false\n      routes: [ ... ]\n    }\n  }\n}\n// ...additional modules for all required UDRs (see full code in repo)...\n</code></pre></p> <p>Supporting Bicep: <code>udr.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource rtgdepdrwus 'Microsoft.Network/routeTables@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\n</code></pre></p> <p>Explanation: - <code>udr_main.bicep</code> deploys all required route tables for the DR environment, including routes for on-premises, spokes, and SDWAN. - The <code>udr.bicep</code> module provisions each route table with the specified routes and settings. - UDRs are essential for custom traffic flow and integration with firewalls and VPNs.</p>"},{"location":"azure-bcpdr-part1/#step-6-create-virtual-networks-and-subnets","title":"Step 6: Create Virtual Networks and Subnets","text":"<p>Deploy VNETs, subnets, and associate them with NSGs and UDRs.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-vnet --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/vnet/vnet_main.bicep\n</code></pre></p> <p>Bicep Code: <code>vnet_main.bicep</code> <pre><code>param location string = resourceGroup().location\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\n\n/*\nvar infrastructure_rg_name = 'dr-rg-gdep-pwus-infrastructure'\nvar fortinet_rg_name = 'dr-rg-gdep-pwus-fortinet'\nvar meraki_sdwan_rg_name = 'dr-rg-gdep-pwus-meraki-sdwan'\n*/\nvar vnet_rg_name = 'dr-rg-gdep-pwus-vnets'\n\n@description('Hub Virtual Network with NVA namely Fortinet Firewall')\nmodule vnetgdepdrwusfortinet './vnet.bicep' = {\n  name: '${deployment().name}-vnet-gdep-pwus-fortinet'\n  scope: resourceGroup(vnet_rg_name)\n  params: {\n    name: 'vnet-gdep-pwus-fortinet'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      addressSpace: {\n        addressPrefixes: [ ... ]\n      }\n      dhcpOptions: {\n        dnsServers: [ ... ]\n      }\n      enableDdosProtection: false\n      subnets: [ ... ]\n    }\n  }\n}\n// ...additional modules for management and production spokes, and VNET peering (see full code in repo)...\n</code></pre></p> <p>Supporting Bicep: <code>vnet.bicep</code> <pre><code>param location string \nparam name string \nparam tags object\nparam properties object\n\nresource vnetgdepdrwus 'Microsoft.Network/virtualNetworks@2023-09-01'={\n  name:name\n  location:location\n  tags:tags\n  properties:properties\n}\noutput virtualnetworkname string = vnetgdepdrwus.name\n</code></pre></p> <p>Explanation: - <code>vnet_main.bicep</code> provisions all required virtual networks and subnets for the DR environment, including hub, management, and production spokes. - Subnets are associated with NSGs and UDRs as needed, and VNET peering is configured for connectivity. - The <code>vnet.bicep</code> module provisions each VNET with the specified address spaces, subnets, and settings.</p> <p>Continue to Part 2: Compute, Firewall, and Final Steps</p>"},{"location":"azure-bcpdr-part2/","title":"Azure BCP/DR with Backup &amp; Restore: Part 2 \u2013 Compute, Firewall, VPN, and Restore","text":"<p>Back to Part 1: Resource Group, Storage, and Network Foundation</p> <p>This part continues the step-by-step BCP/DR process, focusing on compute, firewall, VPN, and restoring VMs. All steps are automated using Bicep and PowerShell, but executed manually for full control and auditability.</p>"},{"location":"azure-bcpdr-part2/#step-7-deploy-fortinet-firewall","title":"Step 7: Deploy Fortinet Firewall","text":"<p>Deploy the Fortinet firewall solution, including load balancers, NVAs, and all required public IPs and NICs.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-nva --resource-group dr-rg-gdep-pwus-deployment --template-file ./compute/vm/nva/nva.bicep --parameters ./compute/vm/nva/nva.json\n</code></pre></p> <p>Bicep Code: <code>nva.bicep</code> <pre><code>param location string = resourceGroup().location\nparam resource_group string\nparam adminusername string\n@secure()\nparam adminPassword string\nparam fortinetoffer string //Will need to check what is availabe at the time of DR (fortinet_fortigate-vm_v5)\nparam fortinetsku string //Will need to check what is availabe at the time of DR (fortinet_fg-vm_payg_2023)\n\n// ...existing code for variables and modules (see repo for full details)...\n// This Bicep deploys:\n// - Public IPs for Fortinet\n// - External and internal load balancers\n// - 8 NICs for Fortinet VMs\n// - 2 Fortinet NVA VMs with all required extensions\n</code></pre></p> <p>Parameter File: <code>nva.json</code> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"resource_group\": {\n      \"value\": \"dr-rg-gdep-pwus-fortinet\"\n    },\n    \"adminusername\": {\n      \"value\": \"nvafwadmin\"\n    },\n    \"fortinetoffer\": {\n      \"value\": \"fortinet_fortigate-vm_v5\"\n    },\n    \"fortinetsku\": {\n      \"value\": \"fortinet_fg-vm_payg_2023\"\n    },\n    \"adminPassword\": {\n      \"reference\": {\n        \"keyVault\": {\n          \"id\": \"/subscriptions/df8d9f29-f5c5-4e48-a004-21ea3b8a4834/resourceGroups/rg-gdep-peus-applications/providers/Microsoft.KeyVault/vaults/kv-gdep-peus-iac\"\n        },\n        \"secretName\": \"new-vm-password\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - The Bicep file provisions Fortinet NVAs, load balancers, public IPs, NICs, and all required networking for the DR firewall solution. - The parameter file provides values for resource group, admin credentials, and Fortinet offer/SKU.</p>"},{"location":"azure-bcpdr-part2/#step-8-fortinet-firewall-manual-steps","title":"Step 8: Fortinet Firewall Manual Steps","text":"<ul> <li>Obtain or create a backup of the NVA configuration (from SFTP or by running <code>backup.py</code> in the scripts folder).</li> <li>Update alias/host names as needed for the DR region.</li> <li>Import configuration via the Fortinet web interface for each NVA in West US.</li> <li>Confirm settings via Serial Console.</li> <li>Ensure Point-to-Site VPN is up before proceeding.</li> </ul>"},{"location":"azure-bcpdr-part2/#step-9-deploy-point-to-site-vpn","title":"Step 9: Deploy Point-to-Site VPN","text":"<p>Deploy the P2S VPN gateway and configuration.</p> <p>Command: <pre><code>az deployment group create --name gdepdr-p2s --resource-group dr-rg-gdep-pwus-deployment --template-file ./networking/p2s/p2s.bicep --parameters ./networking/p2s/p2sp.json\n</code></pre></p> <p>Bicep Code: <code>p2s.bicep</code> <pre><code>/*\nLets create Point to Site VPN for users to connect into US West\nCreate PIP, VPN Gateway, for PIP we have no dependency but \nfor VPN Gateway we do.  As such notice \n*/\nparam location string = resourceGroup().location\nparam resource_group string\n\nvar applicationtag = 'Infrastructure'\nvar environmetag = 'Disaster Recovery'\nvar AADTenant = '${environment().authentication.loginEndpoint}${subscription().tenantId}'\n\nmodule pipgdepdrfw '../../networking/pip/pip.bicep' = {\n  name: '${deployment().name}-pipp2svpn'\n  scope: resourceGroup(resource_group)\n  params: {\n    name: 'pip-gdep-pwus-p2svpn'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      publicIPAllocationMethod: 'Static'\n      publicIPAddressVersion: 'IPv4'\n    }\n  }\n}\n//Create VPN Gateway\nmodule vngwpoint2site './p2sngw.bicep' = {\n  name: '${deployment().name}-vngwpoint2site'\n  scope: resourceGroup('dr-rg-gdep-pwus-vnets')\n  params: {\n    name: 'vngw_gdep_pwus'\n    location: location\n    tags: { Application: applicationtag, Environment: environmetag }\n    properties: {\n      enablePrivateIpAddress: false\n      ipConfigurations: [ ... ]\n      sku: {\n        name: 'VpnGw2'\n        tier: 'VpnGw2'\n      }\n      gatewayType: 'Vpn'\n      vpnType: 'RouteBased'\n      enableBgp: false\n      activeActive: false\n      vpnClientConfiguration: {\n        vpnClientAddressPool: {\n          addressPrefixes: ['10.27.48.0/22']\n        }\n        vpnClientProtocols: ['OpenVPN']\n        vpnAuthenticationTypes: ['AAD']\n        aadTenant: AADTenant\n        aadAudience: '41b23e61-6c1e-4545-b367-cd054e0ed4b4'\n        aadIssuer: '${'https://sts.windows.net/'}${subscription().tenantId}${'/'}'\n      }\n      customRoutes: {\n        addressPrefixes: [ ... ]\n      }\n      vpnGatewayGeneration: 'Generation2'\n    }\n  }\n}\n</code></pre></p> <p>Parameter File: <code>p2sp.json</code> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"resource_group\": {\n      \"value\": \"dr-rg-gdep-pwus-fortinet\"\n    }\n  }\n}\n</code></pre></p> <p>Explanation: - The Bicep file provisions the VPN gateway, public IP, and all required settings for secure remote access to the DR environment.</p>"},{"location":"azure-bcpdr-part2/#step-10-restore-virtual-machines-or-disks","title":"Step 10: Restore Virtual Machines or Disks","text":"<p>Restore VMs using PowerShell Core. All VMs to be restored should be listed in <code>vmlist.json</code>.</p> <p>Command: <pre><code>pwsh ./scripts/bcpdr/vm/restorevms.ps1 -restorediskonly \"false\" -numberofhours2wait 5 -vmlist './scripts/bcpdr/vm/vmlist.json'\n</code></pre></p> <p>PowerShell Script: <code>restorevms.ps1</code> <pre><code># Boolean variable to indicate whether to restore disks only\n# If it is True then VM will also be created so be careful \nparam (\n    [string] $restorediskonly = \"false\",\n    [int] $numberofhours2wait = 2,\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json'\n)\n\n# ...existing code for restore logic (see repo for full details)...\n# This script restores VMs or disks from backup, as defined in the JSON file.\n# Set -restorediskonly to false to create VMs directly.\n# Adjust -numberofhours2wait as needed for your restore window.\n</code></pre></p> <p>Explanation: - The script restores VMs or disks from backup, as defined in the JSON file. - Set <code>-restorediskonly</code> to <code>false</code> to create VMs directly. - Adjust <code>-numberofhours2wait</code> as needed for your restore window.</p>"},{"location":"azure-bcpdr-part2/#step-11-attach-nics-to-restored-vms","title":"Step 11: Attach NICs to Restored VMs","text":"<p>Attach NICs to the restored VMs using PowerShell Core. This can be run multiple times safely.</p> <p>Command: <pre><code>pwsh ./scripts/bcpdr/vm/attachnics.ps1 -vaultname \"&lt;your-recovery-vault&gt;\" -vaultresourcegroupname \"&lt;your-backup-rg&gt;\" -vmlist './scripts/bcpdr/vm/vmlist.json'\n</code></pre></p> <p>PowerShell Script: <code>attachnics.ps1</code> <pre><code>param (\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json',\n    [string] $vaultname = 'rsv-prod-eus-01',\n    [string] $vaultresourcegroupname = 'rg-gdep-peus-backup'\n)\n\n# ...existing code for NIC attachment logic (see repo for full details)...\n# This script attaches network interfaces to the restored VMs.\n# Replace &lt;your-recovery-vault&gt; and &lt;your-backup-rg&gt; with your actual values.\n</code></pre></p> <p>Explanation: - The script attaches network interfaces to the restored VMs. - Replace <code>&lt;your-recovery-vault&gt;</code> and <code>&lt;your-backup-rg&gt;</code> with your actual values.</p>"},{"location":"azure-bcpdr-part2/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you need to re-run any step, you can safely delete previous deployments and start again.</li> <li>All steps are idempotent and can be repeated as needed.</li> </ul>"},{"location":"azure-bcpdr-part2/#summary-why-backup-restore","title":"Summary: Why Backup &amp; Restore?","text":"<ul> <li>Cost: No ongoing replication costs as with Azure Site Recovery (ASR).</li> <li>Control: You decide what to restore and when.</li> <li>RTO/RPO: For many workloads, restore times and backup frequency are sufficient.</li> <li>Flexibility: Restore to any region (here, from East to West US).</li> </ul> <p>When to Use This Approach: - When RTO/RPO requirements are not sub-minute. - When cost is a concern. - When you want full control over the DR process.</p> <p>When to Use Azure Site Recovery: - When you need near-instant failover and minimal data loss. - When you want fully automated DR with minimal manual steps.</p> <p>Back to Part 1</p>"},{"location":"azure-billing/","title":"Download Azure Bill","text":""},{"location":"azure-billing/#programmatically-downloading-and-storing-azure-billing-data","title":"Programmatically Downloading and Storing Azure Billing Data:","text":""},{"location":"azure-billing/#introduction","title":"Introduction","text":"<p>Automating the retrieval and storage of Azure billing data is essential for organizations seeking cost transparency and operational efficiency. This guide details a robust, production-grade approach to programmatically obtaining Azure billing data using Python, authenticating securely with certificates, and efficiently storing the results in a SQL Server database. </p>"},{"location":"azure-billing/#1-secure-authentication-acquiring-an-azure-access-token-with-certificates","title":"1. Secure Authentication: Acquiring an Azure Access Token with Certificates","text":"<p>The first step is to authenticate with Azure Active Directory (Azure AD) using certificate-based authentication. This is more secure than using client secrets and is recommended for automation and service-to-service scenarios. For a deep dive into certificate-based authentication setup, see the dedicated article: Certificate Based Authorization for Azure AD.</p> <p>Python Example:</p> <pre><code>from msal import ConfidentialClientApplication\n\ndef get_access_token(client_id, authority, tenant_id, resource_scopes, cert_thumbprint, cert_key_path):\n    \"\"\"\n    Acquire an Azure AD access token using certificate-based authentication.\n    \"\"\"\n    with open(cert_key_path, \"r\") as key_file:\n        private_key = key_file.read()\n    app = ConfidentialClientApplication(\n        client_id=client_id,\n        authority=f\"{authority}{tenant_id}\",\n        client_credential={\n            \"thumbprint\": cert_thumbprint,\n            \"private_key\": private_key,\n        },\n    )\n    result = app.acquire_token_for_client(scopes=resource_scopes)\n    if \"access_token\" not in result:\n        raise Exception(f\"Token acquisition failed: {result}\")\n    return result[\"access_token\"]\n</code></pre> <ul> <li>Why certificates? They are more secure, support longer lifecycles, and are recommended for automation.</li> <li>MSAL Library: The Microsoft Authentication Library (MSAL) is used for token acquisition, providing flexibility and support for advanced scenarios.</li> </ul>"},{"location":"azure-billing/#2-generating-the-azure-cost-report-via-rest-api","title":"2. Generating the Azure Cost Report via REST API","text":"<p>Once authenticated, you can use the Azure Cost Management API to request a cost details report for your subscription. This involves making a POST request to the appropriate endpoint and polling until the report is ready.</p> <p>Python Example:</p> <pre><code>import requests\nimport time\nimport json\n\ndef generate_azure_cost_report(subscription_id, access_token, start_date, end_date, api_version=\"2022-05-01\"):\n    url = f\"https://management.azure.com/subscriptions/{subscription_id}/providers/Microsoft.CostManagement/generateCostDetailsReport?api-version={api_version}\"\n    payload = json.dumps({\"metric\": \"ActualCost\", \"timePeriod\": {\"start\": start_date, \"end\": end_date}})\n    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}\n    response = requests.post(url, headers=headers, data=payload)\n    # Poll until the report is ready\n    while response.status_code == 202:\n        location_url = response.headers.get('Location')\n        retry_after = int(response.headers.get('Retry-After', 30))\n        time.sleep(retry_after)\n        response = requests.get(url=location_url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to generate cost report: {response.status_code} - {response.text}\")\n    return response.json()\n</code></pre> <ul> <li>Polling: The API may return a 202 status, indicating the report is being generated. Poll the <code>Location</code> header until a 200 response is received.</li> <li>Error Handling: Always check for non-200 responses and handle errors appropriately.</li> </ul>"},{"location":"azure-billing/#3-downloading-the-cost-report-data","title":"3. Downloading the Cost Report Data","text":"<p>The response from the cost report API includes a manifest with one or more blob URLs. Download these blobs to obtain the actual cost data, typically in CSV format.</p> <p>Python Example:</p> <pre><code>import urllib3\n\ndef download_cost_report_blobs(manifest, output_path):\n    http = urllib3.PoolManager()\n    for blob in manifest['blobs']:\n        blob_url = blob['blobLink']\n        with open(output_path, 'wb') as out_file:\n            blob_response = http.request('GET', blob_url, preload_content=False)\n            out_file.write(blob_response.data)\n</code></pre> <ul> <li>Blob Download: Use a robust HTTP client (e.g., <code>urllib3</code>) to download the report data.</li> <li>Output: Save the CSV file to a secure, accessible location for further processing.</li> </ul>"},{"location":"azure-billing/#4-loading-the-cost-data-into-sql-server-efficiently","title":"4. Loading the Cost Data into SQL Server Efficiently","text":"<p>After downloading the cost report, the next step is to load the data into a SQL Server table. For large datasets, use a fast, batch insert method to optimize performance.</p> <p>Python Example:</p> <pre><code>import pyodbc\nimport csv\n\ndef load_csv_to_sql_server(csv_path, connection_string, table_name):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    with open(csv_path, 'r', encoding='utf-8-sig') as csvfile:\n        reader = csv.reader(csvfile)\n        columns = next(reader)  # Header row\n        insert_query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['?' for _ in columns])})\"\n        data = list(reader)\n        cursor.fast_executemany = True\n        cursor.executemany(insert_query, data)\n        conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Fast Insert: The <code>fast_executemany</code> flag in <code>pyodbc</code> enables high-performance bulk inserts.</li> <li>Schema Alignment: Ensure the CSV columns match the SQL table schema.</li> </ul>"},{"location":"azure-billing/#5-orchestrating-the-end-to-end-process","title":"5. Orchestrating the End-to-End Process","text":"<p>A typical workflow to automate Azure billing data retrieval and storage:</p> <pre><code>def fetch_and_update_azure_billing_data():\n    # Step 1: Get access token\n    access_token = get_access_token(\n        client_id=..., authority=..., tenant_id=..., resource_scopes=..., cert_thumbprint=..., cert_key_path=...\n    )\n    # Step 2: Generate cost report\n    report = generate_azure_cost_report(\n        subscription_id=..., access_token=access_token, start_date=..., end_date=...\n    )\n    # Step 3: Download report blob(s)\n    download_cost_report_blobs(report['manifest'], output_path=\"azure_billing.csv\")\n    # Step 4: Load into SQL Server\n    load_csv_to_sql_server(\n        csv_path=\"azure_billing.csv\", connection_string=..., table_name=\"AzureBilling\"\n    )\n</code></pre>"},{"location":"azure-billing/#6-additional-considerations","title":"6. Additional Considerations","text":"<ul> <li>Permissions: The Azure AD application must have the required API permissions (e.g., Cost Management Reader) and access to the subscription.</li> <li>Certificate Security: Store private keys securely and never commit them to source control.</li> <li>Error Handling: Implement robust error handling and logging for production use.</li> <li>Scheduling: Use a scheduler (e.g., cron, Azure Automation) to run the process regularly.</li> </ul>"},{"location":"azure-billing/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can securely and efficiently automate the retrieval and storage of Azure billing data using Python. This enables advanced reporting, cost analysis, and integration with enterprise data platforms.</p>"},{"location":"azure-billing/#references","title":"References","text":"<ul> <li>Azure Cost Management REST API</li> <li>MSAL Python Library</li> <li>pyodbc Documentation</li> <li>Azure AD App Registration: Certificates &amp; Secrets</li> </ul>"},{"location":"azure-container-schedule/","title":"Orchestrating Scheduled Jobs in a Python Container Using a Scheduler Script","text":"<p>This article demonstrates how to build a robust job scheduler in a Python-based container environment. The scheduler coordinates the execution of various Python scripts at specific times or intervals, ensuring that business processes run reliably and in the correct timezone. The approach is suitable for any containerized environment, such as those running in Azure, AWS, or on-premises.</p>"},{"location":"azure-container-schedule/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Key Concepts</li> <li>Core Scheduler Script<ul> <li>Scheduling Jobs</li> <li>Running Jobs in the Background</li> </ul> </li> <li>Container Startup with Supervisor</li> <li>Conclusion</li> </ol>"},{"location":"azure-container-schedule/#overview","title":"Overview","text":"<p>In many automation and integration scenarios, you need to run a set of scripts or jobs on a schedule\u2014some daily, some every few minutes, and some only on certain days. When running in a container, you want the scheduler to start automatically and keep running, launching jobs as needed. This article explains how to implement such a scheduler in Python, using the <code>schedule</code> library, and how to ensure it starts with your container using Supervisor.</p>"},{"location":"azure-container-schedule/#key-concepts","title":"Key Concepts","text":"<ul> <li>Job Scheduling: Use the <code>schedule</code> library to define when each job should run (e.g., every day at a certain time, every N minutes).</li> <li>Background Execution: Use Python's <code>threading</code> module to run jobs asynchronously, so the scheduler loop is never blocked.</li> <li>Timezone Awareness: Use the <code>pytz</code> library to ensure jobs run at the correct local time, even if the container's system time is UTC.</li> <li>Error Handling: Use try/except blocks and logging to capture and report errors without stopping the scheduler.</li> <li>Container Startup: Use Supervisor to ensure the scheduler script starts automatically when the container is created and restarts if it fails.</li> </ul>"},{"location":"azure-container-schedule/#core-scheduler-script","title":"Core Scheduler Script","text":"<p>Below is a simplified and generic version of a Python scheduler script suitable for containerized environments. All company-specific details have been removed.</p>"},{"location":"azure-container-schedule/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>import schedule\nimport time\nimport subprocess\nimport threading\nimport sys\nimport pytz\nfrom datetime import datetime, timedelta\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"scheduler\")\n</code></pre>"},{"location":"azure-container-schedule/#helper-functions","title":"Helper Functions","text":""},{"location":"azure-container-schedule/#timezone-aware-scheduling","title":"Timezone Aware Scheduling","text":"<pre><code>def get_next_utc_for_local(hour, minute=0, timezone_str='America/Chicago'):\n    \"\"\"Calculate the next occurrence of the specified hour in the given timezone and convert to UTC.\"\"\"\n    tz = pytz.timezone(timezone_str)\n    now = datetime.now(tz)\n    target_time = now.replace(hour=hour, minute=minute, second=0, microsecond=0)\n    if now &gt;= target_time:\n        target_time += timedelta(days=1)\n    return target_time.astimezone(pytz.utc).strftime('%H:%M')\n</code></pre>"},{"location":"azure-container-schedule/#running-jobs-in-the-background","title":"Running Jobs in the Background","text":"<pre><code>def run_in_background(target_function):\n    \"\"\"Runs the given function asynchronously in a background thread.\"\"\"\n    thread = threading.Thread(target=target_function, daemon=True)\n    thread.start()\n</code></pre>"},{"location":"azure-container-schedule/#executing-python-scripts","title":"Executing Python Scripts","text":"<pre><code>def execute_script(script_path, *args):\n    \"\"\"Helper function to execute a Python script with optional arguments.\"\"\"\n    subprocess.run(['python', script_path, *args])\n</code></pre>"},{"location":"azure-container-schedule/#defining-job-functions","title":"Defining Job Functions","text":"<p>Each job function can call a different Python script or perform a specific task. For example:</p> <pre><code>def job_example():\n    logger.info(\"Running example job...\")\n    execute_script('path/to/your_script.py', 'optional_arg')\n</code></pre>"},{"location":"azure-container-schedule/#scheduling-jobs","title":"Scheduling Jobs","text":"<p>You can schedule jobs at specific times or intervals. Here are some examples:</p> <pre><code>if __name__ == \"__main__\":\n    try:\n        # Schedule daily jobs at specific local times (converted to UTC)\n        schedule.every().day.at(get_next_utc_for_local(7)).do(lambda: run_in_background(job_example)).tag('job_example', 'daily_task')\n        schedule.every().day.at(get_next_utc_for_local(12, 30)).do(lambda: run_in_background(job_example)).tag('job_example', 'daily_task')\n\n        # Schedule jobs at regular intervals\n        schedule.every(15).minutes.do(lambda: run_in_background(job_example)).tag('job_example', 'frequent_task')\n\n        # Main scheduler loop\n        while True:\n            try:\n                schedule.run_pending()\n            except Exception as e:\n                logger.error(f\"Scheduler Error: {e}\")\n            time.sleep(1)\n    except Exception as e:\n        logger.error(f\"Fatal Scheduler Error: {e}\")\n</code></pre>"},{"location":"azure-container-schedule/#explanation","title":"Explanation","text":"<ul> <li>get_next_utc_for_local: Ensures jobs run at the correct local time, regardless of the container's timezone.</li> <li>run_in_background: Prevents long-running jobs from blocking the scheduler loop.</li> <li>execute_script: Launches other Python scripts as subprocesses, optionally with command-line arguments.</li> <li>schedule.every().day.at(...): Schedules jobs at specific times.</li> <li>schedule.every(N).minutes.do(...): Schedules jobs at regular intervals.</li> <li>Infinite Loop: The script runs forever, checking for jobs to run every second.</li> </ul>"},{"location":"azure-container-schedule/#container-startup-with-supervisor","title":"Container Startup with Supervisor","text":"<p>To ensure your scheduler script starts automatically when the container is created and restarts if it fails, use Supervisor. Add a <code>supervisord.conf</code> file to your container image with the following content:</p> <pre><code>[program:scheduler]\ncommand=python /app/src/gdepscheduler.py\nautostart=true\nautorestart=true\n</code></pre> <ul> <li>[program:scheduler]: Defines a program called \"scheduler\".</li> <li>command: The command to run your scheduler script.</li> <li>autostart=true: Start the program automatically when Supervisor starts.</li> <li>autorestart=true: Restart the program if it exits unexpectedly.</li> </ul> <p>Supervisor will keep your scheduler running, even if it crashes or the container restarts.</p>"},{"location":"azure-container-schedule/#conclusion","title":"Conclusion","text":"<p>By combining the <code>schedule</code> library, background threading, timezone handling, and Supervisor, you can build a reliable, container-friendly job scheduler in Python. This approach ensures your automation and integration jobs run on time, every time, with minimal manual intervention.</p> <p>Adapt the code and configuration to your own scripts and business requirements. This pattern is suitable for any containerized environment where scheduled automation is needed.</p>"},{"location":"azure-query-recovery-services-vault/","title":"Querying Azure Recovery Services Vault (RSV)","text":""},{"location":"azure-query-recovery-services-vault/#introduction","title":"Introduction","text":"<p>Azure Recovery Services Vault (RSV) is a core component for managing and monitoring backups of virtual machines and other resources in Azure. Automating the retrieval and analysis of backup data can help with compliance, reporting, and operational efficiency. This article demonstrates how to:</p> <ul> <li>Connect to Azure using the Python SDK and a service principal</li> <li>Query a Recovery Services Vault for backup details</li> <li>Process and store backup information for further analysis</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"azure-query-recovery-services-vault/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with Recovery Services Vault(s) and VM backups</li> <li>Service principal with appropriate permissions (Backup Reader, etc.)</li> <li>Python 3.8+ and the following packages (install with <code>pip install ...</code>):</li> <li><code>azure-identity</code> (for authentication)</li> <li><code>azure-mgmt-recoveryservicesbackup</code> (for querying backup data)</li> <li><code>azure-mgmt-resource</code> (for resource management, optional)</li> <li><code>requests</code> (for any direct REST API calls, optional)</li> <li><code>python-dateutil</code> (for date parsing, optional)</li> <li><code>csv</code> (standard library, for CSV export)</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"azure-query-recovery-services-vault/#step-1-authenticate-to-azure-with-a-service-principal","title":"Step 1: Authenticate to Azure with a Service Principal","text":"<p>Use the Azure Identity library to authenticate securely:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_client_secret_credential(tenant_id, client_id, client_secret):\n    \"\"\"Obtain a ClientSecretCredential for Azure authentication.\"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n\n**Explanation:**\n- Use a service principal for secure, automated access.\n- Store secrets securely (e.g., Azure Key Vault).\n\n---\n\n## Step 2: Connect to the Recovery Services Backup Client\n\n\n```python\nfrom azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\n\ndef get_backup_client(credential, subscription_id):\n    \"\"\"Create a RecoveryServicesBackupClient for backup operations.\"\"\"\n    return RecoveryServicesBackupClient(credential, subscription_id)\n# Example usage:\n# backup_client = get_backup_client(credential, '&lt;your-subscription-id&gt;')\n</code></pre> <p>Explanation: - The <code>RecoveryServicesBackupClient</code> allows you to query backup items, jobs, and policies.</p>"},{"location":"azure-query-recovery-services-vault/#step-3-query-backup-items-in-a-recovery-services-vault","title":"Step 3: Query Backup Items in a Recovery Services Vault","text":"<pre><code>def list_vm_backups(backup_client, resource_group, vault_name):\n    \"\"\"List all backup items (e.g., Azure VMs) in the specified vault.\"\"\"\n    items = backup_client.backup_protected_items.list(\n        vault_name=vault_name,\n        resource_group_name=resource_group,\n        filter=\"backupManagementType eq 'AzureIaasVM'\"\n    )\n    backup_info = []\n    for item in items:\n        backup_info.append({\n            'vm_name': item.properties.friendly_name,\n            'protection_status': item.properties.protection_status,\n            'last_backup_time': item.properties.last_backup_time,\n            'health_status': item.properties.health_status,\n            'resource_id': item.id\n        })\n    return backup_info\n</code></pre> <p>Explanation: - Lists all VM backup items in the specified Recovery Services Vault. - Extracts key properties for reporting or further processing.</p>"},{"location":"azure-query-recovery-services-vault/#step-4-store-or-report-on-backup-data","title":"Step 4: Store or Report on Backup Data","text":"<p>You can save the backup information to a CSV file or database for further analysis:</p> <pre><code>def save_backup_info_to_csv(backup_info, filename):\n    \"\"\"Save backup information to a CSV file.\"\"\"\n    with open(filename, 'w', newline='') as csvfile:\n        fieldnames = ['vm_name', 'protection_status', 'last_backup_time', 'health_status', 'resource_id']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for row in backup_info:\n            writer.writerow(row)\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    # Retrieve credentials securely\n    tenant_id = '&lt;your-tenant-id&gt;'\n    client_id = '&lt;your-client-id&gt;'\n    client_secret = '&lt;your-client-secret&gt;'\n    subscription_id = '&lt;your-subscription-id&gt;'\n    resource_group = '&lt;your-resource-group&gt;'\n    vault_name = '&lt;your-vault-name&gt;'\n\n    credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n    backup_client = get_backup_client(credential, subscription_id)\n    backup_info = list_vm_backups(backup_client, resource_group, vault_name)\n    save_backup_info_to_csv(backup_info, 'azure_vm_backups.csv')\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"azure-query-recovery-services-vault/#advanced-updating-backup-resource-details-programmatically","title":"Advanced: Updating Backup Resource Details Programmatically","text":"<p>In some enterprise scenarios, you may want to enrich or update your backup resource inventory with additional details from Azure Recovery Services Vault (RSV). The following function demonstrates how to programmatically update a list of backup resources with the latest backup status and metadata for each VM.</p> <pre><code>from azure.mgmt.recoveryservicesbackup import RecoveryServicesBackupClient\nfrom azure.mgmt.recoveryservicesbackup.activestamp.models import AzureIaaSComputeVMProtectedItem\nimport datetime\n\ndef update_backup_resources(resource_group, vault_name, backup_resources):\n    \"\"\"\n    Update backup resource details by fetching relevant VM backup information from Recovery Services Vault (RSV).\n\n    :param resource_group: Name of the Azure resource group\n    :param vault_name: Name of the Recovery Services Vault\n    :param backup_resources: List of backup resource dictionaries to update\n    \"\"\"\n    try:\n        # Obtain Azure credentials (replace with your secure credential retrieval)\n        credential = get_client_secret_credential(tenant_id, client_id, client_secret)\n        backup_client = RecoveryServicesBackupClient(credential, subscription_id)\n\n        # Dictionary to store VM backups from the vault\n        azure_vm_backups = {}\n\n        # Fetch backup-protected items from the Recovery Services Vault\n        rsv_backup_items = backup_client.backup_protected_items.list(vault_name, resource_group)\n        azure_vm_backups.update({\n            item.properties.virtual_machine_id.lower(): item\n            for item in rsv_backup_items\n            if isinstance(item.properties, AzureIaaSComputeVMProtectedItem)\n        })\n\n        # Get today's date in YYYY-MM-DD format\n        today_date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n\n        def format_date(value):\n            return value.strftime(\"%Y-%m-%d\") if isinstance(value, datetime.datetime) else today_date\n\n        # Iterate through the list of backup resources and update details\n        for resource in backup_resources:\n            normalized_vm_id = resource['resource_id'].lower()\n            vm_backup = azure_vm_backups.get(normalized_vm_id)\n            resource.update({\n                \"friendly_name\": getattr(vm_backup.properties, 'friendly_name', '') if vm_backup else '',\n                \"policy_name\": getattr(vm_backup.properties, 'policy_name', '') if vm_backup else '',\n                \"last_backup_status\": getattr(vm_backup.properties, 'last_backup_status', '') if vm_backup else '',\n                \"last_backup_time\": format_date(getattr(vm_backup.properties, 'last_backup_time', None)) if vm_backup else today_date,\n                \"last_recovery_point\": format_date(getattr(vm_backup.properties, 'last_recovery_point', None)) if vm_backup else today_date,\n                \"protection_state\": getattr(vm_backup.properties, 'protection_state', '') if vm_backup else '',\n                \"protection_status\": getattr(vm_backup.properties, 'protection_status', '') if vm_backup else '',\n                \"container_name\": getattr(vm_backup.properties, 'container_name', '') if vm_backup else '',\n            })\n    except Exception as e:\n        print(f\"Error updating backup resources: {e}\")\n</code></pre> <p>Explanation: - This function takes a list of backup resources (e.g., VMs) and updates each with the latest backup metadata from Azure RSV. - It normalizes VM IDs for matching, fetches backup items from the vault, and updates each resource dictionary in-place. - Error handling is included for robustness; in production, use secure credential management and structured logging.</p>"},{"location":"azure-query-recovery-services-vault/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the retrieval and reporting of VM backup data from Azure Recovery Services Vaults. This enables better compliance, reporting, and operational insight into your backup posture.</p>"},{"location":"azure-resources/","title":"Download Azure Resource","text":""},{"location":"azure-resources/#programmatically-downloading-azure-resource-inventory-and-tag-management","title":"Programmatically Downloading Azure Resource Inventory and Tag Management","text":""},{"location":"azure-resources/#introduction","title":"Introduction","text":"<p>Maintaining an up-to-date inventory of Azure resources and their associated tags is critical for governance, cost management, and compliance. This article provides a detailed, production-grade approach to programmatically fetching Azure resource metadata and synchronizing it with a SQL Server database using Python. </p>"},{"location":"azure-resources/#1-authentication-secure-access-to-azure-apis","title":"1. Authentication: Secure Access to Azure APIs","text":"<p>Before accessing Azure resources, authenticate using a secure method. The function below demonstrates using the Azure Identity SDK's <code>ClientSecretCredential</code> for authentication. This is a common approach for automation scenarios, but for higher security, certificate-based authentication is recommended (see the article Certificate Based Authorization for Azure AD.)</p>"},{"location":"azure-resources/#deep-dive-get_azure_credential-function","title":"Deep Dive: <code>get_azure_credential</code> Function","text":"<p>The <code>get_azure_credential</code> function leverages the <code>azure-identity</code> Python SDK, which provides a unified way to authenticate to Azure services. Here, we use the <code>ClientSecretCredential</code> class, which is suitable for service principals (app registrations) with a client secret.</p> <p>Python Example:</p> <pre><code>from azure.identity import ClientSecretCredential\n\ndef get_azure_credential(tenant_id, client_id, client_secret):\n    \"\"\"\n    Returns a credential object for authenticating with Azure SDKs.\n    Uses the azure-identity library's ClientSecretCredential.\n    \"\"\"\n    return ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n</code></pre> <ul> <li>azure-identity SDK: This is the official Microsoft library for Azure authentication in Python. It supports multiple credential types, including secrets, certificates, managed identity, and interactive login.</li> <li>ClientSecretCredential: This class is used for service-to-service authentication using a client ID and secret. It is widely supported by Azure SDKs, including resource management, storage, and more.</li> <li>When to use: Use this for automation where a client secret is securely stored (e.g., in Azure Key Vault or environment variables). For higher security, use <code>CertificateCredential</code> instead.</li> </ul>"},{"location":"azure-resources/#2-fetching-azure-resource-inventory","title":"2. Fetching Azure Resource Inventory","text":"<p>Use the Azure SDK to enumerate all resources in a subscription. Extract key metadata such as resource ID, name, location, type, and tags.</p> <p>Python Example:</p> <pre><code>def fetch_azure_resources(credential, subscription_id):\n    client = ResourceManagementClient(credential, subscription_id)\n    resource_list = []\n    for item in client.resources.list():\n        type_parts = str(item.type).split('/')\n        type1, type2, type3, type4, type5 = (type_parts + [''] * 5)[:5]\n        resource_group_list = str(item.id).split('/')\n        resource_data = {\n            \"id\": str(item.id).replace(f'/subscriptions/{subscription_id}/', ''),\n            \"location\": item.location,\n            \"name\": item.name,\n            \"tags\": item.tags,\n            \"resourceGroup\": resource_group_list[4] if len(resource_group_list) &gt;= 4 else '',\n            \"type1\": type1,\n            \"type2\": type2,\n            \"type3\": type3,\n            \"type4\": type4,\n            \"type5\": type5,\n        }\n        resource_list.append(resource_data)\n    return resource_list\n</code></pre> <ul> <li>Resource Types: The code splits the resource type string to extract up to five type levels for flexible reporting.</li> <li>Tags: Tags are included for governance and cost allocation.</li> </ul>"},{"location":"azure-resources/#3-synchronizing-with-sql-server-fast-bulk-operations","title":"3. Synchronizing with SQL Server: Fast Bulk Operations","text":"<p>Efficiently update the SQL Server inventory table by marking all resources as inactive, then bulk updating existing resources and inserting new ones. This ensures the database reflects the current Azure state.</p> <p>Note: The <code>Dim_Resources</code> table is not a full load (truncate-and-reload) table. Instead, it is designed to retain records of resources that may have been deleted from Azure. By marking resources as inactive rather than removing them, you can track the lifecycle of resources, including those that have been deleted, for audit, compliance, and historical analysis purposes.</p> <p>Python Example:</p> <pre><code>import pyodbc\n\ndef sync_resources_to_sql(resource_list, connection_string):\n    conn = pyodbc.connect(connection_string)\n    cursor = conn.cursor()\n    existing_resource_ids = {str(row[0]).lower() for row in cursor.execute(\"SELECT ResourceID FROM Dim_Resources\").fetchall()}\n    updateresources = [\n        [r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True, r['id']]\n        for r in resource_list if str(r['id']).lower() in existing_resource_ids\n    ]\n    newresources = [\n        [r['id'], r['location'], r['name'], r['resourceGroup'], r['type1'], r['type2'], r['type3'], r['type4'], r['type5'], True]\n        for r in resource_list if str(r['id']).lower() not in existing_resource_ids\n    ]\n    cursor.execute('UPDATE Dim_Resources SET Active = 0')\n    if updateresources:\n        query = '''UPDATE Dim_Resources SET Location=?, Name=?, ResourceGroup=?, Type1=?, Type2=?, Type3=?, Type4=?, Type5=?, Active=? WHERE ResourceId=?'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, updateresources)\n    if newresources:\n        query = '''INSERT INTO Dim_Resources (ResourceID, Location, Name, ResourceGroup, Type1, Type2, Type3, Type4, Type5, Active) VALUES (?,?,?,?,?,?,?,?,?,?)'''\n        cursor.fast_executemany = True\n        cursor.executemany(query, newresources)\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre> <ul> <li>Bulk Operations: Use <code>fast_executemany</code> for high-performance updates and inserts.</li> <li>Active Flag: Mark all resources as inactive before updating, then set active for current resources.</li> </ul>"},{"location":"azure-resources/#4-end-to-end-orchestration","title":"4. End-to-End Orchestration","text":"<p>A typical workflow for resource inventory management:</p> <pre><code>def fetch_and_store_resources():\n    credential = get_azure_credential(tenant_id=..., client_id=..., client_secret=...)\n    resource_list = fetch_azure_resources(credential, subscription_id=...)\n    sync_resources_to_sql(resource_list, connection_string=...)\n</code></pre>"},{"location":"azure-resources/#5-best-practices-and-considerations","title":"5. Best Practices and Considerations","text":"<ul> <li>Security: Use certificate-based authentication for automation when possible. Store credentials securely.</li> <li>Performance: Use bulk operations for large datasets.</li> <li>Data Quality: Regularly update the inventory to reflect the current Azure state.</li> <li>Scheduling: Automate the process with a scheduler (e.g., cron, Azure Automation).</li> <li>Auditing: Keep logs of changes and exceptions for compliance.</li> </ul>"},{"location":"azure-resources/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can automate the discovery and inventory of Azure resources, ensuring your SQL Server database remains a reliable source of truth for governance and reporting.</p>"},{"location":"azure-resources/#references","title":"References","text":"<ul> <li>Azure Resource Management Python SDK</li> <li>azure-identity Python SDK</li> <li>pyodbc Documentation</li> <li>Azure Tagging Best Practices</li> </ul>"},{"location":"azure-restore-from-backup-part1/","title":"Azure VM Restore from Backup \u2013 Part 1: Automated Restore with PowerShell","text":"<p>This article is Part 1 of a two-part series on automating Azure VM restore from Recovery Services Vault (RSV) backups, enabling Business Continuity/Disaster Recovery (BCP/DR) across regions. Here, we focus on the main restore process using the <code>restorevms.ps1</code> script. In Part 2, we cover NIC re-creation and IP assignment for a true like-for-like DR.</p>"},{"location":"azure-restore-from-backup-part1/#overview","title":"Overview","text":"<ul> <li>Goal: Restore VMs from backup in one Azure region (e.g., East US) to another (e.g., West US) for BCP/DR.</li> <li>Approach: Use PowerShell and Azure CLI to automate finding the latest restore point and restoring the VM, including all required network and resource group settings.</li> <li>Why: This approach ensures your DR VM is as close as possible to the original, with minimal manual effort.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#the-script-restorevmsps1","title":"The Script: <code>restorevms.ps1</code>","text":"<p>Below is the full script used to automate the restore process:</p> <pre><code># Boolean variable to indicate whether to restore disks only\n# If it is True then VM will also be created so be careful \nparam (\n    [string] $restorediskonly = \"false\",\n    [int] $numberofhours2wait = 2,\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json'\n)\n\n########################################################\n# Install Azure CLI manually if not installed already\n# curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n########################################################\nfunction Get-Latest-Recovery-Point {\n    param (\n        [string]$vmname,    \n        [string]$vmcontainername,\n        [string]$vaultname,\n        [string]$vaultresourcegroupname\n    )\n\n    $latest_recovery_point = az backup recoverypoint list `\n        --container-name $vmcontainername `\n        --backup-management-type AzureIaasVM `\n        --item-name $vmname `\n        --resource-group $vaultresourcegroupname `\n        --vault-name $vaultname `\n        --query \"[0]\" `\n        --use-secondary-region `\n        --output json `\n    | ConvertFrom-Json\n    | Select-Object -ExpandProperty name\n\n    return $latest_recovery_point\n}\nfunction Restore-Disks {\n    param (\n        [string]$vmname,    \n        [string]$vmcontainername,\n        [string]$vaultname,\n        [string]$vaulresourcegrouptname,\n        [string]$storageaccountname,\n        [string]$recoverypointname,\n        [string]$targetresourcegroupname,\n        [string]$targetvmname = '',\n        [string]$targetvnetname = '',\n        [string]$targetvnetresourcegroup = '',\n        [string]$targetsubnetname = '',\n        [bool]$restoretodisk = $false\n    )\n    if ($restoretodisk) {\n        $restore_operation = az backup restore restore-disks `\n            --resource-group $vaulresourcegrouptname `\n            --vault-name $vaultname `\n            --restore-mode AlternateLocation `\n            --container-name $vmcontainername `\n            --item-name $vmname `\n            --storage-account $storageaccountname `\n            --rp-name $recoverypointname `\n            --target-resource-group $targetresourcegroupname `\n            --restore-to-staging-storage-account $true `\n            --use-secondary-region\n    }\n    else {\n        $restore_operation = az backup restore restore-disks `\n            --resource-group $vaulresourcegrouptname `\n            --vault-name $vaultname `\n            --container-name $vmcontainername `\n            --item-name $vmname `\n            --storage-account $storageaccountname `\n            --rp-name $recoverypointname `\n            --target-resource-group $targetresourcegroupname `\n            --target-vm-name $targetvmname `\n            --target-vnet-name $targetvnetname `\n            --target-vnet-resource-group $targetvnetresourcegroup `\n            --target-subnet-name $targetsubnetname `\n            --use-secondary-region \n    }    \n\n    return $restore_operation\n}\nfunction Invoke-AzureCLICommand {\n    param (\n        [string]$command,\n        [string]$message = \"\"\n    )\n\n    if (-not [string]::IsNullOrEmpty($message)) {\n        Write-CustomLog (\"About to run message {0}\" -f $message)\n        Write-CustomLog (\"About to run command {0}\" -f $command)\n    }\n    $executionResult = Invoke-Expression $command\n    return $executionResult\n}\n# logging function which should work on both local dev container and Git Hub action\nfunction Write-CustomLog {\n    param (\n        [string]$message\n    )\n    Write-Host \"::notice::$message\"\n}\n\n$booleanRestorediskonly = $false\n\nswitch ($restorediskonly.ToLower()) {\n    \"true\" {\n        $restorediskonly = [bool]$true\n        $booleanRestorediskonly = $true\n    }\n    \"false\" {\n        $restorediskonly = [bool]$false\n        $booleanRestorediskonly = $false\n    }\n    default {\n        throw \"Invalid value for restorediskonly: $restorediskonly. Must be 'true' or 'false'.\"\n    }\n}\n\n# Log values with type information\nWrite-CustomLog (\"BooleanRestorediskonly variable value: $booleanRestorediskonly (Type: $($booleanRestorediskonly.GetType().Name))\")\nWrite-CustomLog (\"Numberofhours2wait variable value: $numberofhours2wait (Type: $($numberofhours2wait.GetType().Name))\")\n\n# Use the boolean variable in further logic\nif ($booleanRestorediskonly) {\n    Write-CustomLog \"Restoring disk only.\"\n} else {\n    Write-CustomLog \"Performing full restore.\"\n}\n\n############################################################\n# Variables, note the staging SA we created in earler steps\n############################################################\n$gdep_rsv_vault_name = \"rsv-prod-eus-01\"\n$gdep_rsv_vault_resource_group = \"rg-gdep-peus-backup\"\n\n$gdep_staging_storage_account = az storage account show --name storegdeppwusstaging --query 'id' --output tsv\nWrite-CustomLog (\"Staging storage account is {0}\" -f $gdep_staging_storage_account)\n\n&lt;#\n    gdep_virtual_machine                = Specifies the name of the backed-up VM item to restore.\n    gdep_virtual_machine_containername  = Specifies the name of the container within the vault that holds the backed-up VM\n    gdep_target_resource_group_name     = Specifies the resource group where the restored VM will be created.\n    gdep_target_vm_name                 = Specifies the name to assign to the newly restored VM\n    gdep_target_vnet_name               = Specifies the name of the virtual network (VNet) to which the restored VM will be connected\n    gdep_target_vnet_resource_group     = Specifies the resource group name of the VNet where the restored VM will be connected\n    gdep_target_subnet_name             = Specifies the name of the subnet within the target VNet where the restored VM will be placed\n\n    vaultname = Specifies the name of the Recovery Services vault from which the backup will be restored\n    vaulresourcegrouptname = Specifies the resource group name where the vault is located. \n    storageaccountname = Specifies the name of the storage account where the VM disks will be restored. \n    recoverypointname = Specifies the name of the recovery point to restore from.\n\n    gdep_virtual_machine_nic_name = Name of the NIC resource\n    gdep_virtual_machine_nic_ip = Final IP desired for this NIC\n    gdep_virtual_machine_nic_subnet_resource_group = \n    gdep_virtual_machine_nic_vnet_name = \n    gdep_virtual_machine_nic_snet_name = \n#&gt;\n\n$vm_list = Get-Content -Path $vmlist | ConvertFrom-Json\n# Start of script\n\n# Lets add restore Job related properties to keep track of each job \nforeach ($gdepvm in $vm_list) {\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_restore_job_name\" -Value \"\"\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_restore_job_status\" -Value \"Not Started\"\n    $gdepvm | Add-Member -MemberType NoteProperty -Name \"gdep_nic_operation_completed\" -Value $false\n}\n\nforeach ($gdepvm in $vm_list) {\n    try {\n        # Extract parameters from the hashtable\n        $gdep_virtual_machine = $gdepvm.gdep_virtual_machine\n        $gdep_virtual_machine_container_name = $gdepvm.gdep_virtual_machine_containername\n        $gdep_target_resource_group_name = $gdepvm.gdep_target_resource_group_name\n        $gdep_target_vm_name = $gdepvm.gdep_target_vm_name\n        $gdep_target_vnet_name = $gdepvm.gdep_target_vnet_name\n        $gdep_target_vnet_resource_group = $gdepvm.gdep_target_vnet_resource_group\n        $gdep_target_subnet_name = $gdepvm.gdep_target_subnet_name\n\n        Write-CustomLog (\"About to obtain recovery point for VM {0}\" -f $gdep_virtual_machine)\n\n        $latest_recovery_point = Get-Latest-Recovery-Point `\n            -vmname $gdep_virtual_machine `\n            -vmcontainername $gdep_virtual_machine_container_name `\n            -vaultname $gdep_rsv_vault_name `\n            -vaultresourcegroupname $gdep_rsv_vault_resource_group\n\n        if ($booleanRestorediskonly) {\n            Write-CustomLog (\"Sould not have come into this loop {0}\" -f $gdep_virtual_machine)\n            if (-not [string]::IsNullOrEmpty($latest_recovery_point)) {\n                $restore_operation = Restore-Disks `\n                    -vmname $gdep_virtual_machine `\n                    -vmcontainername $gdep_virtual_machine_container_name `\n                    -vaultname $gdep_rsv_vault_name `\n                    -vaulresourcegrouptname $gdep_rsv_vault_resource_group `\n                    -storageaccountname $gdep_staging_storage_account `\n                    -recoverypointname $latest_recovery_point `\n                    -targetresourcegroupname $gdep_target_resource_group_name `\n                    -restoretodisk $true\n                # ($restore_operation | ConvertFrom-Json).properties.activityId\n                $gdepvm.gdep_restore_job_name = ($restore_operation | ConvertFrom-Json).name\n                Write-CustomLog (\"Working on Restoring Disk for job name {0}\" -f $gdepvm.gdep_restore_job_name)\n            }\n            else {\n                Write-CustomLog (\"No disk(s) recovery point found for VM specfified {0}\" -f $gdep_virtual_machine)\n            }\n        }\n        else {\n            if (-not [string]::IsNullOrEmpty($latest_recovery_point)) {\n                Write-CustomLog (\"This is the correct condition for VM {0}\" -f $gdep_virtual_machine)\n                $restore_operation = Restore-Disks `\n                    -vmname $gdep_virtual_machine `\n                    -vmcontainername $gdep_virtual_machine_container_name `\n                    -vaultname $gdep_rsv_vault_name `\n                    -vaulresourcegrouptname $gdep_rsv_vault_resource_group `\n                    -storageaccountname $gdep_staging_storage_account `\n                    -recoverypointname $latest_recovery_point `\n                    -targetresourcegroupname $gdep_target_resource_group_name `\n                    -targetvmname $gdep_target_vm_name `\n                    -targetvnetname $gdep_target_vnet_name `\n                    -targetvnetresourcegroup $gdep_target_vnet_resource_group `\n                    -targetsubnetname $gdep_target_subnet_name `\n                    -restoretodisk $false\n                $gdepvm.gdep_restore_job_name = ($restore_operation | ConvertFrom-Json).name\n                Write-CustomLog (\"Working on Restoring Virtual Machine for job name {0}\" -f $gdepvm.gdep_restore_job_name)\n            }\n            else {\n                Write-CustomLog (\"No virtual machine recovery point found for VM {0}\" -f $gdep_virtual_machine)\n            }\n        }\n    }\n    catch {\n        Write-Error \"An error occurred: $_\"\n    }\n}\n</code></pre>"},{"location":"azure-restore-from-backup-part1/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"azure-restore-from-backup-part1/#1-parameters-and-setup","title":"1. Parameters and Setup","text":"<ul> <li><code>restorediskonly</code>: If <code>true</code>, only disks are restored; if <code>false</code>, a full VM is created.</li> <li><code>numberofhours2wait</code>: How long to wait for restore jobs (not used directly in this script, but can be used for polling/wait logic).</li> <li><code>vmlist</code>: Path to the JSON file listing VMs to restore (see example below).</li> </ul>"},{"location":"azure-restore-from-backup-part1/#2-helper-functions","title":"2. Helper Functions","text":"<ul> <li>Get-Latest-Recovery-Point: Finds the most recent backup recovery point for a VM in the secondary region.</li> <li>Restore-Disks: Runs the Azure CLI command to restore either disks or a full VM, depending on parameters.</li> <li>Invoke-AzureCLICommand: Utility to run arbitrary Azure CLI commands and log them.</li> <li>Write-CustomLog: Standardized logging for both local and CI environments.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#3-main-logic","title":"3. Main Logic","text":"<ul> <li>Loads the VM list from the provided JSON file.</li> <li>For each VM, finds the latest recovery point and triggers a restore (either disk or full VM).</li> <li>Tracks job names and statuses for each VM.</li> </ul>"},{"location":"azure-restore-from-backup-part1/#4-vm-list-json-example","title":"4. VM List JSON Example","text":"<p>The script expects a JSON file like this:</p> <pre><code>[\n  {\n    \"gdep_virtual_machine\": \"MDCAVDPRDAE02\",\n    \"gdep_virtual_machine_containername\": \"MDCAVDPRDAE02\",\n    \"gdep_target_resource_group_name\": \"dr-rg-gdep-pwus-infrastructure\",\n    \"gdep_target_vm_name\": \"DRMDCAVDPRDAE02\",\n    \"gdep_target_vnet_name\": \"vnet-gdep-pwus-management\",\n    \"gdep_target_vnet_resource_group\": \"dr-rg-gdep-pwus-vnets\",\n    \"gdep_target_subnet_name\": \"snet-gdep-pwus-management-restore\"\n    // ...other NIC properties...\n  }\n]\n</code></pre>"},{"location":"azure-restore-from-backup-part1/#summary","title":"Summary","text":"<ul> <li>This script automates the restore of VMs from Azure RSV backups to a new region.</li> <li>It ensures you always restore from the latest available backup.</li> <li>For full DR, combine with the NIC re-creation process in Part 2.</li> </ul> <p>Continue to Part 2: NIC Re-Creation and IP Assignment</p>"},{"location":"azure-restore-from-backup-part2/","title":"Azure VM Restore from Backup \u2013 Part 2: NIC Re-Creation and IP Assignment","text":"<p>This article is Part 2 of the series on automating Azure VM restore from Recovery Services Vault (RSV) backups for BCP/DR. Here, we focus on the process of re-creating and attaching NICs to restored VMs, ensuring each VM receives the correct IP address and network configuration\u2014just as it was in the source region.</p> <p>If you haven't already, see Part 1 for the main restore process.</p>"},{"location":"azure-restore-from-backup-part2/#overview","title":"Overview","text":"<ul> <li>Goal: After restoring VMs from backup, ensure each VM has a NIC with the same IP and subnet as in the original region.</li> <li>Approach: Use PowerShell and Azure CLI to automate NIC creation, assignment, and validation.</li> <li>Why: Azure restores VMs with default NICs; for true DR, you must re-create and attach NICs with the desired configuration.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#the-script-attachnicsps1","title":"The Script: <code>attachnics.ps1</code>","text":"<p>Below is the full script used to automate the NIC re-creation and assignment process:</p> <pre><code>param (\n    [string] $vmlist = './scripts/bcpdr/vm/vmlist.json',\n    [string] $vaultname = 'rsv-prod-eus-01',\n    [string] $vaultresourcegroupname = 'rg-gdep-peus-backup'\n)\n\nfunction Write-CustomLog {\n    param (\n        [string]$message\n    )\n    Write-Host \"::notice::$message\"\n}\n\nfunction Get-Last-Restore-JobID {\n    param (\n        [string]$vaultname,\n        [string]$vaultresourcegroupname,\n        [string]$vmname,\n        [string]$timeRangeStart\n    )\n\n    $vmname = $vmname.ToLower()\n    $jobid = ''\n\n    $backupJobsJson = az backup job list `\n        --resource-group $vaultresourcegroupname `\n        --vault-name $vaultname `\n        --query \"[?properties.endTime &gt;= '$timeRangeStart' `\n                &amp;&amp; properties.operation == 'CrossRegionRestore'  `\n                &amp;&amp; properties.jobType == 'AzureIaaSVMJob' `\n                &amp;&amp; properties.status == 'Completed' `\n                ] | sort_by(@, &amp;properties.endTime) | reverse(@)\" `\n        --output json\n    $backupJobsJson = $backupJobsJson | ConvertFrom-Json\n\n    foreach ($job in $backupJobsJson) {\n        if ($job.properties.entityFriendlyName.ToLower() -like \"*$vmName*\") {\n            $vmNameRestored = $job.properties.entityFriendlyName\n            Write-CustomLog \"Cross Region Restore for VM has completed : $vmNameRestored\"\n            $jobid = $job.id\n            break\n        }\n    }\n    return $jobid\n}\n\nfunction Invoke-AzureCLICommand {\n    param (\n        [string]$command,\n        [string]$message = \"\"\n    )\n\n    if (-not [string]::IsNullOrEmpty($message)) {\n        Write-CustomLog (\"About to run message {0}\" -f $message)\n        Write-CustomLog (\"About to run command {0}\" -f $command)\n    }\n    $executionResult = Invoke-Expression $command\n    return $executionResult\n}\n\n$vm_list = Get-Content -Path $vmlist | ConvertFrom-Json\n$deploymentresourceGroupName = \"dr-rg-gdep-pwus-deployment\"\n$templateFilePath = \"./networking/nic/nic4drvms.bicep\"\n$vaultname = 'rsv-prod-eus-01'\n$vaultresourcegroupname = 'rg-gdep-peus-backup'\n$data2gobacktoo = (Get-Date).AddDays(-2).ToString(\"yyyy-MM-ddTHH:mm:ssZ\")\n\nforeach ($gdepvm in $vm_list) {\n    try {\n\n        $nicName = $gdepvm.gdep_virtual_machine_nic_name\n        $deploymentGroupName = \"gdepdr-\" + $nicName\n        $nicResouceGroup = $gdepvm.gdep_target_resource_group_name\n        $nicIpAddress = $gdepvm.gdep_virtual_machine_nic_ip\n        $nicSubnetResourceGroup = $gdepvm.gdep_virtual_machine_nic_subnet_resource_group\n        $nicVnetName = $gdepvm.gdep_virtual_machine_nic_vnet_name\n        $nicSubnetName = $gdepvm.gdep_virtual_machine_nic_snet_name\n        $vmName = $gdepvm.gdep_target_vm_name\n        $vmresourceGroupName = $gdepvm.gdep_target_resource_group_name\n\n        Write-CustomLog (\"Retrieving last completed cross region restore status for VM {0} since {1}\" -f $gdepvm.gdep_virtual_machine,$data2gobacktoo)\n        $gdep_restore_job_id = Get-Last-Restore-JobID `\n            -vaultname $vaultname `\n            -vaultresourcegroupname $vaultresourcegroupname `\n            -vmname $gdepvm.gdep_virtual_machine `\n            -timeRangeStart $data2gobacktoo\n\n        if (-not [string]::IsNullOrEmpty($gdep_restore_job_id)) {\n            #This means that VM has been restored \n            Write-CustomLog (\"Restore job {0} has finished for VM {1}\" -f $gdep_restore_job_id, $gdepvm.gdep_virtual_machine)\n            #Check if we have already created and attached the new NIC for this or not\n            $nicExists = az network nic show --resource-group $nicResouceGroup --name $nicName --query id --output tsv\n            if ($nicExists) {\n                Write-CustomLog (\"NIC {0} has already been created and attached to VM {1} nothing to do\" -f $gdepvm.gdep_virtual_machine_nic_name, $gdepvm.gdep_target_vm_name)\n            }\n            else {\n                Write-CustomLog (\"About to create new NIC namely {0}\" -f $gdepvm.gdep_virtual_machine_nic_name)\n                $command = \"az deployment group create --name $deploymentGroupName --resource-group $deploymentresourceGroupName --template-file $templateFilePath --parameters nic_name=$nicName nic_resource_group=$nicResouceGroup nic_ipaddress=$nicIpAddress nic_subnet_resourcegroup=$nicSubnetResourceGroup nic_vnet_name=$nicVnetName nic_subnet_name=$nicSubnetName\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"About to Create new NIC for VM : $vmName\"\n                Write-CustomLog \"Provisioning State: $((($executionResult | ConvertFrom-Json).properties.provisioningState))\"\n                #Setup New NIC Variables we are going to need later\n                $nicid2add = (az network nic show -g $nicResouceGroup -n $nicName | ConvertFrom-Json).id\n                #Dealocate the VM first as it would be up and running \n                Write-CustomLog (\"About to deallocate VM namely {0}\" -f $vmName)\n                $command = \"az vm deallocate --name $vmName --resource-group $vmresourceGroupName --no-wait\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"About to deallocate restored VM: $vmName\"\n\n                # Verify that VM is deallocated\n                $vmStatus = $null\n                while ($vmStatus -ne \"VM deallocated\") {\n                    $vmStatus = az vm get-instance-view --name $gdepvm.gdep_target_vm_name --resource-group $gdepvm.gdep_target_resource_group_name\n                    $vmObject = $vmStatus | ConvertFrom-Json\n                    $nicid2remove = $vmObject.networkProfile.networkInterfaces[0].id\n                    $nicname2remove = az network nic show --ids $nicid2remove --query \"name\" -o tsv\n\n                    $vmInstanceView = ($vmStatus | ConvertFrom-Json).instanceView  \n                    $powerState = ($vmInstanceView.statuses | Where-Object { $_.code -eq \"PowerState/deallocated\" }).displayStatus\n                    if ($powerState -eq \"VM deallocated\") {\n                        Write-CustomLog (\"VM namely {0} is deallocated \" -f $vmName)\n                        $vmStatus = \"VM deallocated\"\n                    }\n                    else {\n                        Write-CustomLog (\"VM namely {0} is still not deallocated sleeping for 15 seconds\" -f $vmName)\n                        Start-Sleep -Seconds 15\n                    }      \n                }\n\n                # First we need to associate the new NIC to this VM and Make it primary\n                $command = \"az vm nic add -g $vmresourceGroupName --vm-name $vmName --nics $nicid2add --primary-nic $nicid2add\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Adding NIC $nicid2add to $vmName\"\n                # Now lets remove and delete NIC\n                $command = \"az vm nic remove -g $vmresourceGroupName --vm-name $vmName --nics $nicid2remove\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Removing NIC $nicid2remove from $vmName\"\n                $command = \"az network nic delete -g $nicResouceGroup -n $nicname2remove\"\n                $executionResult = Invoke-AzureCLICommand -command $command -message \"Deleting NIC $nicname2remove\"\n\n                # Check if the NIC name is 'nic-dr-azgdepprt01' so we can add to backend pool of printer lb created earlier\n                if ($nicIpAddress -eq \"10.27.17.39\") {\n                    $backendAddressPoolName = \"bep-lbi-gdep-pwus-print\"\n                    $printerilbname = \"lbi-gdep-pwus-print-001\"\n                    Write-CustomLog (\"Adding NIC $nicid2add to backend address pool $backendAddressPoolName\")\n                    $command = \"az network nic ip-config address-pool add --address-pool $backendAddressPoolName --ip-config-name ipconfig --nic-name $nicName --resource-group $nicResouceGroup --lb-name $printerilbname\"\n                    $executionResult = Invoke-AzureCLICommand -command $command -message \"Adding NIC $nicid2add to backend address pool $backendAddressPoolName\"\n                }\n            }\n        }\n        else {\n            Write-CustomLog (\"No completed restore job found for VM {0}\" -f $gdepvm.gdep_virtual_machine)\n        }\n    }\n    catch {\n        Write-Error \"An error occurred: $_\"\n    }\n}\n</code></pre>"},{"location":"azure-restore-from-backup-part2/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"azure-restore-from-backup-part2/#1-parameters-and-setup","title":"1. Parameters and Setup","text":"<ul> <li><code>vmlist</code>: Path to the JSON file listing VMs and their desired NIC/network configuration.</li> <li><code>vaultname</code>, <code>vaultresourcegroupname</code>: Used for querying restore jobs.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#2-helper-functions","title":"2. Helper Functions","text":"<ul> <li>Write-CustomLog: Standardized logging for local and CI environments.</li> <li>Get-Last-Restore-JobID: Finds the most recent completed cross-region restore job for a VM.</li> <li>Invoke-AzureCLICommand: Utility to run and log Azure CLI commands.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#3-main-logic","title":"3. Main Logic","text":"<ul> <li>Loads the VM list from the provided JSON file.</li> <li>For each VM:</li> <li>Checks if a NIC with the desired name already exists and is attached. If so, skips.</li> <li>If not, creates a new NIC with the correct IP, subnet, and VNet.</li> <li>Deallocates the VM, attaches the new NIC as primary, removes and deletes the old NIC.</li> <li>Special handling for printer NICs to add to the correct backend pool.</li> </ul>"},{"location":"azure-restore-from-backup-part2/#4-vm-list-json-example","title":"4. VM List JSON Example","text":"<p>The script expects a JSON file like this:</p> <pre><code>[\n  {\n    \"gdep_virtual_machine\": \"MDCAVDPRDAE02\",\n    \"gdep_virtual_machine_nic_name\": \"nic-dr-mdcavdprdae02\",\n    \"gdep_virtual_machine_nic_ip\": \"10.27.11.5\",\n    \"gdep_virtual_machine_nic_subnet_resource_group\": \"dr-rg-gdep-pwus-vnets\",\n    \"gdep_virtual_machine_nic_vnet_name\": \"vnet-gdep-pwus-management\",\n    \"gdep_virtual_machine_nic_snet_name\": \"snet-gdep-pwus-management\"\n    // ...other properties...\n  }\n]\n</code></pre>"},{"location":"azure-restore-from-backup-part2/#summary","title":"Summary","text":"<ul> <li>This script automates the process of re-creating and attaching NICs to restored VMs in a DR region.</li> <li>Ensures each VM receives the correct IP address and network configuration for a true like-for-like DR.</li> <li>Use in conjunction with Part 1 for a complete BCP/DR restore workflow.</li> </ul> <p>You now have a fully automated, script-driven process for restoring Azure VMs and their network configuration across regions!</p>"},{"location":"devops-build-container/","title":"Building the Container Image: Dockerfile Explained","text":"<p>This article explains how to build the container image for your Azure Container Instance, focusing on the <code>Dockerfile</code> and <code>requirements.txt</code>. This is the foundation for running your application in the cloud. For deploying and running the image using Bicep and GitHub Actions, see the related articles linked below.</p>"},{"location":"devops-build-container/#overview","title":"Overview","text":"<p>The container image is built using a <code>Dockerfile</code> and a <code>requirements.txt</code> file. The Dockerfile defines the environment, dependencies, and setup steps, while <code>requirements.txt</code> lists the Python packages needed by your application.</p>"},{"location":"devops-build-container/#dockerfile-walkthrough","title":"Dockerfile Walkthrough","text":"<p>The main Dockerfile is located at <code>compute/ci/interface/container/Dockerfile</code>.</p>"},{"location":"devops-build-container/#key-sections","title":"Key Sections","text":"<pre><code># Use a specific version of the base image\nFROM mcr.microsoft.com/devcontainers/python:3\n\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Install necessary packages\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y \\\n    supervisor \\\n    unzip \\\n    build-essential \\\n    python3-dev \\\n    git \\\n    curl \\\n    gnupg \\\n    snmp \n\n# Add Microsoft GPG key and SQL Server repository\nRUN curl -fsSL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg\nRUN curl https://packages.microsoft.com/config/debian/12/prod.list | tee /etc/apt/sources.list.d/mssql-release.list\n\n# Install Microsoft SQL Server related packages\nRUN apt-get update &amp;&amp; \\\n    ACCEPT_EULA=Y apt-get install -y \\\n    msodbcsql18 \\\n    mssql-tools18 \n\n# Install PowerShell Core and modules\nRUN curl -fsSL https://packages.microsoft.com/config/debian/12/packages-microsoft-prod.deb -o packages-microsoft-prod.deb &amp;&amp; \\\ndpkg -i packages-microsoft-prod.deb &amp;&amp; \\\nrm packages-microsoft-prod.deb &amp;&amp; \\\napt-get update &amp;&amp; \\\napt-get install -y powershell\n\nRUN pwsh -Command Install-Module PnP.PowerShell -Force -AllowClobber -SkipPublisherCheck\nRUN pwsh -Command Install-Module ExchangeOnlineManagement -Force -AllowClobber -SkipPublisherCheck\nRUN pwsh -Command Install-Module MicrosoftTeams -Force -AllowClobber -SkipPublisherCheck\n\n# Clean up\nRUN apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Add MS SQL Server tools to PATH\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' &gt;&gt; ~/.bashrc\n\n# Supervisor configuration\nRUN mkdir -p /etc/supervisor/conf.d\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# Copy and install Python dependencies\nCOPY requirements.txt .\nRUN python -m pip install -r requirements.txt\n\n# Clone the private repository using a build argument\nARG GITHUB_PAT\nRUN git clone https://$GITHUB_PAT@github.com/GDEnergyproducts/GDEP-INTFC.git /app\n\nWORKDIR /app\n\nRUN chmod +x scripts/container/setuppyrfc.sh\nRUN chmod +x scripts/container/mountstorage.sh\nRUN ./scripts/container/setuppyrfc.sh\n\n# Run tests\nRUN pytest\n\n# Start supervisor\nCMD [\"/usr/bin/supervisord\"]\n</code></pre> <p>Explanation: - Base Image: Uses a Python dev container as the base. - System Packages: Installs tools for Python, SQL Server, PowerShell, and more. - PowerShell Modules: Installs modules for Azure and Microsoft 365 automation. - Python Dependencies: Installs all Python packages listed in <code>requirements.txt</code>. - Source Code: Clones the application code from a private GitHub repository using a build argument for authentication. - Setup Scripts: Makes and runs setup scripts executable. - Testing: Runs <code>pytest</code> to ensure the build is valid. - Supervisor: Uses Supervisor to manage processes in the container.</p>"},{"location":"devops-build-container/#requirementstxt","title":"requirements.txt","text":"<p>This file lists all Python dependencies needed by your application. It is copied into the image and installed with pip. Keeping dependencies in this file makes builds reproducible and easy to update.</p>"},{"location":"devops-build-container/#building-the-image-locally","title":"Building the Image Locally","text":"<p>To build the image locally, run:</p> <pre><code>docker build --build-arg GITHUB_PAT=your_token_here -t myacr.azurecr.io/myimage:latest compute/ci/interface/container\n</code></pre> <p>Replace <code>your_token_here</code> with a valid GitHub personal access token.</p>"},{"location":"devops-build-container/#related-articles","title":"Related Articles","text":"<ul> <li>How to Deploy an Azure Container Instance Using Bicep (IaC)</li> <li>How to Run Your Azure Container Instance Bicep Deployment</li> </ul>"},{"location":"devops-deploy-container-github-action/","title":"How to Run Your Azure Container Instance Bicep Deployment","text":"<p>This article explains how to execute your Bicep-based Azure Container Instance deployment, both via GitHub Actions and from the command line.</p>"},{"location":"devops-deploy-container-github-action/#running-with-github-actions","title":"Running with GitHub Actions","text":"<p>The workflow file <code>.github/workflows/ciinterface.yml</code> automates the build, push, and deployment process.</p>"},{"location":"devops-deploy-container-github-action/#key-steps-in-the-workflow","title":"Key Steps in the Workflow","text":"<pre><code>name: Interface Container Instance\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: 'Environment'\n        required: true\n        default: 'Development'\n        type: 'choice'\n        options:\n          - 'Development'\n          - 'Production'\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Login to Azure\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n      - name: Azure key vault - Get Secret\n        uses: Azure/get-keyvault-secrets@v1\n        with:\n          keyvault: ${{ vars.AZURE_KEY_VAULT_NAME }}\n          secrets: patgdepintfcrepo\n        id: getAZKVpatgdepiacrepo\n\n      - name: Build Docker image\n        run: |\n          docker build --build-arg GITHUB_PAT=\"${{ steps.getAZKVpatgdepiacrepo.outputs.patgdepintfcrepo }}\" \\\n                       -t ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }}/${{ vars.IMAGE_NAME }}:latest \\\n                       compute/ci/interface/container\n\n      - name: Login to Azure Container Registry\n        run: |\n          echo $ACR_PASSWORD | docker login ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }} -u $ACR_USERNAME --password-stdin\n        env:\n          ACR_USERNAME: ${{ vars.ACR_USERNAME }}\n          ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}\n\n      - name: Push Docker image to Azure Container Registry\n        run: docker push ${{ vars.CONTAINER_REGISTRY_LOGIN_SERVER }}/${{ vars.IMAGE_NAME }}:latest\n\n      - name: Deploy the bicep file\n        if: ${{ github.event.inputs.environment == 'Development' }}\n        uses: azure/arm-deploy@v1\n        with:\n          scope: 'resourcegroup'\n          subscriptionId: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n          resourceGroupName: rg-gdep-peus-2delete\n          template: ./compute/ci/interface/interface.bicep\n          parameters: ./compute/ci/interface/interface-dev.parameters.json\n</code></pre> <p>Explanation: - Build and Push: Docker image is built and pushed to Azure Container Registry. - Secrets: Pulled securely from Azure Key Vault. - Deploy: The Bicep template is deployed using the specified parameters file.</p>"},{"location":"devops-deploy-container-github-action/#running-from-the-command-line","title":"Running from the Command Line","text":"<p>You can also deploy the Bicep template directly using Azure CLI:</p> <pre><code>az login\naz account set --subscription \"&lt;your-subscription-id&gt;\"\naz deployment group create \\\n  --resource-group &lt;your-resource-group&gt; \\\n  --template-file ./compute/ci/interface/interface.bicep \\\n  --parameters @./compute/ci/interface/interface-dev.parameters.json\n</code></pre> <p>Tips: - Make sure you have the Azure CLI and Bicep CLI installed. - Use Key Vault references in your parameters file for secrets.</p>"},{"location":"devops-deploy-container-github-action/#related-articles","title":"Related Articles","text":"<ul> <li>How to Build Your Container Image</li> <li>How to Deploy an Azure Container Instance Using Bicep (IaC)</li> </ul>"},{"location":"devops-deploy-container-iac/","title":"Deploying Azure Container Instances with BICEP (IaC)","text":"<p>This article demonstrates how to use Infrastructure as Code (IaC) with BICEP to deploy an Azure Container Instance (ACI). We'll walk through the main BICEP template, explain each section, and highlight best practices such as using Azure Key Vault for secrets management.</p>"},{"location":"devops-deploy-container-iac/#introduction","title":"Introduction","text":"<p>Azure Container Instances provide a fast and simple way to run containers in Azure, without managing virtual machines. Using BICEP, you can define your container infrastructure as code, making deployments repeatable and version-controlled.</p>"},{"location":"devops-deploy-container-iac/#bicep-template-overview","title":"BICEP Template Overview","text":"<p>The main BICEP file is <code>compute/ci/interface/interface.bicep</code>. It defines all the parameters, secrets, and resources needed to deploy an ACI.</p>"},{"location":"devops-deploy-container-iac/#parameters","title":"Parameters","text":"<pre><code>param image_name string \nparam image_tag string \nparam container_registry_name string \nparam aci_name string\nparam subnet_name string\nparam tags object\nparam subnet_id string\nparam volume_name string \n\n@secure()\nparam azure_file_share_name string\n@secure()\nparam azure_file_share_user_name string\n@secure()\nparam application_interface_clientid string\n@secure()\nparam application_interface_clientsecret string\n@secure()\nparam application_interface_tenantid string\n@secure()\nparam container_registry_password string\n@secure()\nparam azure_file_share_password string\n@secure()\nparam ukg_encrypt_passphrase string\nparam location string = resourceGroup().location\nparam containerRegistryServer string = '${container_registry_name}.azurecr.io' \n</code></pre> <p>Explanation: - Parameters allow you to customize the deployment. - <code>@secure()</code> marks secrets so they are not logged or exposed. - Many parameters are injected from Azure Key Vault for security.</p>"},{"location":"devops-deploy-container-iac/#variables","title":"Variables","text":"<pre><code>var container_registry_username  = container_registry_name\n</code></pre> <p>Explanation: - Sets the registry username to the registry name for convenience.</p>"},{"location":"devops-deploy-container-iac/#resource-definition","title":"Resource Definition","text":"<pre><code>resource aci 'Microsoft.ContainerInstance/containerGroups@2023-05-01' = {\n  name: aci_name\n  location: location\n  tags: tags\n\n  properties: {\n    containers: [\n      {\n        name: aci_name\n        properties: {\n          image: '${containerRegistryServer}/${image_name}:${image_tag}'\n          ports: [\n            {\n              port: 80\n              protocol: 'TCP'\n            }\n          ]\n          resources: {\n            limits: {\n              cpu: 4\n              memoryInGB: json('16')\n            }\n            requests: {\n              cpu: 4\n              memoryInGB: json('16')\n            }\n          }\n          environmentVariables:[\n            {\n              name:'application_interface_clientid'\n              secureValue:application_interface_clientid\n            }\n            {\n              name:'application_interface_clientsecret'\n              secureValue:application_interface_clientsecret\n            }\n            {\n              name:'application_interface_tenantid'\n              secureValue:application_interface_tenantid\n            }\n            {\n              name:'azure_file_share_name'\n              secureValue:azure_file_share_name\n            }\n            {\n              name:'azure_file_share_user_name'\n              secureValue:azure_file_share_user_name\n            }\n            {\n              name:'azure_file_share_password'\n              secureValue:azure_file_share_password\n            }\n            {\n              name:'ukg_encrypt_passphrase'\n              secureValue:ukg_encrypt_passphrase\n            }\n          ]\n          volumeMounts: [\n            {\n              name: volume_name\n              mountPath: '/mnt/azure'\n              readOnly: false\n            }\n          ]\n        }\n      }\n    ]\n    osType: 'Linux'\n    imageRegistryCredentials: [\n      {\n        server: containerRegistryServer\n        username: container_registry_username\n        password: container_registry_password\n      }\n    ]\n    volumes: [\n      {\n        name: volume_name \n        azureFile: {\n          shareName: azure_file_share_name\n          storageAccountName: azure_file_share_user_name\n          storageAccountKey: azure_file_share_password\n        }\n      }\n    ]\n    priority: 'Regular'\n    restartPolicy: 'Never'\n    sku: 'Standard'\n    subnetIds: [\n      {\n        id: subnet_id\n        name: subnet_name\n      }\n    ]\n    dnsConfig: {\n      nameServers: [\n        '10.27.11.4'\n        '10.27.11.5'\n        '168.63.129.16'  \n      ]\n    }\n  }\n}\n</code></pre> <p>Explanation: - Container Definition: Specifies the image, ports, resources, environment variables, and volume mounts. - Resources: Both <code>limits</code> and <code>requests</code> are set to 4 CPUs and 16GB RAM. - Environment Variables: Secure values are injected from Key Vault. - Volumes: Azure File Share is mounted for persistent storage. - Image Registry Credentials: Pulled securely from parameters. - Networking: Subnet and DNS configuration are specified.</p>"},{"location":"devops-deploy-container-iac/#parameters-file","title":"Parameters File","text":"<p>The <code>interface-dev.parameters.json</code> file provides all parameter values for deployment. This keeps secrets and environment-specific values out of the main template.</p> <p>Advantages: - Separation of Concerns: Code and configuration are separated. - Reusability: Use different parameter files for different environments.</p>"},{"location":"devops-deploy-container-iac/#why-use-azure-key-vault","title":"Why Use Azure Key Vault?","text":"<ul> <li>Security: Secrets like passwords and client secrets are never stored in source control.</li> <li>Centralized Management: Rotate secrets in one place without updating code.</li> <li>Integration: BICEP and Azure Resource Manager can reference Key Vault secrets directly.</li> </ul>"},{"location":"devops-deploy-container-iac/#related-articles","title":"Related Articles","text":"<ul> <li>How to Build Your Container Image</li> <li>How to Run Your Azure Container Instance Bicep Deployment</li> </ul>"},{"location":"fabric-wharehouse-fact-import/","title":"Importing Data into Fabric Data Warehouse","text":""},{"location":"fabric-wharehouse-fact-import/#introduction","title":"Introduction","text":"<p>In this article, we will walk through a robust Python-based workflow for importing employee data into Azure Fabric Data Warehouse (DW). This workflow leverages a combination of Azure SDKs, ODBC, and custom utility functions to automate the process of uploading, transforming, and loading data from CSV files into a cloud-based data warehouse. We will explore the key functions involved, their implementation, and how they work together to achieve a seamless data integration pipeline.</p>"},{"location":"fabric-wharehouse-fact-import/#python-libraries-used","title":"Python Libraries Used","text":"<ul> <li>pyodbc: Enables Python to connect to ODBC-compliant databases, such as SQL Server, for executing SQL queries and transactions.</li> <li>os, sys: Standard Python libraries for interacting with the operating system and system-specific parameters.</li> <li>csv: Provides tools for reading and writing CSV files.</li> <li>datetime: Used for manipulating dates and times.</li> <li>io.StringIO: Allows treating strings as file-like objects, useful for CSV parsing.</li> <li>typing: Provides type hints for better code clarity and static analysis.</li> <li>azure.storage.filedatalake: Azure SDK for interacting with Azure Data Lake Storage Gen2, including file and directory operations.</li> </ul>"},{"location":"fabric-wharehouse-fact-import/#the-data-import-workflow","title":"The Data Import Workflow","text":"<p>The workflow is orchestrated by the <code>process_fabric_employees_dw</code> function, which coordinates the following steps:</p> <ol> <li>Upload the latest HR system file to the Azure Data Lake 'unprocessed' folder.</li> <li>Retrieve all unprocessed files from the Data Lake.</li> <li>Fetch metadata from the HR SQL Data Warehouse.</li> <li>For each unprocessed file:<ul> <li>Parse the CSV content.</li> <li>Determine the date identifier for the data.</li> <li>Transform the data for database insertion.</li> <li>Insert the transformed data into the database.</li> <li>Move the processed file to the 'processed' folder in Data Lake.</li> </ul> </li> </ol> <p>Let's examine the key functions involved in this workflow.</p>"},{"location":"fabric-wharehouse-fact-import/#1-process_fabric_employees_dw","title":"1. <code>process_fabric_employees_dw</code>","text":"<p>This is the main orchestration function. It manages the end-to-end process of uploading, processing, and loading employee data files.</p> <pre><code>def process_fabric_employees_dw(adp_file):\n    try:\n        upload_file_to_datalake(adp_file)\n        files = get_employee_unprocessed_files()\n        if files:\n            metadata = get_metadata_lists_from_hr_sql_wh()\n            department, costcenter, jobfunction, location, payrate, terminationreason, *_ = metadata\n            try:\n                for file in files:\n                    file_name = file['file_name']\n                    file_datetime = file['file_datetime']\n                    logger.info(f\"Processing file: {file_name} (Datetime: {file_datetime})\")\n                    headers, rows = parse_csv(file['file_content'])\n                    date_auto_id = determine_date_auto_id(file_datetime.date().strftime('%Y-%m-%d'))\n                    transformed_data = transform_data(headers, rows, date_auto_id, department, costcenter, jobfunction, location, payrate, terminationreason)\n                    insertsuccessfull = insert_into_db(date_auto_id, transformed_data, f\"{HR_DBOBJECT_PREFIX}.[FactEmployee]\", batch_size=50)\n                    if insertsuccessfull:\n                        move_file_to_processed_folder(file_name, f'{DATALAKE_UNPROCESSED_FOLDER}', f'{DATALAKE_PROCESSED_FOLDER}')\n                        logger.info(f\"Finished Successfully Processing file: {file_name} (Datetime: {file_datetime})\")\n            except Exception as localexception:\n                handle_global_exception(sys._getframe().f_code.co_name, localexception)\n    except Exception as exception_obj:\n        handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function first uploads the provided HR system file to the Data Lake, then processes all unprocessed files by parsing, transforming, and loading them into the database. Successfully processed files are moved to a 'processed' folder.</p>"},{"location":"fabric-wharehouse-fact-import/#2-upload_file_to_datalake","title":"2. <code>upload_file_to_datalake</code>","text":"<p>Handles uploading a local file to the Azure Data Lake 'unprocessed' folder.</p> <pre><code>def upload_file_to_datalake(file_path: str):\n    try:\n        datalake_folder = DATALAKE_UNPROCESSED_FOLDER\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        file_name = os.path.basename(file_path)\n        destination_path = f\"{datalake_folder}/{file_name}\"\n        with open(ADP_LOCAL_FOLDER + file_path, \"rb\") as file_data:\n            file_client = file_system_client.get_file_client(destination_path)\n            file_client.upload_data(file_data, overwrite=True)\n        logger.info(f\"Uploaded file {file_name} to {destination_path} in Data Lake.\")\n    except Exception as e:\n        logger.error(f\"Failed to upload file {file_path} to Data Lake: {e}\")\n        raise\n</code></pre> <p>This function uses the Azure Data Lake SDK to authenticate and upload the file. The file is placed in the 'unprocessed' folder, ready for further processing.</p>"},{"location":"fabric-wharehouse-fact-import/#3-get_employee_unprocessed_files","title":"3. <code>get_employee_unprocessed_files</code>","text":"<p>Retrieves all files in the 'unprocessed' folder of the Data Lake that match the expected filename format.</p> <pre><code>def get_employee_unprocessed_files() -&gt; List[Dict[str, str]]:\n    try:\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        paths = file_system_client.get_paths(path=f\"{DATALAKE_UNPROCESSED_FOLDER}\")\n        valid_files = []\n        for path in paths:\n            if not path.is_directory:\n                file_name = path.name.split(\"/\")[-1]\n                file_datetime = GDEPUtils.parse_datetime_from_filename(file_name)\n                if file_datetime:\n                    try:\n                        file_client = file_system_client.get_file_client(path.name)\n                        file_content = file_client.download_file().readall().decode('utf-8-sig')\n                        valid_files.append({\n                            \"file_name\": file_name,\n                            \"file_datetime\": file_datetime,\n                            \"file_content\": file_content,\n                        })\n                    except Exception as exception_obj:\n                        logger.warning(f\"Failed to process file {file_name}: {exception_obj}\")\n        valid_files.sort(key=lambda x: x[\"file_datetime\"])\n        GDEPUtils.send_email(recipients=GDEPUtils.EMAIL_TO_SEND_EXCEPTIONS, subject='Number of files', plain_message=f'Found {len(valid_files)} valid files.')\n        logger.info(f\"Found {len(valid_files)} valid files.\")\n        return valid_files\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n        return []\n</code></pre> <p>This function lists all files in the 'unprocessed' directory, filters them by filename pattern, downloads their content, and returns a list of valid files for processing.</p>"},{"location":"fabric-wharehouse-fact-import/#4-get_metadata_lists_from_hr_sql_wh","title":"4. <code>get_metadata_lists_from_hr_sql_wh</code>","text":"<p>Fetches reference metadata from the HR SQL Data Warehouse, such as departments, cost centers, job functions, etc.</p> <pre><code>def get_metadata_lists_from_hr_sql_wh():\n    try:\n        metadata_sources = [\n            (f\"{HR_DBOBJECT_PREFIX}.[DimDepartment]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimCostCenter]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimJobFunction]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimLocation]\", lambda row: (row[1], {row[0]: row[3]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimPayRate]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimTerminationReason]\", lambda row: (row[1], {row[0]: row[2]})),\n            (f\"{HR_DBOBJECT_PREFIX}.[DimDate]\", lambda row: (row[1].strftime('%m-%d-%Y'), {row[0]: row[1]})),\n        ]\n        return_list = []\n        sql_connection = get_sql_server_connection_hr_wh()\n        db_cursor = sql_connection.cursor()\n        for query, processor in metadata_sources:\n            working_dict = {}\n            try:\n                db_cursor.execute(f\"SELECT * FROM {query}\")\n                rows = db_cursor.fetchall()\n                for row in rows:\n                    key, value = processor(row)\n                    working_dict[key] = value\n            except Exception as exception_obj:\n                GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n            return_list.append(working_dict)\n        return return_list\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function queries several dimension tables in the data warehouse and returns their contents as dictionaries for use in data transformation.</p>"},{"location":"fabric-wharehouse-fact-import/#5-parse_csv","title":"5. <code>parse_csv</code>","text":"<p>Parses the content of a CSV file into headers and rows.</p> <pre><code>def parse_csv(file_content: str) -&gt; Tuple[List[str], List[List[str]]]:\n    try:\n        csv_reader = csv.reader(StringIO(file_content))\n        file_data = list(csv_reader)\n        headers = file_data[0]\n        rows = file_data[1:]\n        return headers, rows\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function uses Python's built-in <code>csv</code> module to parse the file content, returning the header row and the data rows separately.</p>"},{"location":"fabric-wharehouse-fact-import/#6-determine_date_auto_id","title":"6. <code>determine_date_auto_id</code>","text":"<p>Determines or creates a unique date identifier for a given transaction date in the data warehouse.</p> <pre><code>def determine_date_auto_id(date: str) -&gt; int:\n    sql_connection = None\n    try:\n        sql_connection = get_sql_server_connection_hr_wh()\n        cursor = sql_connection.cursor()\n        cursor.execute(f\"SELECT DateAutoID FROM {HR_DBOBJECT_PREFIX}.[DimDate] WHERE TransactionDate = ?\", (date,))\n        result = cursor.fetchone()\n        if result:\n            return result[0]\n        cursor.execute(f\"SELECT MAX(DateAutoID) FROM {HR_DBOBJECT_PREFIX}.[DimDate]\")\n        max_date_auto_id = cursor.fetchone()[0]\n        new_date_auto_id = (max_date_auto_id or 0) + 1\n        cursor.execute(f\"INSERT INTO {HR_DBOBJECT_PREFIX}.[DimDate] (DateAutoID, TransactionDate) VALUES (?, ?)\", (new_date_auto_id, date))\n        sql_connection.commit()\n        return new_date_auto_id\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n        raise\n    finally:\n        if sql_connection:\n            sql_connection.close()\n</code></pre> <p>This function checks if a date already exists in the <code>DimDate</code> table. If not, it inserts a new record and returns the new identifier.</p>"},{"location":"fabric-wharehouse-fact-import/#7-transform_data","title":"7. <code>transform_data</code>","text":"<p>Transforms raw CSV data into the format required for database insertion, mapping codes to IDs using metadata.</p> <pre><code>def transform_data(headers: List[str], rows: List[List[str]], date_auto_id: int, department: Dict[str, Dict], costcenter: Dict[int, Dict], jobfunction: Dict[str, Dict], location: Dict[str, Dict], payrate: Dict[str, Dict], terminationreason: Dict[str, Dict]) -&gt; List[List]:\n    try:\n        def get_code_value(dictionary: Dict, code, default=None) -&gt; int:\n            if code is None or (isinstance(code, str) and not code.strip()):\n                return int(list(dictionary.get(default, {0: None}).keys())[0])\n            return int(list(dictionary.get(code, {0: None}).keys())[0])\n        transformed_data = []\n        for row in rows:\n            row_dict = dict(zip(headers, row))\n            cost_center_number = row_dict.get('Home Cost Number Code', 0)\n            try:\n                cost_center_number = int(cost_center_number)\n            except (ValueError, TypeError):\n                cost_center_number = 0\n            is_management_position = row_dict.get('This is a Management position', 'false').strip()\n            try:\n                if is_management_position.lower() == 'yes':\n                    is_management_position = True\n                else:\n                    is_management_position = False\n            except Exception:\n                is_management_position = False\n            transformed_data.append([\n                date_auto_id,\n                get_code_value(department, row_dict.get('Home Department Code', 'UNDF')),\n                get_code_value(costcenter, cost_center_number),\n                get_code_value(jobfunction, row_dict.get('Job Function Code', 'UNDF')),\n                get_code_value(location, row_dict.get('Location Code', 'UNDF')),\n                get_code_value(payrate, row_dict.get('Regular Pay Rate Code', 'UNDF')),\n                get_code_value(terminationreason, row_dict.get('Termination Reason Code', 'UNDF')),\n                str(row_dict.get('File Number', '')).strip(),\n                str(row_dict.get('Position ID', '')).strip(),\n                str(row_dict.get('Legal First Name', '')).strip(),\n                str(row_dict.get('Preferred First Name', '')).strip(),\n                '',\n                str(row_dict.get('Last Name', '')).strip(),\n                convert_to_date(row_dict.get('Hire Date', '')),\n                convert_to_date(row_dict.get('Rehire Date', '')),\n                str(row_dict.get('Job Title Description', '')).strip(),\n                is_management_position,\n                str(row_dict.get('Work Contact: Work Email', '')).strip(),\n                str(row_dict.get('Personal Contact: Personal Email', '')).strip(),\n                str(row_dict.get('Reports To Name', '')).strip(),\n                str(row_dict.get('Reports To Position ID', '')).strip(),\n                str(row_dict.get('Payroll Company Code', '')).strip(),\n                str(row_dict.get('Job Class Description', '')).strip(),\n                str(row_dict.get('Position Status', '')).strip(),\n                str(row_dict.get('Personal Contact: Personal Mobile', '')).strip(),\n                str(row_dict.get('Work Contact: Work Phone', '')).strip(),\n                convert_to_date(row_dict.get('Termination Date', '')),\n                str(row_dict.get('Worker Category Code', '')).strip(),\n                str(row_dict.get('Associate ID', '')).strip(),\n                str(row_dict.get('Assigned Shift Description', '')).strip(),\n                str(row_dict.get('FLSA Description', '')).strip(),\n            ])\n        return transformed_data\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function maps each row of the CSV to the required database schema, converting codes to IDs and handling data type conversions.</p>"},{"location":"fabric-wharehouse-fact-import/#8-insert_into_db","title":"8. <code>insert_into_db</code>","text":"<p>Inserts the transformed data into the target SQL table in batches for efficiency.</p> <pre><code>def insert_into_db(date_auto_id: int, data: List[List], table_name: str, batch_size: int = 50):\n    try:\n        returnvalue = False\n        sql_connection = get_sql_server_connection_hr_wh()\n        cursor = sql_connection.cursor()\n        cursor.execute(f\"DELETE FROM {table_name} WHERE DateAutoID = {date_auto_id}\")\n        sql_connection.commit()\n        sql_query = f\"INSERT INTO {table_name} ([DateAutoID],[DepartmentAutoID],[CostCenterAutoID],[JobFunctionAutoID],[LocationAutoID],[PayRateAutoID],[TerminationReasonAutoID],[FileNumber],[PositionID],[FirstName],[PreferredFirstName],[MiddleInitial],[LastName],[HireDate],[RehireDate],[JobTitleDescription],[IsManagementPosition],[WorkEmail],[PersonalEmail],[ManagerName],[ManagerPositionID],[PayrollCompanyCode],[JobClassDescription],[PositionStatus],[PersonalMobile],[WorkMobile],[TerminationDate],[WorkerCategoryCode],[AssociateID],[AssignedShiftDescription],[FLSADescription]) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n        cursor.fast_executemany = True\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i + batch_size]\n            cursor.executemany(sql_query, batch)\n        sql_connection.commit()\n        logger.info(f\"Inserted {len(data)} rows into {table_name}.\")\n        returnvalue = True\n        return returnvalue\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n    finally:\n        sql_connection.close()\n        return returnvalue\n</code></pre> <p>This function first deletes any existing records for the given date, then inserts the new data in batches for performance.</p>"},{"location":"fabric-wharehouse-fact-import/#9-move_file_to_processed_folder","title":"9. <code>move_file_to_processed_folder</code>","text":"<p>Moves a file from the 'unprocessed' folder to the 'processed' folder in Azure Data Lake by renaming it.</p> <pre><code>def move_file_to_processed_folder(file_name: str, source_folder: str, destination_folder: str):\n    try:\n        credential = get_client_sceret_credential()\n        service_client = DataLakeServiceClient(account_url=FABRIC_URL, credential=credential)\n        file_system_client = service_client.get_file_system_client(file_system=FABRIC_WS_HR_NAME)\n        source_path = f\"{source_folder}/{file_name}\"\n        destination_path = f\"{destination_folder}/{file_name}\"\n        logger.info(f\"Source Path: {source_path}\")\n        logger.info(f\"Destination Path: {destination_path}\")\n        file_client = file_system_client.get_file_client(source_path)\n        file_client.rename_file(f\"{file_system_client.file_system_name}/{destination_path}\")\n        logger.info(f\"Moved file {file_name} from {source_folder} to {destination_folder}.\")\n    except Exception as exception_obj:\n        GDEPUtils.handle_global_exception(sys._getframe().f_code.co_name, exception_obj)\n</code></pre> <p>This function uses the Azure Data Lake SDK to rename (move) the file, ensuring that processed files are archived and not reprocessed.</p>"},{"location":"fabric-wharehouse-fact-import/#supplement-get_client_sceret_credential","title":"Supplement: <code>get_client_sceret_credential</code>","text":"<p>This function provides the Azure credential used for authentication in all Data Lake and Azure SDK operations. It is a wrapper around Azure's <code>ClientSecretCredential</code> and is essential for secure, programmatic access to Azure resources.</p> <pre><code>def get_client_sceret_credential():\n    try:\n        return ClientSecretCredential(\n            tenant_id=AZURE_TENANT_ID,\n            client_id=AZURE_CONFIDENTIAL_APP_ID,\n            client_secret=AZURE_CONFIDENTIAL_SECRET\n        )\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre> <p>This function should be used in place of any direct reference to <code>GDEPAzure.get_client_sceret_credential</code> in the workflow. For example, in the <code>upload_file_to_datalake</code> and other related functions, replace <code>GDEPAzure.get_client_sceret_credential()</code> with <code>get_client_sceret_credential()</code> for clarity and modularity.</p>"},{"location":"fabric-wharehouse-fact-import/#references","title":"References","text":"<ul> <li>Azure Data Lake Storage Gen2 Python SDK</li> </ul>"},{"location":"fabric-wharehouse-fact-import/#conclusion","title":"Conclusion","text":"<p>By combining Python's data processing capabilities with Azure's scalable storage and compute, this workflow provides a reliable and efficient way to automate the import of employee data into Azure Fabric Data Warehouse. The modular design allows for easy extension and maintenance, while leveraging batch operations and cloud-native features ensures performance and scalability. This approach can be adapted for similar ETL scenarios involving other data sources and targets in the Azure ecosystem.</p>"},{"location":"github-clean-deployments/","title":"Automating GitHub Deployment Cleanup with Bash, GitHub CLI, and jq","text":"<p>Managing deployments in GitHub can be tedious, especially when you want to delete all deployments and there is no UI option to do so. This article explains how to automate the cleanup of deployments in a GitHub repository using a Bash script, the GitHub CLI, and the <code>jq</code> tool for JSON parsing.</p> <p>Note: This is a focused, practical solution for a common DevOps quirk\u2014bulk deleting deployments from a GitHub repo when no UI exists for this action.</p>"},{"location":"github-clean-deployments/#prerequisites","title":"Prerequisites","text":"<ol> <li>GitHub CLI: Install the GitHub CLI (<code>gh</code>) for interacting with GitHub from the command line.</li> <li>jq: Install <code>jq</code> for parsing JSON responses.</li> <li>Personal Access Token (PAT): Export your GitHub PAT as an environment variable:    <pre><code>export GITHUB_TOKEN=\"&lt;YOUR_PERSONAL_ACCESS_TOKEN&gt;\"\n</code></pre></li> <li>Make the Script Executable:    <pre><code>chmod +x ./scripts/github/deployments.sh\n</code></pre></li> </ol>"},{"location":"github-clean-deployments/#the-script-deploymentssh","title":"The Script: <code>deployments.sh</code>","text":"<p>Below is the full code for the script that automates the deletion of deployments in a GitHub repository:</p> <pre><code>#!/bin/sh\n\n# Set your repository and token\n# Save the token in the environment variable GITHUB_TOKEN\n# Example: export GITHUB_TOKEN=\"&lt;PAT GOES HERE&gt;\"\n\n# Ensure the script is executable\n# Example: chmod +x ./scripts/deployments.sh\n\n# Ensure GitHub CLI and jq libraries are available\n# They should be included in your Dockerfile or build environment\n\n# Repository to manage deployments\nREPO=\"GDEnergyproducts/GDEP-IAC\"\n\n# Retrieve the GitHub token from environment variable\nTOKEN=\"${GITHUB_TOKEN}\"\n\n# Fetch the list of deployments and extract deployment IDs\necho \"Fetching deployment IDs...\"\nDEPLOYMENTS=$(curl -s -H \"Authorization: token $TOKEN\" \\\n                    -H \"Accept: application/vnd.github.v3+json\" \\\n                    https://api.github.com/repos/$REPO/deployments \\\n                    | jq -r '.[] | .id')\n\n# Check if there are any deployments\nif [ -z \"$DEPLOYMENTS\" ]; then\n  echo \"No deployments found.\"\n  exit 0\nfi\n\n# Print the list of deployment IDs\necho \"Deployments found:\"\necho \"$DEPLOYMENTS\"\n\n# Determine the active deployment ID\nACTIVE_DEPLOYMENT_ID=$(curl -s -H \"Authorization: token $TOKEN\" \\\n                             -H \"Accept: application/vnd.github.v3+json\" \\\n                             https://api.github.com/repos/$REPO/deployments \\\n                             | jq -r '.[] | select(.status == \"active\") | .id')\n\necho \"Active Deployment ID: $ACTIVE_DEPLOYMENT_ID\"\n\n# Loop through each deployment ID and delete it, skipping the active deployment\necho \"Deleting deployments...\"\nfor ID in $DEPLOYMENTS; do\n  if [ \"$ID\" != \"$ACTIVE_DEPLOYMENT_ID\" ]; then\n    echo \"Deleting deployment $ID\"\n    curl -X DELETE -H \"Authorization: token $TOKEN\" \\\n         -H \"Accept: application/vnd.github.v3+json\" \\\n         https://api.github.com/repos/$REPO/deployments/$ID\n  else\n    echo \"Skipping active deployment $ID\"\n  fi\n done\n</code></pre>"},{"location":"github-clean-deployments/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Set Up Environment Variables</li> <li> <p>The script expects your GitHub Personal Access Token to be set as <code>GITHUB_TOKEN</code> in your environment.</p> </li> <li> <p>Fetch Deployment IDs</p> </li> <li>Uses <code>curl</code> to call the GitHub API and retrieve all deployments for the specified repository.</li> <li> <p>Pipes the JSON response to <code>jq</code> to extract all deployment IDs.</p> </li> <li> <p>Check for Deployments</p> </li> <li> <p>If no deployments are found, the script exits gracefully.</p> </li> <li> <p>Identify the Active Deployment</p> </li> <li>Fetches deployments again and uses <code>jq</code> to find the deployment with status <code>active</code> (if any).</li> <li> <p>This deployment is skipped during deletion to avoid disrupting an active deployment.</p> </li> <li> <p>Delete Deployments</p> </li> <li>Loops through all deployment IDs.</li> <li>For each deployment, if it is not the active deployment, sends a DELETE request to the GitHub API to remove it.</li> <li>Prints a message for each deletion or if skipping the active deployment.</li> </ol>"},{"location":"github-clean-deployments/#why-use-jq","title":"Why Use jq?","text":"<ul> <li><code>jq</code> is a lightweight and flexible command-line JSON processor.</li> <li>It allows you to extract, filter, and manipulate JSON data returned by the GitHub API with ease.</li> <li>In this script, <code>jq</code> is used to:</li> <li>List all deployment IDs: <code>.[] | .id</code></li> <li>Find the active deployment: <code>.[] | select(.status == \"active\") | .id</code></li> </ul>"},{"location":"github-clean-deployments/#usage","title":"Usage","text":"<ol> <li>Export your GitHub token:    <pre><code>export GITHUB_TOKEN=\"&lt;YOUR_PERSONAL_ACCESS_TOKEN&gt;\"\n</code></pre></li> <li>Make the script executable:    <pre><code>chmod +x ./scripts/github/deployments.sh\n</code></pre></li> <li>Run the script from your project root:    <pre><code>./scripts/github/deployments.sh\n</code></pre></li> </ol>"},{"location":"github-clean-deployments/#summary","title":"Summary","text":"<ul> <li>This script provides a simple, automated way to clean up deployments in a GitHub repository.</li> <li>It leverages the GitHub API, <code>curl</code>, and <code>jq</code> for powerful, flexible automation.</li> <li>There is no UI in GitHub to bulk delete deployments\u2014this script fills that gap for DevOps teams.</li> </ul> <p>Automate your GitHub deployment cleanup today!</p>"},{"location":"meraki-nagios-device-sync/","title":"Automating Cisco Meraki Device Discovery and Nagios XI Monitoring Integration","text":""},{"location":"meraki-nagios-device-sync/#introduction","title":"Introduction","text":"<p>Keeping your network monitoring system in sync with your actual device inventory is critical for reliable operations. This article provides a deep dive into a robust Python workflow that:</p> <ul> <li>Discovers all current devices from the Cisco Meraki cloud API</li> <li>Uses SNMP OIDs to obtain Meraki hostnames</li> <li>Compares Meraki inventory to Nagios XI monitored hosts</li> <li>Adds missing devices to Nagios XI, including handling special device types</li> <li>Checks firmware status for compliance</li> </ul> <p>All code is provided and explained so you can adapt this solution for your own environment.</p>"},{"location":"meraki-nagios-device-sync/#required-python-libraries","title":"Required Python Libraries","text":"<p>This workflow uses the following Python libraries:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library. Used for all Meraki cloud API calls.</li> <li>requests: For making HTTP requests to the Nagios XI REST API.</li> <li>subprocess: To run SNMP commands (e.g., <code>snmpwalk</code>) from Python.</li> <li>re: For parsing SNMP command output with regular expressions.</li> </ul> <p>Install any missing libraries with pip:</p> <pre><code>pip install meraki requests\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1-authenticating-to-cisco-meraki-and-nagios-xi-apis","title":"1. Authenticating to Cisco Meraki and Nagios XI APIs","text":""},{"location":"meraki-nagios-device-sync/#cisco-meraki-api-authentication","title":"Cisco Meraki API Authentication","text":"<p>To connect to the Meraki Dashboard API, you need an API key. This key can be generated in your Meraki dashboard under Organization &gt; Settings &gt; Dashboard API access.</p> <pre><code>import meraki\n\nMERAKI_API_KEY = 'YOUR_MERAKI_API_KEY'  # Replace with your Meraki API key\nMERAKI_BASE_URL = 'https://api.meraki.com/api/v1/'\n\ndashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    base_url=MERAKI_BASE_URL,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre>"},{"location":"meraki-nagios-device-sync/#nagios-xi-api-authentication","title":"Nagios XI API Authentication","text":"<p>Nagios XI provides a REST API. You need an API key, which can be generated in the Nagios XI web interface under My Account &gt; API Keys.</p> <pre><code>import requests\n\nNAGIOS_XI_API_URL = 'https://your-nagios-server.example.com/nagiosxi/api/v1/'  # Replace with your Nagios XI URL\nNAGIOS_XI_API_KEY = 'YOUR_NAGIOS_API_KEY'  # Replace with your Nagios XI API key\n\ndef call_nagios_api(endpoint, method='GET', data=None):\n    url = f\"{NAGIOS_XI_API_URL}{endpoint}\"\n    headers = {'Authorization': f'Bearer {NAGIOS_XI_API_KEY}'}\n    if method == 'GET':\n        response = requests.get(url, headers=headers)\n    elif method == 'POST':\n        response = requests.post(url, headers=headers, json=data)\n    elif method == 'PUT':\n        response = requests.put(url, headers=headers, json=data)\n    elif method == 'DELETE':\n        response = requests.delete(url, headers=headers)\n    else:\n        raise ValueError('Unsupported HTTP method')\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"meraki-nagios-device-sync/#1a-understanding-and-setting-up-meraki_dashboard_snmp_community_string","title":"1a. Understanding and Setting Up <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>","text":""},{"location":"meraki-nagios-device-sync/#what-is-meraki_dashboard_snmp_community_string","title":"What is <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code>?","text":"<p>The <code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING</code> is a shared secret (like a password) used for authenticating SNMP v2c queries to the Meraki cloud SNMP endpoint. It is required to retrieve device information via SNMP, such as hostnames and other device attributes.</p>"},{"location":"meraki-nagios-device-sync/#how-to-set-up-the-snmp-community-string-in-meraki-dashboard","title":"How to Set Up the SNMP Community String in Meraki Dashboard","text":"<ol> <li>Log in to your Meraki Dashboard</li> <li>Navigate to Organization &gt; Settings</li> <li>Scroll to the SNMP section</li> <li>Enable Cloud Monitoring (SNMP v2c)</li> <li>Set your desired SNMP Community String (e.g., <code>mysnmpcommunity</code>)</li> <li>Save your changes</li> <li>Whitelist your public IP address in the SNMP section to allow SNMP queries from your monitoring server</li> </ol> <p>Note: The SNMP community string acts as a password for SNMP v2c. Keep it secure and do not share it publicly.</p>"},{"location":"meraki-nagios-device-sync/#plugging-the-community-string-into-your-code","title":"Plugging the Community String into Your Code","text":"<p>In your Python code, set the value as follows:</p> <pre><code>MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING = 'mysnmpcommunity'  # Replace with your actual SNMP community string\n</code></pre> <p>This value is then used in SNMP queries, for example:</p> <pre><code>import subprocess\nimport re\n\ndef get_snmp_data(snmp_server, port, oid, community):\n    command = [\n        \"snmpwalk\",\n        \"-v\", \"2c\",\n        \"-c\", community,\n        f\"{snmp_server}:{port}\",\n        oid\n    ]\n    try:\n        result = subprocess.run(command, capture_output=True, text=True, check=True)\n        output = result.stdout\n        snmp_dict = {}\n        pattern = re.compile(r'(\\S+)\\s+=\\s+STRING:\\s+\"([^\"]+)\"')\n        for match in pattern.finditer(output):\n            oid = match.group(1)\n            string_value = match.group(2)\n            snmp_dict[string_value] = oid\n        return snmp_dict\n    except Exception as e:\n        print(f\"SNMP error: {e}\")\n        return None\n\nMERAKI_DASHBOARD_SNMP_HOST_NAME = 'snmp.meraki.com'\nMERAKI_DASHBOARD_SNMP_PORT = '16100'\n\nmerakihostnames = get_snmp_data(\n    MERAKI_DASHBOARD_SNMP_HOST_NAME,\n    MERAKI_DASHBOARD_SNMP_PORT,\n    '1.3.6.1.4.1.29671.1.1.4.1.2',\n    MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING\n)\n</code></pre> <p>If you change the community string in the Meraki dashboard, update it in your code as well.</p>"},{"location":"meraki-nagios-device-sync/#2-obtaining-all-current-devices-from-meraki","title":"2. Obtaining All Current Devices from Meraki","text":"<p>We use the official Meraki Dashboard API to fetch all organizations, devices, and networks:</p> <p><pre><code>gdepOrganizations = dashboard.organizations.getOrganizations()\norganizationid = gdepOrganizations[0]['id']\ngdepdevices = dashboard.organizations.getOrganizationDevices(organizationid, -1)\ngdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid, -1)\n</code></pre> - <code>getOrganizations()</code> returns all organizations your API key can access. - <code>getOrganizationDevices()</code> fetches all devices (appliances, switches, cameras, wireless, etc.). - <code>getOrganizationNetworks()</code> fetches all networks (logical groupings of devices).</p>"},{"location":"meraki-nagios-device-sync/#3-obtaining-meraki-hostnames-via-snmp-oid","title":"3. Obtaining Meraki Hostnames via SNMP OID","text":"<p>To get hostnames as seen by Meraki's SNMP dashboard, we use the SNMP OID <code>1.3.6.1.4.1.29671.1.1.4.1.2</code> (see code above).</p> <ul> <li>This function runs an <code>snmpwalk</code> command and parses the output into a dictionary of hostnames and OIDs.</li> <li>SNMP access must be enabled and your IP whitelisted in the Meraki dashboard.</li> </ul>"},{"location":"meraki-nagios-device-sync/#4-checking-firmware-status-for-each-network","title":"4. Checking Firmware Status for Each Network","text":"<p>For each network, we check the current firmware status of all products using the Meraki Dashboard API's <code>getNetworkFirmwareUpgrades</code> method.</p>"},{"location":"meraki-nagios-device-sync/#what-is-getnetworkfirmwareupgrades","title":"What is <code>getNetworkFirmwareUpgrades</code>?","text":"<p>This method retrieves the current and available firmware versions for all devices in a given Meraki network. It helps you: - Audit firmware compliance - Identify devices that need upgrades - Track which products are running which firmware</p>"},{"location":"meraki-nagios-device-sync/#example-usage","title":"Example Usage","text":"<pre><code># For each network, get firmware upgrade status\nfor network in gdepnetworks:\n    network_id = network['id']\n    networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network_id)\n    print(f\"Firmware info for network {network['name']}:\\n\", networkupgrades)\n    if 'products' in networkupgrades:\n        products = networkupgrades['products']\n        for product_type, firmware_info in products.items():\n            print(f\"Product: {product_type}\")\n            print(f\"Current Version: {firmware_info.get('currentVersion', {}).get('name', 'N/A')}\")\n            print(f\"Available Version: {firmware_info.get('availableVersion', {}).get('name', 'N/A')}\")\n            print(f\"Status: {firmware_info.get('status', 'N/A')}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#sample-output-structure","title":"Sample Output Structure","text":"<p>The returned dictionary typically looks like:</p> <pre><code>{\n  \"products\": {\n    \"appliance\": {\n      \"currentVersion\": {\"name\": \"MX 18.107.2\"},\n      \"availableVersion\": {\"name\": \"MX 18.107.4\"},\n      \"status\": \"Up to date\"\n    },\n    \"switch\": {\n      \"currentVersion\": {\"name\": \"MS 15.21\"},\n      \"availableVersion\": {\"name\": \"MS 15.22\"},\n      \"status\": \"Upgrade available\"\n    }\n  }\n}\n</code></pre> <ul> <li><code>currentVersion</code>: The firmware currently running on the product type.</li> <li><code>availableVersion</code>: The latest available firmware for that product type.</li> <li><code>status</code>: Whether the device is up to date or needs an upgrade.</li> </ul> <p>This information can be used to automate firmware compliance checks and trigger upgrades as needed.</p>"},{"location":"meraki-nagios-device-sync/#5-comparing-meraki-devices-to-nagios-xi-hosts","title":"5. Comparing Meraki Devices to Nagios XI Hosts","text":"<p>We fetch all hosts from Nagios XI and compare them to the Meraki inventory:</p> <p><pre><code>nagioshost = call_nagios_api('objects/host')\nfor device in gdepdevices:\n    nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n    if len(nagioshostitems) == 0:\n        # Device is missing from Nagios XI\n        # ...add to missing list and prepare for addition...\n</code></pre> - Devices not found in Nagios XI are flagged for addition. - Special handling for device types (appliance, switch, camera, wireless, etc.).</p>"},{"location":"meraki-nagios-device-sync/#6-adding-missing-devices-to-nagios-xi","title":"6. Adding Missing Devices to Nagios XI","text":"<p>For each missing device, we call helper functions to create/update hosts and services in Nagios XI:</p> <p><pre><code>if len(str(device['name']).strip()) != 0:\n    if (str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS):\n        if (str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS):\n            if (device['productType'] == 'appliance' and 'VMX' not in device['model']):\n                applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                if len(vlan999) == 0:\n                    device['lanIp'] = '0.0.0.0'\n                else:\n                    device['lanIp'] = vlan999[0]['applianceIp']\n            if device['lanIp'] is None:\n                device['lanIp'] = '0.0.0.0'\n            create_update_meraki_host(device, nagioshostitems, gdepnetworks, False)\n            nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n            create_update_meraki_host_services(device, nagiosserviceitems, False, merakihostnames)\n</code></pre> - <code>create_update_meraki_host()</code> and <code>create_update_meraki_host_services()</code> are responsible for adding/updating hosts and their services in Nagios XI. - VLAN and IP logic ensures correct addressing for appliances.</p>"},{"location":"meraki-nagios-device-sync/#7-applying-configuration","title":"7. Applying Configuration","text":"<p>After all additions/updates, we apply the Nagios XI configuration:</p> <pre><code>data = {'alias': 'Nagios XI', 'applyconfig': '1'}\ncall_nagios_api('config/host/localhost', method='PUT', data=data)\nnagioshost = call_nagios_api('objects/host')\n</code></pre>"},{"location":"meraki-nagios-device-sync/#8-full-function-code-add_missing_network_device_to_nagios","title":"8. Full Function Code: <code>add_missing_network_device_to_nagios</code>","text":"<p>Below is the complete function, ready to adapt for your own environment:</p> <pre><code>def add_missing_network_device_to_nagios():\n    try:\n        # OID to obtain all host names from Meraki SNMP Dashboard\n        merakihostnames = get_snmp_data(\n            MERAKI_DASHBOARD_SNMP_HOST_NAME, \n            MERAKI_DASHBOARD_SNMP_PORT,\n            '1.3.6.1.4.1.29671.1.1.4.1.2',\n            MERAKI_DASHBOARD_SNMP_COMMUNITY_STRING)\n        gdepOrganizations = dashboard.organizations.getOrganizations()\n        organizationid = gdepOrganizations[0]['id']\n        gdepdevices = dashboard.organizations.getOrganizationDevices(organizationid,-1)\n        gdepnetworks = dashboard.organizations.getOrganizationNetworks(organizationid,-1)\n\n        for network in gdepnetworks:\n            networkupgrades = dashboard.networks.getNetworkFirmwareUpgrades(network['id'])\n            if ('products' in networkupgrades):\n                products = networkupgrades['products']\n                # ...process firmware info as needed...\n\n        nagioshost = call_nagios_api('objects/host')\n        nagioshostservices = call_nagios_api('objects/service')\n        nagioshostconfig = call_nagios_api('config/host')\n        nagioshostgroupmembers = call_nagios_api('objects/hostgroupmembers')\n        nagioshostservicesconfig = call_nagios_api('config/service')\n\n        SKIP_MERAKI_HOSTS = ['tst','tes']\n\n        for device in gdepdevices:\n            if (device['productType'] == 'appliance'):\n                # ...handle appliance types...\n                pass\n            elif (device['productType'] == 'camera'):\n                pass\n            elif (device['productType'] == 'switch'):\n                pass\n            elif (device['productType'] == 'wireless'):\n                pass\n            # ...other device handling as needed...\n            nagioshostitems = list(filter(lambda nh: str(nh['host_name']).lower() == str(device['name']).lower(), nagioshost))\n            if (len(nagioshostitems) == 0):\n                if (len(str(device['name']).strip()) != 0):\n                    if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                            # ...add to Nagios XI...\n                            pass\n            if (len(str(device['name']).strip()) != 0):\n                if ((str(device['name']).strip()[0:3].lower() not in SKIP_MERAKI_HOSTS)):\n                    if ((str(device['name']).strip().lower() not in SKIP_MERAKI_HOSTS)):\n                        if ((device['productType'] == 'appliance') and ('VMX' not in device['model'])):\n                            applianceVLANs = dashboard.appliance.getNetworkApplianceVlans(device['networkId'])\n                            vlan999 = list(filter(lambda av: str(av['id']).lower() == str('999').lower(), applianceVLANs))\n                            if (len(vlan999) == 0):\n                                device['lanIp'] = '0.0.0.0'\n                            else:\n                                device['lanIp'] = vlan999[0]['applianceIp']\n                        if (device['lanIp'] is None):\n                            device['lanIp'] = '0.0.0.0'\n                        create_update_meraki_host(device,nagioshostitems,gdepnetworks,False)\n                        nagiosserviceitems = list(filter(lambda ns: str(ns['host_name']).lower() == str(device['name']).lower(), nagioshostservices))\n                        create_update_meraki_host_services(device,nagiosserviceitems,False,merakihostnames)\n        data = {'alias': 'Nagios XI', 'applyconfig': '1'}\n        call_nagios_api('config/host/localhost', method='PUT', data=data)\n        nagioshost = call_nagios_api('objects/host')\n    except Exception as exception_obj:\n        print(f\"Error: {exception_obj}\")\n</code></pre>"},{"location":"meraki-nagios-device-sync/#9-conclusion","title":"9. Conclusion","text":"<p>This workflow ensures your Nagios XI monitoring system is always in sync with your actual Meraki device inventory, with full visibility into firmware status and device types. By automating device discovery, comparison, and configuration, you can maintain a reliable, up-to-date monitoring environment with minimal manual effort.</p>"},{"location":"meraki-nagios-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Python meraki library</li> </ul>"},{"location":"meraki-unused-device-sync/","title":"Identifying Unused Meraki Inventory Devices","text":""},{"location":"meraki-unused-device-sync/#introduction","title":"Introduction","text":"<p>In many enterprise environments, it's important to keep track of network hardware inventory and ensure that all devices are properly assigned and utilized. Unused devices can represent wasted resources or missed opportunities for redeployment. This article demonstrates how to use the Meraki Dashboard API and Python to programmatically identify all Meraki appliances in your organization's inventory that are not currently assigned to any network.</p>"},{"location":"meraki-unused-device-sync/#python-libraries-and-imports","title":"Python Libraries and Imports","text":"<p>The following Python libraries are used in the solution:</p> <ul> <li>meraki: Official Cisco Meraki Dashboard API Python library for interacting with Meraki cloud resources.</li> <li>sys: Provides access to system-specific parameters and functions.</li> <li>subprocess: Used for running shell commands from Python (not directly used in the main function, but present for SNMP examples).</li> <li>re: Regular expressions for parsing SNMP output (not used in the main function).</li> <li>gdepcommon.utils: Custom utility module for error handling and secret management.</li> </ul>"},{"location":"meraki-unused-device-sync/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid Meraki API key with read access to your organization's inventory.</li> <li>The <code>meraki</code> Python package installed (<code>pip install meraki</code>).</li> <li>The API key securely stored and retrieved (in this example, from Azure Key Vault via a utility function).</li> </ul>"},{"location":"meraki-unused-device-sync/#the-get_unused_inventory-function","title":"The <code>get_unused_inventory</code> Function","text":"<p>Below is the complete function to retrieve all unused Meraki inventory devices:</p> <pre><code>def get_unused_inventory():\n    \"\"\"\n    Returns a list of unused inventory devices from the first Meraki organization.\n    Unused inventory is defined as devices in inventory that are not assigned to any network.\n    \"\"\"\n    dashboard = meraki.DashboardAPI(\n        api_key=MERAKI_API_KEY,\n        output_log=False,\n        print_console=False,\n        suppress_logging=True\n    )\n    organizations = dashboard.organizations.getOrganizations()\n    if not organizations:\n        return []\n    organization_id = organizations[0]['id']\n    # Get all inventory devices\n    inventory = dashboard.organizations.getOrganizationInventoryDevices(organization_id)\n    # Filter for unused devices (not assigned to any network)\n    unused_devices = [\n        device for device in inventory\n        if not device.get('networkId')\n    ]\n    for device in unused_devices:\n        product_type = device.get('productType')\n        if product_type == 'camera':\n            device['ciproductType'] = 'Camera (' + device['model'] + ')'\n        elif product_type == 'switch':\n            device['ciproductType']  = 'Switch (' + device['model'] + ')'\n        elif product_type == 'appliance':\n            device['ciproductType'] = 'Appliance (' + device['model'] + ')'\n        elif product_type == 'wireless':\n            device['ciproductType'] = 'Wireless (' + device['model'] + ')'\n        else:\n            device['ciproductType'] = 'Unknown' + device['model']\n\n    return unused_devices\n</code></pre>"},{"location":"meraki-unused-device-sync/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Dashboard API Initialization <pre><code>dashboard = meraki.DashboardAPI(\n    api_key=MERAKI_API_KEY,\n    output_log=False,\n    print_console=False,\n    suppress_logging=True\n)\n</code></pre></li> <li> <p>Initializes the Meraki Dashboard API client using your API key. Logging and console output are suppressed for cleaner operation.</p> </li> <li> <p>Get Organizations <pre><code>organizations = dashboard.organizations.getOrganizations()\nif not organizations:\n    return []\norganization_id = organizations[0]['id']\n</code></pre></p> </li> <li> <p>Retrieves all organizations accessible by the API key. The function uses the first organization found.</p> </li> <li> <p>Get Inventory Devices <pre><code>inventory = dashboard.organizations.getOrganizationInventoryDevices(organization_id)\n</code></pre></p> </li> <li> <p>Fetches all devices in the organization's inventory.</p> </li> <li> <p>Filter for Unused Devices <pre><code>unused_devices = [\n    device for device in inventory\n    if not device.get('networkId')\n]\n</code></pre></p> </li> <li> <p>Filters the inventory for devices that do not have a <code>networkId</code> property, meaning they are not assigned to any network.</p> </li> <li> <p>Label Device Types <pre><code>for device in unused_devices:\n    product_type = device.get('productType')\n    if product_type == 'camera':\n        device['ciproductType'] = 'Camera (' + device['model'] + ')'\n    elif product_type == 'switch':\n        device['ciproductType']  = 'Switch (' + device['model'] + ')'\n    elif product_type == 'appliance':\n        device['ciproductType'] = 'Appliance (' + device['model'] + ')'\n    elif product_type == 'wireless':\n        device['ciproductType'] = 'Wireless (' + device['model'] + ')'\n    else:\n        device['ciproductType'] = 'Unknown' + device['model']\n</code></pre></p> </li> <li> <p>Adds a human-readable label to each unused device based on its type and model.</p> </li> <li> <p>Return the List <pre><code>return unused_devices\n</code></pre></p> </li> <li>Returns the list of unused devices, each with additional labeling for easier reporting or further processing.</li> </ol>"},{"location":"meraki-unused-device-sync/#references","title":"References","text":"<ul> <li>Cisco Meraki Dashboard API Documentation</li> <li>Meraki Python Library on PyPI</li> <li>Official Meraki Python SDK GitHub</li> </ul>"},{"location":"meraki-unused-device-sync/#conclusion","title":"Conclusion","text":"<p>By using the Meraki Dashboard API and Python, you can quickly identify unused inventory devices in your organization. This enables better asset management, cost savings, and improved operational efficiency. The approach can be extended to automate device assignment, generate reports, or integrate with other IT asset management systems.</p>"},{"location":"meraki-vm-deployment/","title":"Deploying Cisco Meraki vMX with BICEP","text":"<p>This technical article walks you through deploying a Cisco Meraki vMX virtual appliance in Azure using a Bicep template. The vMX is commonly used for SD-WAN and secure connectivity between Azure and on-premises or branch locations. This guide explains the Bicep code, required parameters, and best practices for secure deployment.</p>"},{"location":"meraki-vm-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Meraki Dashboard Authentication Token: Obtain your Meraki authentication string from the Meraki Dashboard. Do not hardcode this in your template or documentation.</li> <li>Azure Subscription: Ensure you have sufficient permissions to deploy marketplace resources and managed applications.</li> <li>Resource Groups: Identify or create the resource groups for your vMX and virtual network.</li> </ul>"},{"location":"meraki-vm-deployment/#bicep-template-merakibicep","title":"Bicep Template: <code>meraki.bicep</code>","text":"<p>Below is the Bicep template for deploying the Cisco Meraki vMX. Sensitive values (such as the authentication token) are not included and should be provided securely at deployment time.</p> <pre><code>@description('Deployment location')\nparam location string = 'westus'\n\n@description('This is the name of your VM')\n@metadata({ title: 'VM Name' })\nparam vmName string = 'DRAZGDEPMEDGE01'\n\n@description('This is your authentication string generated by Meraki Dashboard')\nparam merakiAuthToken string // Provide securely at deployment time\n\n@description('Availability zone number for the vMX')\n@allowed([\n  '0'\n  '1'\n  '2'\n  '3'\n])\nparam zone string = '0'\n\n@description('New or Existing VNet Name')\nparam virtualNetworkName string = 'vnet-gdep-pwus-fortinet'\n\n@description('Boolean indicating whether the VNet is new or existing')\nparam virtualNetworkNewOrExisting string = 'existing'\n\n@description('VNet address prefix')\nparam virtualNetworkAddressPrefix string = '10.27.1.0/24'\n\n@description('Resource group of the VNet')\nparam virtualNetworkResourceGroup string = 'dr-rg-gdep-pwus-vnets'\n\n@description('The size of the VM')\nparam virtualMachineSize string = 'Standard_F4s_v2'\n\n@description('New or Existing subnet Name')\nparam subnetName string = 'snet-gdep-pwus-sdwan-public-new'\n\n@description('Subnet address prefix')\nparam subnetAddressPrefix string = '10.27.35.0/24'\nparam applicationResourceName string = 'DRCiscoMeraki'\nparam managedResourceGroupId string = '/subscriptions/&lt;your-subscription-id&gt;/resourceGroups/&lt;your-managed-rg&gt;'\n\nparam managedIdentity object = { type: 'SystemAssigned' }\n\nresource applicationResource 'Microsoft.Solutions/applications@2021-07-01' = {\n  name: applicationResourceName\n  location: location\n  kind: 'MarketPlace'\n  identity: managedIdentity\n  plan: {\n    name: 'cisco-meraki-vmx'\n    product: 'cisco-meraki-vmx'\n    publisher: 'cisco'\n    version: '15.37.4'\n  }\n  properties: {\n    managedResourceGroupId: managedResourceGroupId\n    parameters: {\n      location: {\n        value: location\n      }\n      merakiAuthToken: {\n        value: merakiAuthToken\n      }\n      subnetAddressPrefix: {\n        value: subnetAddressPrefix\n      }\n      subnetName: {\n        value: subnetName\n      }\n      virtualMachineSize: {\n        value: virtualMachineSize\n      }\n      virtualNetworkAddressPrefix: {\n        value: virtualNetworkAddressPrefix\n      }\n      virtualNetworkName: {\n        value: virtualNetworkName\n      }\n      virtualNetworkNewOrExisting: {\n        value: virtualNetworkNewOrExisting\n      }\n      virtualNetworkResourceGroup: {\n        value: virtualNetworkResourceGroup\n      }\n      vmName: {\n        value: vmName\n      }\n      zone: {\n        value: zone\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"meraki-vm-deployment/#parameter-explanations","title":"Parameter Explanations","text":"<ul> <li>location: Azure region for deployment (e.g., <code>westus</code>).</li> <li>vmName: Name for the Meraki vMX VM.</li> <li>merakiAuthToken: Meraki Dashboard authentication string. Provide this securely at deployment time (e.g., via parameter file or secret).</li> <li>zone: Availability zone for the vMX (0-3).</li> <li>virtualNetworkName: Name of the VNet to deploy into.</li> <li>virtualNetworkNewOrExisting: Specify if the VNet is new or existing.</li> <li>virtualNetworkAddressPrefix: Address prefix for the VNet.</li> <li>virtualNetworkResourceGroup: Resource group containing the VNet.</li> <li>virtualMachineSize: Azure VM size for the vMX.</li> <li>subnetName: Name of the subnet for the vMX.</li> <li>subnetAddressPrefix: Address prefix for the subnet.</li> <li>applicationResourceName: Name for the managed application resource.</li> <li>managedResourceGroupId: Resource ID of the managed resource group for the application (update with your values).</li> <li>managedIdentity: System-assigned managed identity for the deployment.</li> </ul>"},{"location":"meraki-vm-deployment/#how-the-bicep-template-works","title":"How the Bicep Template Works","text":"<ul> <li>Marketplace Deployment: Uses the <code>Microsoft.Solutions/applications</code> resource to deploy the Cisco Meraki vMX from the Azure Marketplace.</li> <li>Parameterization: All key settings (network, VM size, zone, etc.) are parameterized for flexibility.</li> <li>Security: The Meraki authentication token is never hardcoded\u2014always provide it securely.</li> <li>Managed Identity: Uses a system-assigned managed identity for secure resource access.</li> </ul>"},{"location":"meraki-vm-deployment/#best-practices","title":"Best Practices","text":"<ul> <li>Never commit sensitive tokens to source control. Use parameter files, Azure Key Vault, or pipeline secrets.</li> <li>Review and update the <code>managedResourceGroupId</code> and other resource IDs for your environment.</li> <li>Monitor deployment via Azure Portal or CLI for success and troubleshooting.</li> </ul>"},{"location":"meraki-vm-deployment/#summary","title":"Summary","text":"<p>This Bicep template enables automated, secure deployment of Cisco Meraki vMX in Azure. By parameterizing all key settings and handling secrets securely, you can quickly integrate Meraki SD-WAN into your Azure landing zone.</p> <p>Ready to deploy? Use this template with your own parameters and secrets for a secure, repeatable deployment.</p>"},{"location":"monitoring-itsm-integration/","title":"Integrating Monitoring System with ITSM System","text":"<p>This technical article provides a comprehensive, vendor-neutral guide to integrating a monitoring system (Nagios) with an ITSM system (ServiceDesk Plus by ManageEngine) using Python. The solution demonstrates how to automatically open and close ITSM tickets based on monitoring events, with all relevant code and in-line explanations. </p>"},{"location":"monitoring-itsm-integration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Non-Standard Python Libraries Used</li> <li>Connecting to Nagios and ServiceDesk Plus APIs</li> <li>Core Integration Workflow<ul> <li>open_and_close_incidents Function</li> <li>closeincidents Function</li> <li>openincidents Function</li> </ul> </li> <li>Supporting Functions</li> <li>Conclusion</li> </ol>"},{"location":"monitoring-itsm-integration/#overview","title":"Overview","text":"<p>Automating the integration between a monitoring system (such as Nagios) and an ITSM tool (such as ServiceDesk Plus) enables organizations to streamline incident management. This integration ensures that alerts from the monitoring system automatically generate, update, or close tickets in the ITSM tool, reducing manual effort and improving response times.</p> <p>This article provides a step-by-step guide, including all relevant Python code, to: - Connect to both Nagios and ServiceDesk Plus via their APIs - Open tickets in the ITSM tool when monitoring events occur - Close tickets in the ITSM tool when issues are resolved in the monitoring system</p>"},{"location":"monitoring-itsm-integration/#non-standard-python-libraries-used","title":"Non-Standard Python Libraries Used","text":"<p>The following non-standard Python libraries are used in this solution:</p> <ul> <li><code>requests</code>: For making HTTP requests to Nagios and ServiceDesk Plus APIs</li> <li><code>datetime</code>, <code>calendar</code>: For date and time manipulations</li> <li><code>csv</code>: For exporting data to CSV (if needed)</li> </ul> <p>Install these libraries using pip if not already available:</p> <pre><code>pip install requests\n</code></pre>"},{"location":"monitoring-itsm-integration/#connecting-to-nagios-and-servicedesk-plus-apis","title":"Connecting to Nagios and ServiceDesk Plus APIs","text":"<p>To interact with both systems, you need API endpoints and credentials. Below are example constants (replace with your own values):</p> <pre><code># Constants for ServiceDesk Plus (ITSM)\nSERVICE_DESK_API_KEY = 'YOUR_SERVICEDESKPLUS_API_KEY'\nSERVICEDESK_BASE_URL = 'https://your-servicedeskplus-instance/api/v3/'\nSERVICE_DESK_USER = 'automation_user'  # The user that creates tickets via API\n\n# Constants for Nagios\nNAGIOS_API_KEY = 'YOUR_NAGIOS_API_KEY'\nNAGIOSXI_BASE_URL = 'https://your-nagios-instance/nagiosxi/api/v1/'\nNAGIOS_COMMENT_FORMAT = 'Automated Ticket Number Added By Interface Engine '\n</code></pre>"},{"location":"monitoring-itsm-integration/#core-integration-workflow","title":"Core Integration Workflow","text":"<p>The main workflow is orchestrated by the <code>open_and_close_incidents</code> function, which: - Retrieves the current Nagios host inventory from a database - Fetches the current state of monitored hosts and services from Nagios - Closes incidents in the ITSM tool if the corresponding monitoring issue is resolved - Opens new incidents in the ITSM tool for new monitoring issues</p>"},{"location":"monitoring-itsm-integration/#open_and_close_incidents-function","title":"open_and_close_incidents Function","text":"<pre><code>def open_and_close_incidents():\n    try:\n        # Retrieve Nagios host inventory from a database\n        nagioshostinventory = execute_sql_fetch_dicts('select lower(host_name) as host_name, notes from fact_nagios_hosts ')\n        # Fetch current Nagios host group members via API\n        nagioshostgroupmembers = get_data_from_Nagios(None, 'objects/hostgroupmembers', 'hostgroup')\n\n        # Close incidents in ITSM tool for resolved monitoring issues\n        hostandservicesjustclosed = closeincidents(nagioshostgroupmembers)\n        # Open new incidents in ITSM tool for new monitoring issues\n        openincidents(nagioshostgroupmembers, nagioshostinventory, hostandservicesjustclosed)\n\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation","title":"Explanation","text":"<ul> <li>execute_sql_fetch_dicts: Retrieves the current Nagios host inventory from a database table.</li> <li>get_data_from_Nagios: Calls the Nagios API to get host group membership information.</li> <li>closeincidents: Handles closing tickets in the ITSM tool for issues that have been resolved in Nagios.</li> <li>openincidents: Handles opening new tickets in the ITSM tool for new issues detected by Nagios.</li> </ul>"},{"location":"monitoring-itsm-integration/#closeincidents-function","title":"closeincidents Function","text":"<p>This function closes tickets in the ITSM tool when the corresponding monitoring issue is resolved in Nagios.</p> <pre><code>def closeincidents(nagioshostgroupmembers):\n    try:\n        # Calculate timestamp for incidents created in the last 10 days\n        twoweeksago = datetime.datetime.now() - datetime.timedelta(days=10)\n        twoweeksago = str(calendar.timegm(twoweeksago.timetuple())) + '000'\n        # Search for resolved/closed/cancelled tickets created by the automation user\n        searchcrteria = [\n            {\"field\": \"status.name\", \"condition\": \"is \", \"values\": [\"Resolved\", \"Closed\", \"Cancelled\"]},\n            {\"field\": \"created_by.name\", \"condition\": \"is\", \"logical_operator\": \"and\", \"value\": str(SERVICE_DESK_USER)},\n            {\"field\": \"created_time\", \"condition\": \"greater than\", \"value\": twoweeksago, \"logical_operator\": \"and\"}\n        ]\n        servicedeskincidents = get_all_Service_Desk_Requests('requests', 'requests', searchcrteria, SERVICE_DESK_API_KEY, SERVICEDESK_BASE_URL)\n        nagiosallcurrentcomments = get_data_from_Nagios(None, 'objects/comment', 'comment')\n        for eachNagiosComment in nagiosallcurrentcomments:\n            commentData = str(eachNagiosComment['comment_data'])\n            if NAGIOS_COMMENT_FORMAT in commentData:\n                nagiosticketnumber = commentData[(len(NAGIOS_COMMENT_FORMAT) - len(commentData)):]\n                deleteNagiosAcknowledgement = False\n                for eachiSightRequest in servicedeskincidents:\n                    if eachiSightRequest['id'] == nagiosticketnumber:\n                        deleteNagiosAcknowledgement = True\n                        break\n                if deleteNagiosAcknowledgement:\n                    delete_nagios_acknowledgement(eachNagiosComment['host_name'], eachNagiosComment['service_description'])\n        # Close requests in ITSM tool if not present in Nagios comments\n        for eachiSightRequest in servicedeskincidents:\n            hname, sname, onerowaffected = close_Request_if_ticket_not_in_comments(eachiSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers)\n            if onerowaffected and hname is not None:\n                hostandservicesjustclosed.append({\"hname\": hname, \"sname\": sname})\n        # Repeat for open tickets\n        searchcrteria = [\n            {\"field\": \"status.name\", \"condition\": \"is not\", \"values\": [\"Resolved\", \"Closed\", \"Cancelled\"]},\n            {\"field\": \"created_by.name\", \"condition\": \"is\", \"logical_operator\": \"and\", \"value\": str(SERVICE_DESK_USER)}\n        ]\n        servicedeskincidents = get_all_Service_Desk_Requests('requests', 'requests', searchcrteria, SERVICE_DESK_API_KEY, SERVICEDESK_BASE_URL)\n        nagiosallcurrentcomments = get_data_from_Nagios(None, 'objects/comment', 'comment')\n        for eachiSightRequest in servicedeskincidents:\n            hname, sname, onerowaffected = close_Request_if_ticket_not_in_comments(eachiSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers)\n            if onerowaffected and hname is not None:\n                hostandservicesjustclosed.append({\"hname\": hname, \"sname\": sname})\n        return hostandservicesjustclosed\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation_1","title":"Explanation","text":"<ul> <li>get_all_Service_Desk_Requests: Queries the ITSM tool for tickets matching certain criteria.</li> <li>get_data_from_Nagios: Retrieves current comments (acknowledgements) from Nagios.</li> <li>delete_nagios_acknowledgement: Removes acknowledgement in Nagios if the corresponding ticket is resolved.</li> <li>close_Request_if_ticket_not_in_comments: Closes the ITSM ticket if it is no longer present in Nagios comments.</li> </ul>"},{"location":"monitoring-itsm-integration/#openincidents-function","title":"openincidents Function","text":"<p>This function opens new tickets in the ITSM tool for new monitoring issues detected by Nagios.</p> <pre><code>def openincidents(nagioshostgroupmembers, nagioshostinventory, hostandservicesjustclosed):\n    try:\n        # ...implementation to open new tickets in ITSM tool based on Nagios alerts...\n        # Typically, this involves:\n        # 1. Fetching current Nagios issues (hosts/services in warning/critical state)\n        # 2. Checking if a ticket already exists for the issue\n        # 3. If not, creating a new ticket in the ITSM tool via API\n        # 4. Adding a comment/acknowledgement in Nagios with the ticket number\n        pass\n    except Exception as localExceptionObject:\n        handle_global_exception(sys._getframe().f_code.co_name, localExceptionObject)\n    finally:\n        pass\n</code></pre>"},{"location":"monitoring-itsm-integration/#explanation_2","title":"Explanation","text":"<ul> <li>openincidents is responsible for creating new tickets in the ITSM tool for issues detected by Nagios that do not already have an open ticket.</li> <li>The function typically fetches current issues from Nagios, checks for existing tickets, and creates new ones as needed.</li> </ul>"},{"location":"monitoring-itsm-integration/#supporting-functions","title":"Supporting Functions","text":"<p>Below are key supporting functions referenced in the workflow. These functions handle API calls, database queries, and other integration logic.</p>"},{"location":"monitoring-itsm-integration/#execute_sql_fetch_dicts","title":"execute_sql_fetch_dicts","text":"<pre><code>def execute_sql_fetch_dicts(sqlstatement):\n    \"\"\"\n    Executes a SQL query and returns the results as a list of dictionaries.\n    Implementation depends on your database setup. Example below uses pyodbc.\n    \"\"\"\n    import pyodbc\n    results = []\n    try:\n        # Replace with your actual connection string\n        conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=your_server;DATABASE=your_db;UID=your_user;PWD=your_password')\n        cursor = conn.cursor()\n        cursor.execute(sqlstatement)\n        columns = [column[0] for column in cursor.description]\n        for row in cursor.fetchall():\n            results.append(dict(zip(columns, row)))\n    except Exception as e:\n        print(f\"SQL execution error: {e}\")\n    finally:\n        try:\n            conn.close()\n        except:\n            pass\n    return results\n</code></pre>"},{"location":"monitoring-itsm-integration/#get_data_from_nagios","title":"get_data_from_Nagios","text":"<pre><code>import requests\nimport json\n\ndef get_data_from_Nagios(queryParms, object2Query, responseField):\n    \"\"\"\n    Calls the Nagios API and returns the requested data.\n    \"\"\"\n    try:\n        params = {'apikey': NAGIOS_API_KEY}\n        if queryParms is not None:\n            params.update(queryParms)\n        response = requests.get(NAGIOSXI_BASE_URL + object2Query, params=params, verify=False)\n        response_content = response.text\n        json_data = json.loads(response_content)\n        if responseField is None:\n            return json_data\n        else:\n            return json_data[responseField]\n    except Exception as e:\n        print(f\"Nagios API error: {e}\")\n        return None\n</code></pre>"},{"location":"monitoring-itsm-integration/#get_all_service_desk_requests","title":"get_all_Service_Desk_Requests","text":"<pre><code>import requests\nimport json\n\ndef get_all_Service_Desk_Requests(object2Query, responseField, searchcriteria, apikey, apibaseurl):\n    \"\"\"\n    Calls the ServiceDesk Plus API to retrieve tickets matching the search criteria.\n    \"\"\"\n    try:\n        headers = {'Authtoken': apikey}\n        url = f\"{apibaseurl}{object2Query}\"\n        params = {\"input_data\": json.dumps({\"criteria\": searchcriteria})}\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        return response.json().get(responseField, [])\n    except Exception as e:\n        print(f\"ServiceDesk API error: {e}\")\n        return []\n</code></pre>"},{"location":"monitoring-itsm-integration/#delete_nagios_acknowledgement","title":"delete_nagios_acknowledgement","text":"<pre><code>import requests\n\ndef delete_nagios_acknowledgement(hostname, servicedescription):\n    \"\"\"\n    Removes an acknowledgement (comment) from Nagios for the given host/service.\n    \"\"\"\n    try:\n        if not servicedescription:\n            nagiosCommand = f'cmd=REMOVE_HOST_ACKNOWLEDGEMENT;{hostname}'\n        else:\n            nagiosCommand = f'cmd=REMOVE_SVC_ACKNOWLEDGEMENT;{hostname};{servicedescription}'\n        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n        OBJECT_URL = 'system/corecommand'\n        response = requests.post(\n            NAGIOSXI_BASE_URL + OBJECT_URL,\n            params={'apikey': NAGIOS_API_KEY},\n            headers=headers,\n            data=nagiosCommand,\n            verify=False\n        )\n        return response.status_code == 200\n    except Exception as e:\n        print(f\"Nagios acknowledgement deletion error: {e}\")\n        return False\n</code></pre>"},{"location":"monitoring-itsm-integration/#close_request_if_ticket_not_in_comments","title":"close_Request_if_ticket_not_in_comments","text":"<pre><code>def close_Request_if_ticket_not_in_comments(iSightRequest, nagiosallcurrentcomments, nagioshostgroupmembers):\n    \"\"\"\n    Closes the ITSM ticket if it is no longer present in Nagios comments.\n    Returns (hostname, servicename, rowaffected)\n    \"\"\"\n    lookforRequestNumber = str(iSightRequest['id'])\n    closerequest = True\n    hostname = None\n    servicename = None\n    onerowaffected = False\n    for eachNagiosComment in nagiosallcurrentcomments:\n        commentData = str(eachNagiosComment['comment_data'])\n        if lookforRequestNumber in commentData:\n            closerequest = False\n            break\n    if closerequest:\n        # Here you would call the ITSM API to close the ticket\n        # For demonstration, we just print and return\n        print(f\"Closing ITSM ticket {lookforRequestNumber} as it is not present in Nagios comments.\")\n        onerowaffected = True\n        # Optionally, update the ticket status via API here\n    return hostname, servicename, onerowaffected\n</code></pre>"},{"location":"monitoring-itsm-integration/#handle_global_exception","title":"handle_global_exception","text":"<pre><code>def handle_global_exception(functionName, exceptionObject):\n    \"\"\"\n    Handles exceptions and sends notification emails if needed.\n    \"\"\"\n    import traceback\n    print(f\"Exception in {functionName}: {exceptionObject}\")\n    print(traceback.format_exc())\n    # Optionally, send an email notification here\n    # send_email(recipients=[...], subject='Exception occurred', plain_message=str(exceptionObject))\n</code></pre>"},{"location":"monitoring-itsm-integration/#conclusion","title":"Conclusion","text":"<p>By following the approach and code provided in this article, you can automate the integration between your monitoring system (Nagios) and ITSM tool (ServiceDesk Plus or similar). This enables automatic ticket creation and closure based on real-time monitoring events, improving incident response and reducing manual workload.</p> <p>Adapt the code and API calls as needed for your specific environment and ITSM/monitoring platforms.</p>"},{"location":"proofpoint-user-management/","title":"Automating User Management in Proofpoint Essentials","text":""},{"location":"proofpoint-user-management/#introduction","title":"Introduction","text":"<p>Proofpoint Essentials provides robust APIs for managing users and optimizing licensing. Marking certain users as \"functional accounts\" (such as service, shared, or terminated accounts) can help reduce licensing costs and improve compliance. This article demonstrates how to:</p> <ul> <li>Connect to the Proofpoint Essentials API using an API user and key</li> <li>Retrieve active users from Proofpoint</li> <li>Compare users to your HR system (e.g., ADP or any HRIS)</li> <li>Mark users as functional accounts via API</li> </ul> <p>All code is provided in Python, and the approach is company-agnostic and suitable for any enterprise environment.</p>"},{"location":"proofpoint-user-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Proofpoint Essentials administrator access</li> <li>An API user and API key (see below)</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Access to your HR system data (e.g., via SQL, API, or CSV)</li> </ul>"},{"location":"proofpoint-user-management/#how-to-create-an-api-key-in-proofpoint-essentials","title":"How to Create an API Key in Proofpoint Essentials","text":"<ol> <li>Log in to the Proofpoint Essentials admin portal.</li> <li>Navigate to Account Management &gt; API Keys.</li> <li>Click Create API Key.</li> <li>Assign the key to a dedicated API user with appropriate permissions.</li> <li>Save the API key securely (e.g., in Azure Key Vault or a secrets manager).</li> </ol> <p>For more details, refer to the official Proofpoint Essentials API documentation. </p>"},{"location":"proofpoint-user-management/#step-1-connect-to-the-proofpoint-essentials-api","title":"Step 1: Connect to the Proofpoint Essentials API","text":"<pre><code>import requests\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\nPROOFPOINT_BASE_API = 'https://&lt;your-region&gt;.proofpointessentials.com/api/v1/'\nPROOFPOINT_API_USER = get_azure_kv_sceret('pp-api-user')\nPROOFPOINT_API_PASSWORD = get_azure_kv_sceret('pp-api-key')\n\n# Example: Get all users in your organization\nurl = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\nresponse = requests.get(\n    url=url,\n    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n)\nresponse.raise_for_status()\nusers = response.json().get('users', [])\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The API call retrieves all users for your organization.</p>"},{"location":"proofpoint-user-management/#step-2-retrieve-user-data-from-your-hr-system","title":"Step 2: Retrieve User Data from Your HR System","text":"<p>Assume you have a function to get user data from your HR system (e.g., via SQL):</p> <pre><code>def get_latest_hr_data():\n    sql_statement = \"\"\"\n        SELECT status, email, company_code, worker_category_code, location_code\n        FROM hr_employees WHERE email IS NOT NULL\n    \"\"\"\n    return execute_sql_fetch_dicts(sql_statement)\n</code></pre>"},{"location":"proofpoint-user-management/#step-3-compare-and-mark-users-as-functional-accounts","title":"Step 3: Compare and Mark Users as Functional Accounts","text":"<p>The following function compares Proofpoint users to your HR data and marks users as functional accounts via the API:</p> <pre><code>def mark_users_as_functional(hr_users):\n    try:\n        # Get active users from Proofpoint\n        url_to_invoke = PROOFPOINT_BASE_API + 'orgs/&lt;your-domain&gt;/users'\n        response = requests.get(\n            url=url_to_invoke,\n            headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD},\n        )\n        response.raise_for_status()\n        active_users_from_pp = response.json().get('users', [])\n\n        hr_emails = {user['email'].lower(): user for user in hr_users}\n        users_to_mark_functional = []\n\n        for user in active_users_from_pp:\n            if user.get('type') == 'end_user':\n                email = user['primary_email'].lower()\n                hr_user = hr_emails.get(email)\n                # Example logic: mark as functional if not in HR or if terminated\n                if not hr_user or hr_user['status'].lower() == 'terminated':\n                    users_to_mark_functional.append(user)\n\n        for user in users_to_mark_functional:\n            email_to_lookup = user['primary_email']\n            url_update = PROOFPOINT_BASE_API + f'orgs/&lt;your-domain&gt;/users/{email_to_lookup}'\n            json_update_values = {\n                \"uid\": user['uid'],\n                \"primary_email\": email_to_lookup,\n                \"is_active\": True,\n                \"type\": \"functional_account\"\n            }\n            try:\n                requests.put(\n                    url_update,\n                    json=json_update_values,\n                    headers={'X-user': PROOFPOINT_API_USER, 'X-password': PROOFPOINT_API_PASSWORD}\n                ).raise_for_status()\n            except requests.RequestException:\n                pass\n        return True\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n</code></pre> <p>Explanation: - Retrieves all active users from Proofpoint. - Compares each user to the HR system. - Marks users as functional accounts if they are not in HR or are terminated. - Updates are made via the Proofpoint API.</p>"},{"location":"proofpoint-user-management/#full-example-orchestrating-the-process","title":"Full Example: Orchestrating the Process","text":"<pre><code>def main():\n    hr_data = get_latest_hr_data()\n    mark_users_as_functional(hr_data)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"proofpoint-user-management/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the process of marking functional accounts in Proofpoint Essentials, optimizing your licensing and compliance posture. The approach is secure, repeatable, and adaptable to any enterprise environment.</p> <p>For more details, consult the Proofpoint Essentials API documentation.</p>"},{"location":"sap-concur-expense-reports-aggregation/","title":"Automating SAP Concur Expense Report Aggregation and Adaptive Card Notifications","text":""},{"location":"sap-concur-expense-reports-aggregation/#introduction","title":"Introduction","text":"<p>This article provides a comprehensive, company-agnostic walkthrough for automating SAP Concur expense report aggregation and delivering actionable, interactive notifications to managers using Adaptive Cards. We\u2019ll cover:</p> <ul> <li>Securely connecting to SAP Concur with OAuth2</li> <li>Fetching and processing users and expense reports</li> <li>Aggregating by employee and by full management chain (organization-wide rollup)</li> <li>Creating and sending Adaptive Card emails with summary/detail toggles</li> <li>All supporting functions, with code and explanations</li> </ul> <p>By the end, you\u2019ll be able to connect to your own SAP Concur instance and deliver organization-wide expense insights to managers in a modern, interactive format.</p>"},{"location":"sap-concur-expense-reports-aggregation/#1-connecting-to-sap-concur-api","title":"1. Connecting to SAP Concur API","text":"<p>To fetch expense reports, you need to: - Obtain an OAuth2 access token using your SAP Concur client credentials and refresh token. - Use the access token to call the Concur API endpoints for users and expense reports.</p>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_scope","title":"Supporting Function: <code>get_scope()</code>","text":"<p>SAP Concur APIs require a specific OAuth2 scope string. This function returns the required scope for all expense and user operations:</p> <pre><code>def get_scope():\n    return (\n        \"openid USER user.read user.write LIST spend.list.read spend.listitem.read CONFIG EXPRPT FISVC \"\n        \"creditcardaccount.read IMAGE expense.exchangerate.writeonly profile.user.generaluser.read \"\n        \"profile.user.generalemployee.read expense.report.read expense.report.readwrite spend.list.write \"\n        \"spend.listitem.write identity.user.ids.read identity.user.core.read identity.user.coresensitive.read \"\n        \"identity.user.enterprise.read identity.user.event.read\"\n    )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_access_token","title":"Supporting Function: <code>get_access_token()</code>","text":"<p>This function retrieves an OAuth2 access token using your client ID, secret, and refresh token:</p> <pre><code>def get_access_token():\n    try:\n        return get_authentication_token(\n            client_id=SAP_CONCUR_CLIENT_APP_ID,\n            client_secret=SAP_CONCUR_CLIENT_SECRET,\n            refresh_token=SAP_CONCUR_REFRESH_TOKEN,\n            scope=get_scope(),\n        )\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_authentication_token","title":"Supporting Function: <code>get_authentication_token()</code>","text":"<p>Handles the actual OAuth2 token request:</p> <pre><code>def get_authentication_token(client_id, client_secret, refresh_token, scope):\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n        \"scope\": scope,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n    response = requests.post(SAP_CONCUR_OAUTH_END_POINT, headers=headers, data=data)\n    response.raise_for_status()\n    return response.json().get(\"access_token\")\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_cached_access_token","title":"Supporting Function: <code>get_cached_access_token()</code>","text":"<p>Caches the access token to avoid unnecessary requests:</p> <pre><code>access_token_cache = {\"token\": None, \"expires_at\": None}\n\ndef get_cached_access_token():\n    if access_token_cache[\"token\"] and access_token_cache[\"expires_at\"] &gt; datetime.now(timezone.utc):\n        return access_token_cache[\"token\"]\n    new_token = get_access_token()\n    if new_token:\n        access_token_cache[\"token\"] = new_token\n        access_token_cache[\"expires_at\"] = datetime.now(timezone.utc) + timedelta(hours=1)\n    return new_token\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#2-fetching-users-and-expense-reports","title":"2. Fetching Users and Expense Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-get_all_sap_concur_users","title":"Supporting Function: <code>get_all_sap_concur_users()</code>","text":"<p>Fetches all users from SAP Concur (with pagination):</p> <pre><code>def get_all_sap_concur_users():\n    try:\n        access_token = get_cached_access_token()\n        base_url = \"https://us.api.concursolutions.com/profile/identity/v4.1/Users\"\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\"\n        }\n        all_users = []\n        next_cursor = None\n        while True:\n            url = base_url\n            if next_cursor:\n                url += f\"?cursor={next_cursor}\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n            all_users.extend(data.get(\"items\", []))\n            next_cursor = data.get(\"nextCursor\")\n            if not next_cursor:\n                break\n        return all_users\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_expense_reports","title":"Supporting Function: <code>fetch_expense_reports()</code>","text":"<p>Fetches all expense reports for a given user:</p> <pre><code>def fetch_expense_reports(user_name, query_parameters):\n    access_token = get_cached_access_token()\n    base_url = f\"https://us.api.concursolutions.com/api/v3.0/expense/reports\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\", \"Accept\": \"application/json\"}\n    reports = []\n    next_page = f\"{base_url}?user={user_name}{query_parameters}\"\n    while next_page:\n        response = requests.get(next_page, headers=headers)\n        response.raise_for_status()\n        data = response.json()\n        reports.extend(data.get(\"Items\", []))\n        next_page = data.get(\"NextPage\")\n    return reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-fetch_all_expense_reports","title":"Supporting Function: <code>fetch_all_expense_reports()</code>","text":"<p>Fetches all reports for all users:</p> <pre><code>def fetch_all_expense_reports(user_mappings, query_parameters):\n    all_reports = []\n    for user in user_mappings:\n        reports = fetch_expense_reports(user, query_parameters)\n        for report in reports:\n            report[\"UserId\"] = user\n        all_reports.extend(reports)\n    return all_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#3-processing-and-aggregating-reports","title":"3. Processing and Aggregating Reports","text":""},{"location":"sap-concur-expense-reports-aggregation/#supporting-function-process_reports","title":"Supporting Function: <code>process_reports()</code>","text":"<p>Normalizes report data for aggregation:</p> <pre><code>def process_reports(all_reports):\n    return [\n        {\n            \"UserId\": report.get(\"UserId\"),\n            \"Name\": report.get(\"Name\"),\n            \"Total\": report.get(\"Total\"),\n            \"CurrencyCode\": report.get(\"CurrencyCode\"),\n            \"SubmitDate\": report.get(\"SubmitDate\"),\n            \"OwnerLoginID\": report.get(\"OwnerLoginID\"),\n            \"OwnerName\": report.get(\"OwnerName\"),\n            \"ApproverLoginID\": report.get(\"ApproverLoginID\"),\n            \"ApproverName\": report.get(\"ApproverName\"),\n            \"ApprovalStatusName\": report.get(\"ApprovalStatusName\"),\n            \"ApprovalStatusCode\": report.get(\"ApprovalStatusCode\"),\n            \"PaymentStatusName\": report.get(\"PaymentStatusName\"),\n            \"PaymentStatusCode\": report.get(\"PaymentStatusCode\"),\n            \"LastModifiedDate\": report.get(\"LastModifiedDate\"),\n            \"AmountDueEmployee\": report.get(\"AmountDueEmployee\"),\n            \"AmountDueCompanyCard\": report.get(\"AmountDueCompanyCard\"),\n            \"TotalClaimedAmount\": report.get(\"TotalClaimedAmount\"),\n            \"TotalApprovedAmount\": report.get(\"TotalApprovedAmount\"),\n            \"LedgerName\": report.get(\"LedgerName\"),\n            \"PolicyID\": report.get(\"PolicyID\"),\n            \"EverSentBack\": report.get(\"EverSentBack\"),\n            \"HasException\": report.get(\"HasException\"),\n            \"URI\": report.get(\"URI\"),\n        }\n        for report in all_reports\n    ]\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-employee-aggregate_expense_reports_by_employee","title":"Aggregating by Employee: <code>aggregate_expense_reports_by_employee()</code>","text":"<p>Groups and sums expense reports for each employee, optionally by approval status or by individual report.</p> <pre><code>def aggregate_expense_reports_by_employee(processed_reports, summary):\n    employee_reports = {}\n    for report in processed_reports:\n        user_name = str(report.get(\"OwnerLoginID\", \"\") or \"\").lower()\n        report_name = report.get(\"Name\", \"\")\n        report_id = report.get(\"ReportID\", \"\")\n        approval_status_code = str(report.get(\"ApprovalStatusCode\", \"\") or \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        key = (\n            f\"{approval_status_code}-({approval_status_name})\"\n            if summary\n            else f\"{report_name}-({report_id})-{approval_status_code}-({approval_status_name})\"\n        )\n        total = report.get(\"Total\", 0)\n        employee_reports.setdefault(user_name, {}).setdefault(key, 0)\n        employee_reports[user_name][key] += total\n    return employee_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#aggregating-by-organization-aggregate_expense_reports_by_full_oraganization","title":"Aggregating by Organization: <code>aggregate_expense_reports_by_full_oraganization()</code>","text":"<p>Rolls up expense totals for each manager, including all direct and indirect reports, using a recursive helper.</p> <pre><code>def aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, summary):\n    object_organization_reports = {}\n    # Build a reverse mapping of manager to their direct reports\n    manager_to_reports = {}\n    for employee, details in management_upns.items():\n        manager = details.get(\"manager\")\n        if manager:\n            manager_to_reports.setdefault(manager.lower(), []).append(employee.lower())\n    def aggregate_totals_upwards(manager, visited):\n        if manager in visited:\n            return\n        visited.add(manager)\n        if manager not in object_organization_reports:\n            object_organization_reports[manager] = {}\n        for employee in manager_to_reports.get(manager, []):\n            aggregate_totals_upwards(employee, visited)\n            for status, total in object_organization_reports.get(employee, {}).items():\n                if status not in object_organization_reports[manager]:\n                    object_organization_reports[manager][status] = 0\n                object_organization_reports[manager][status] += total\n    # Populate initial totals for each employee based on processed reports\n    for report in processed_reports:\n        if report.get(\"UserManager\"):\n            user_manager = report.get(\"UserManager\", \"\").lower()\n        else:\n            continue\n        approval_status_code = report.get(\"ApprovalStatusCode\", \"\").lower()\n        approval_status_name = report.get(\"ApprovalStatusName\", \"\")\n        user_name = report.get(\"OwnerLoginID\", \"\")\n        key = f\"{approval_status_code}-({approval_status_name})\" if summary else f\"{user_name}-{approval_status_code}-({approval_status_name})\"\n        total = report.get(\"Total\", 0)\n        if user_manager not in object_organization_reports:\n            object_organization_reports[user_manager] = {}\n        if key not in object_organization_reports[user_manager]:\n            object_organization_reports[user_manager][key] = 0\n        object_organization_reports[user_manager][key] += total\n    # Aggregate totals upwards starting from all unique managers\n    visited = set()\n    for manager in manager_to_reports.keys():\n        aggregate_totals_upwards(manager, visited)\n    return object_organization_reports\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#4-creating-adaptive-card-emails-summary-vs-detail-toggle","title":"4. Creating Adaptive Card Emails (Summary vs. Detail Toggle)","text":"<p>Adaptive Cards are JSON payloads that Outlook and Teams can render as interactive UI. Here\u2019s how to create a card with a summary and a toggle for details:</p> <pre><code>def create_adaptive_info_card_for_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    try:\n        summary_total = summary_by_organization.get(manager_email, 0)\n        detail_items = [\n            {\n                \"type\": \"TextBlock\",\n                \"text\": f\"{user}: {summary_by_employee.get(user, 0):,.2f}\",\n                \"wrap\": True\n            }\n            for user in detail_by_organization.get(manager_email, [])\n        ]\n        adaptive_card = {\n            \"type\": \"AdaptiveCard\",\n            \"version\": \"1.4\",\n            \"body\": [\n                {\"type\": \"TextBlock\", \"text\": \"Expense Report Summary\", \"weight\": \"Bolder\", \"size\": \"Large\"},\n                {\"type\": \"TextBlock\", \"text\": f\"Total for your organization: {summary_total:,.2f}\", \"wrap\": True},\n                {\n                    \"type\": \"TextBlock\",\n                    \"text\": \"Click below to view details.\",\n                    \"wrap\": True,\n                    \"spacing\": \"Medium\"\n                },\n                {\n                    \"type\": \"Container\",\n                    \"id\": \"detailsContainer\",\n                    \"isVisible\": False,\n                    \"items\": detail_items\n                }\n            ],\n            \"actions\": [\n                {\n                    \"type\": \"Action.ToggleVisibility\",\n                    \"title\": \"Show/Hide Details\",\n                    \"targetElements\": [\"detailsContainer\"]\n                }\n            ]\n        }\n        return adaptive_card\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return None\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#sending-the-adaptive-card-email","title":"Sending the Adaptive Card Email","text":"<pre><code>def send_adaptive_info_email_to_manager(manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports):\n    adaptive_card = create_adaptive_info_card_for_manager(\n        manager_email, summary_by_employee, summary_by_organization, detail_by_organization, user_expense_reports\n    )\n    email_payload = {\n        \"message\": {\n            \"subject\": \"Expense Report Summary\",\n            \"body\": {\n                \"contentType\": \"HTML\",\n                \"content\": (\n                    f\"&lt;html&gt;&lt;head&gt;&lt;meta http-equiv='Content-Type' content='text/html; charset=utf-8'&gt;\"\n                    f\"&lt;script type='application/adaptivecard+json'&gt;{json.dumps(adaptive_card, indent=4)}&lt;/script&gt;\"\n                    f\"&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\n                )\n            },\n            \"from\": {\"emailAddress\": {\"address\": SMTP_FROM_SEND_EMAIL}},\n            \"toRecipients\": [{\"emailAddress\": {\"address\": manager_email}}],\n        }\n    }\n    send_adaptive_card_email(email_payload)\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#5-end-to-end-workflow-example","title":"5. End-to-End Workflow Example","text":"<p>Here\u2019s a high-level workflow you can adapt:</p> <pre><code>def main():\n    # 1. Fetch management hierarchy from your HR system\n    management_upns = fetch_management_upns()  # {employee: {\"manager\": manager_email, ...}}\n    # 2. Fetch all SAP Concur users\n    sap_concur_users = get_all_sap_concur_users()\n    # 3. Fetch all expense reports for all users\n    all_reports = fetch_all_expense_reports(sap_concur_users, \"&amp;submitDateAfter=2025-01-01\")\n    # 4. Normalize and process reports\n    processed_reports = process_reports(all_reports)\n    # 5. Aggregate by employee and organization\n    summary_by_employee = aggregate_expense_reports_by_employee(processed_reports, True)\n    summary_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, True)\n    detail_by_organization = aggregate_expense_reports_by_full_oraganization(processed_reports, management_upns, False)\n    # 6. Send Adaptive Card emails to each manager\n    for manager_email in summary_by_organization:\n        send_adaptive_info_email_to_manager(\n            manager_email, summary_by_employee, summary_by_organization, detail_by_organization, processed_reports\n        )\n</code></pre>"},{"location":"sap-concur-expense-reports-aggregation/#references","title":"References","text":"<ul> <li>SAP Concur API Reference</li> <li>Microsoft Adaptive Cards</li> <li>Microsoft Graph API for Sending Mail</li> </ul>"},{"location":"sap-concur-expense-reports-aggregation/#conclusion","title":"Conclusion","text":"<p>With these patterns and supporting functions, you can connect to your own SAP Concur instance, fetch and aggregate expense reports by employee and by full reporting chain, and deliver actionable, interactive notifications to managers using Adaptive Cards. This enables powerful, organization-wide financial insights and automated reporting for managers at every level.</p>"},{"location":"sap-rfc-python-container/","title":"Installing the <code>PyRFC</code> Module for SAP Integration: A Step-by-Step Guide","text":"<p>Integrating Python with SAP systems using the <code>PyRFC</code> module can unlock powerful automation and data access capabilities. This article provides a clear, professional walkthrough for setting up the SAP NetWeaver RFC SDK and building the <code>PyRFC</code> Python package from scratch.</p>"},{"location":"sap-rfc-python-container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the SAP NetWeaver RFC SDK (download from the official SAP website)</li> <li>Basic familiarity with Linux command line</li> <li>Python 3.x and administrative privileges on your system</li> </ul>"},{"location":"sap-rfc-python-container/#1-download-the-netweaver-rfc-sdk","title":"1. Download the NetWeaver RFC SDK","text":"<ul> <li>Download the latest NetWeaver RFC SDK from the SAP website.</li> <li>Place the downloaded file (<code>nwrfc750P_14-70002752.zip</code>) in your repository's <code>assets</code> folder for easy access.</li> </ul>"},{"location":"sap-rfc-python-container/#2-prepare-the-sap-sdk-directory","title":"2. Prepare the SAP SDK Directory","text":"<p>Create the target directory for the SAP SDK:</p> <pre><code>sudo mkdir -p /usr/local/sap/\n</code></pre>"},{"location":"sap-rfc-python-container/#3-extract-and-copy-the-sdk","title":"3. Extract and Copy the SDK","text":"<ul> <li>Extract the <code>nwrfcsdk</code> folder from the ZIP file.</li> <li>Copy the extracted <code>nwrfcsdk</code> folder to <code>/usr/local/sap/</code>.</li> </ul>"},{"location":"sap-rfc-python-container/#4-configure-the-library-path","title":"4. Configure the Library Path","text":"<p>Create a configuration file for the dynamic linker and add the SDK library path:</p> <pre><code>sudo nano /etc/ld.so.conf.d/nwrfcsdk.conf\n\n# Add the following line to the file:\n/usr/local/sap/nwrfcsdk/lib\n</code></pre>"},{"location":"sap-rfc-python-container/#5-update-the-library-cache-and-set-environment-variable","title":"5. Update the Library Cache and Set Environment Variable","text":"<p>Update the system's library cache and set the required environment variable:</p> <pre><code>sudo ldconfig\n# Verify the path configuration should not have any errors\nldconfig -p | grep sap\n# Set Environment Variable\nexport SAPNWRFC_HOME=/usr/local/sap/nwrfcsdk\n</code></pre>"},{"location":"sap-rfc-python-container/#6-install-cython-and-build-essentials","title":"6. Install Cython and Build Essentials","text":"<p>Install the necessary build tools and Python dependencies:</p> <pre><code>pip install Cython\nsudo apt-get update\nsudo apt-get install -y build-essential python3-dev\n</code></pre>"},{"location":"sap-rfc-python-container/#7-build-and-install-pyrfc","title":"7. Build and Install <code>pyrfc</code>","text":"<p>Clone the PyRFC repository and build the package:</p> <pre><code>git clone https://github.com/SAP/PyRFC.git\ncd PyRFC\npython -m pip install --upgrade build\nPYRFC_BUILD_CYTHON=yes python -m build --wheel --sdist --outdir dist\npip install --upgrade --no-index --find-links=dist pyrfc\n</code></pre> <p>Pro Tip: Double-check all paths and environment variables before building. For troubleshooting, consult the PyRFC documentation or reach out to the SAP community forums.</p> <p>By following these steps, you\u2019ll have a working Python-to-SAP integration environment using the <code>pyrfc</code> module. Happy coding!</p>"},{"location":"sap-rfc-python/","title":"Calling SAP RFC Function Modules from Python Using PyRFC: A Step-by-Step Guide","text":"<p>Note: For details on installing and configuring the <code>PyRFC</code> module inside a container, see the companion article: Installing the PyRFC Module for SAP Integration</p>"},{"location":"sap-rfc-python/#introduction","title":"Introduction","text":"<p>SAP ECC systems expose powerful RFC (Remote Function Call) interfaces that allow external programs to interact with SAP data and business logic. Python, with the help of the PyRFC library, makes it possible to call these RFC function modules directly and process the results in a modern, flexible way.</p> <p>This article demonstrates how to: - Connect to an SAP ECC 6.0 (EHP 8) system from Python - Call a custom RFC function module  - Pass parameters to the RFC - Retrieve tabular data - Save the results to a CSV file</p> <p>We will use a modular, production-ready approach inspired by real-world enterprise integration scripts.</p>"},{"location":"sap-rfc-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an SAP ECC system with a custom RFC function module you can call</li> <li>SAP user credentials with RFC permissions</li> <li>The PyRFC library installed (see Installing the PyRFC Module for SAP Integration for setup)</li> <li>Python 3.7+</li> </ul>"},{"location":"sap-rfc-python/#example-extracting-data-from-sap-via-rfc","title":"Example: Extracting Data from SAP via RFC","text":"<p>Suppose you want to extract financial data from SAP using a custom RFC function module. The following example shows how to do this in a robust, reusable way.</p>"},{"location":"sap-rfc-python/#1-define-your-rfc-connection-and-extract-configuration","title":"1. Define Your RFC Connection and Extract Configuration","text":"<pre><code>from pyrfc import Connection, LogonError, ABAPApplicationError, ABAPRuntimeError\nimport csv\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"sap_rfc_extract\")\n\n# --- RFC Connection Parameters (replace with your SAP system details) ---\nSAP_CONN_PARAMS = {\n    'ashost': 'SAP_APP_SERVER_HOST',   # SAP application server\n    'sysnr': '00',                     # System number\n    'client': '100',                   # Client number\n    'user': 'SAP_USERNAME',            # SAP user\n    'passwd': 'SAP_PASSWORD',          # SAP password\n    'lang': 'EN',                      # Language\n}\n\n# --- RFC Extract Configuration ---\nEXTRACT_CONFIG = {\n    'example_extract': {\n        'function_module': 'ZMY_CUSTOM_RFC_MODULE',  # Replace with your RFC FM name\n        'table_name': 'IT_RESULT_TAB',              # The table returned by the RFC\n        'params': ['IM_CC', 'IM_YEAR', 'IM_PERIOD'],\n        'default_params': {'IM_CC': '1000', 'IM_YEAR': '2025', 'IM_PERIOD': '05'},\n        'filename_fmt': 'sap_extract_{cc}_{year}_{period}.csv',\n    },\n}\n</code></pre>"},{"location":"sap-rfc-python/#2-utility-functions-for-rfc-calls-and-csv-export","title":"2. Utility Functions for RFC Calls and CSV Export","text":"<pre><code>def call_rfc(conn_params, function_module, params):\n    try:\n        conn = Connection(**conn_params)\n        logger.info(f\"Calling RFC: {function_module} with params: {params}\")\n        return conn.call(function_module, **params)\n    except LogonError as e:\n        logger.error(f\"Logon Error: {e}\")\n    except ABAPApplicationError as e:\n        logger.error(f\"ABAP Application Error: {e}\")\n    except ABAPRuntimeError as e:\n        logger.error(f\"ABAP Runtime Error: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n    return None\n\ndef export_result_to_csv(table_data, filename):\n    if not table_data:\n        logger.warning(\"No data to export.\")\n        return\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=table_data[0].keys())\n        writer.writeheader()\n        writer.writerows(table_data)\n    logger.info(f\"Exported data to {filename}\")\n</code></pre>"},{"location":"sap-rfc-python/#3-main-script-running-the-extract","title":"3. Main Script: Running the Extract","text":"<pre><code>import argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run SAP RFC extract via PyRFC.\")\n    parser.add_argument('--im_cc', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_CC'], help='Company code')\n    parser.add_argument('--im_year', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_YEAR'], help='Fiscal year')\n    parser.add_argument('--im_period', default=EXTRACT_CONFIG['example_extract']['default_params']['IM_PERIOD'], help='Fiscal period')\n    args = parser.parse_args()\n\n    # Prepare parameters for RFC call\n    params = {\n        'IM_CC': args.im_cc,\n        'IM_YEAR': args.im_year,\n        'IM_PERIOD': args.im_period,\n    }\n\n    config = EXTRACT_CONFIG['example_extract']\n    result = call_rfc(SAP_CONN_PARAMS, config['function_module'], params)\n    if result and config['table_name'] in result:\n        # Build filename\n        filename = config['filename_fmt'].format(\n            cc=args.im_cc, year=args.im_year, period=args.im_period\n        )\n        export_result_to_csv(result[config['table_name']], filename)\n    else:\n        logger.error(\"No data returned from RFC or table not found in result.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sap-rfc-python/#4-running-the-script","title":"4. Running the Script","text":"<p>You can run the script from the command line, specifying parameters as needed:</p> <pre><code>python sap_rfc_extract.py --im_cc=1000 --im_year=2025 --im_period=05\n</code></pre> <ul> <li>The script will connect to SAP, call the RFC, and save the results to a CSV file (e.g., <code>sap_extract_1000_2025_05.csv</code>).</li> <li>You can override any parameter using the command line.</li> </ul>"},{"location":"sap-rfc-python/#5-step-by-step-explanation","title":"5. Step-by-Step Explanation","text":"<ol> <li>Configuration:</li> <li>All SAP connection details and extract metadata are defined at the top for easy maintenance.</li> <li> <p>The RFC function module name and table name are generic placeholders\u2014replace them with your actual SAP details.</p> </li> <li> <p>Calling the RFC:</p> </li> <li>The <code>call_rfc</code> function establishes a connection and calls the RFC, handling common SAP errors.</li> <li> <p>Parameters are passed as a dictionary, matching the RFC signature.</p> </li> <li> <p>Exporting Data:</p> </li> <li> <p>The <code>export_result_to_csv</code> function writes the returned table to a CSV file, using the first row's keys as headers.</p> </li> <li> <p>Command-Line Interface:</p> </li> <li> <p>The script uses <code>argparse</code> to allow easy parameter overrides from the command line.</p> </li> <li> <p>Error Handling:</p> </li> <li>All errors are logged, and the script will not crash on SAP or network errors.</li> </ol>"},{"location":"sap-rfc-python/#conclusion","title":"Conclusion","text":"<p>With this approach, you can easily: - Call any SAP RFC function module from Python - Parameterize your extracts - Save results to CSV for downstream processing - Integrate SAP data into modern Python workflows</p> <p>For more advanced scenarios (multi-company code loops, dynamic extract configuration, etc.), see the full project code or reach out for further examples.</p>"},{"location":"sap-rfc-python/#further-reading","title":"Further Reading","text":"<ul> <li>PyRFC Documentation</li> <li>SAP RFC SDK</li> <li>Installing the PyRFC Module for SAP Integration \u2014 How to install and configure PyRFC in a container</li> </ul>"},{"location":"sharepoint-site-library-enumeration/","title":"SharePoint Files and Folders Inventory with Python and Microsoft Graph API","text":""},{"location":"sharepoint-site-library-enumeration/#introduction","title":"Introduction","text":"<p>This article provides a detailed, company-agnostic guide to inventorying all files and folders across all SharePoint sites and document libraries in a Microsoft 365 tenant using Python and the Microsoft Graph API. It focuses on the <code>get_all_files_from_sp</code> function and its supporting functions, with best practices for handling large environments, including recommendations for running the code in an Azure container.</p>"},{"location":"sharepoint-site-library-enumeration/#microsoft-graph-api-endpoint-constant","title":"Microsoft Graph API Endpoint Constant","text":"<p>The code uses the following constant for all Microsoft Graph API v1.0 calls:</p> <p><pre><code>AZURE_GRAPH_V1 = 'https://graph.microsoft.com/v1.0/'\n</code></pre> This ensures all API requests are made to the correct Microsoft Graph endpoint.</p>"},{"location":"sharepoint-site-library-enumeration/#key-functions-and-code-walkthrough","title":"Key Functions and Code Walkthrough","text":""},{"location":"sharepoint-site-library-enumeration/#1-get_all_files_from_sp","title":"1. <code>get_all_files_from_sp</code>","text":"<p>This is the main orchestration function for SharePoint inventory. It: - Retrieves all root SharePoint sites using <code>get_all_sp_sites</code>. - Expands the list to include all subsites with <code>fetch_all_sites_including_subsites</code>. - Iterates through every site and its document libraries, calling <code>process_document_library</code> for each. - Sends notification emails on progress and completion.</p> <p>Full Function Code: <pre><code>def get_all_files_from_sp():\n    try:\n        gdep_sharepoint_root_sites = get_all_sp_sites()\n        gdep_all_sites = fetch_all_sites_including_subsites(gdep_sharepoint_root_sites)\n\n        for site in gdep_all_sites:\n            site_id = site[\"id\"]\n            site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives\"\n\n            document_libraries = execute_odata_query_get(site_url)\n            for library in document_libraries:\n                process_document_library(site_id, library[\"id\"], library[\"name\"], gdep_all_sites)\n                send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n                    subject=f'Completed Doc Lib --&gt;{library[\"name\"]} - on site {site[\"webUrl\"]}',\n                    plain_message=f'Update on SP Library{library[\"name\"]} - for site {site_url}')\n\n        send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n            subject=f'Finished all Sites',\n            plain_message=f'Finished all Sites')\n\n    except Exception as e:\n        handle_global_exception(inspect.currentframe().f_code.co_name, e)\n    finally:\n        pass\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#explanation","title":"Explanation","text":"<ul> <li>Site Discovery: Uses <code>get_all_sp_sites()</code> to get all root sites, then <code>fetch_all_sites_including_subsites()</code> to get all subsites.</li> <li>Document Library Enumeration: For each site, queries all document libraries (drives) and processes them.</li> <li>Progress Notification: Sends emails after each library and when all sites are complete.</li> <li>Error Handling: All exceptions are logged and reported.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#2-get_all_sp_sites","title":"2. <code>get_all_sp_sites</code>","text":"<p>Fetches all root-level SharePoint sites in the tenant using the Microsoft Graph API: <pre><code>def get_all_sp_sites():\n    url = f\"{AZURE_GRAPH_V1}sites?search=*\"\n    return execute_odata_query_get(url)\n</code></pre> - Purpose: Returns a list of all root SharePoint sites. - API Used: List SharePoint Sites</p>"},{"location":"sharepoint-site-library-enumeration/#3-fetch_all_sites_including_subsites","title":"3. <code>fetch_all_sites_including_subsites</code>","text":"<p>Recursively discovers all subsites for each root site: <pre><code>def fetch_all_sites_including_subsites(sharepoint_root_sites):\n    all_sites = []\n    for site in sharepoint_root_sites:\n        logger.info(f\"Started site {site['webUrl']}\")\n        all_sites.append({\"id\": site[\"id\"], \"webUrl\": site[\"webUrl\"]})\n        sharepoint_subsites = get_sp_subsites(site[\"id\"])\n        if len(sharepoint_subsites) &gt; 0:\n            for subsite in sharepoint_subsites:\n                all_sites.append({\"id\": subsite[\"id\"], \"webUrl\": subsite[\"webUrl\"]})\n    return all_sites\n</code></pre> - Purpose: Ensures every site and subsite is included in the inventory. - API Used: List Subsites</p>"},{"location":"sharepoint-site-library-enumeration/#4-process_document_library","title":"4. <code>process_document_library</code>","text":"<p>Processes each document library (drive) for a site: <pre><code>def process_document_library(site_id, drive_id, drive_name, all_sites):\n    data = []\n    logger.info(f\"Started Document Library -- {drive_name}\")\n    start_time = time.perf_counter()\n    site_url = f\"{AZURE_GRAPH_V1}sites/{site_id}/drives/{drive_id}/root/delta{DOCUMENT_LIB_SELECT_QUERY}\"\n    search_results = execute_odata_query_get(site_url)\n    for item in search_results:\n        entry = {\n            \"site_id\": site_id,\n            \"webUrl\": next(site[\"webUrl\"] for site in all_sites if site[\"id\"] == site_id),\n            \"drive_id\": drive_id,\n            \"document_id\": item[\"id\"],\n            \"name\": item[\"name\"],\n            \"lastModifiedDateTime\": parse_iso_date(item.get(\"lastModifiedDateTime\")),\n            \"size\": item.get(\"size\") if \"file\" in item else \"\"\n        }\n        data.append(entry)\n    write_data_to_csv(data, SP_WITHOUT_VERSION_CSV_FILE_PATH)\n    elapsed_time = time.perf_counter() - start_time\n    logger.info(f\"Document Library '{drive_name}' took {elapsed_time:.2f} seconds to process.\")\n</code></pre> - Purpose:   - Queries all files in the document library using the Graph API delta endpoint.   - Collects metadata for each file.   - Writes results to a CSV for further processing or database import.   - Logs processing time for performance monitoring.</p>"},{"location":"sharepoint-site-library-enumeration/#supporting-utilities-full-implementations","title":"Supporting Utilities (Full Implementations)","text":""},{"location":"sharepoint-site-library-enumeration/#execute_odata_query_geturl","title":"<code>execute_odata_query_get(url)</code>","text":"<p>Handles authenticated GET requests to the Microsoft Graph API, including error handling and token refresh. <pre><code>def execute_odata_query_get(url):\n    try:\n        token = get_access_token_API_Access_AAD()\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json().get(\"value\", [])\n    except Exception as e:\n        handle_global_exception(sys._getframe().f_code.co_name, e)\n        return []\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#parse_iso_datedate_str","title":"<code>parse_iso_date(date_str)</code>","text":"<p>Converts ISO 8601 date strings to Python datetime objects for easier manipulation and formatting. <pre><code>def parse_iso_date(date_str: str):\n    if not date_str:\n        return None\n    date_str = date_str.rstrip('Z')\n    formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\"]\n    for fmt in formats:\n        try:\n            return datetime.strptime(date_str, fmt).date()\n        except ValueError:\n            continue\n    return None\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#write_data_to_csvdata-file_path","title":"<code>write_data_to_csv(data, file_path)</code>","text":"<p>Appends data to a CSV file, writing headers if the file does not exist. <pre><code>def write_data_to_csv(data, file_path):\n    file_exists = os.path.isfile(file_path)\n    with open(file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n        fieldnames = [\"site_id\", \"webUrl\", \"drive_id\", \"document_id\", \"name\", \"lastModifiedDateTime\", \"size\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerows(data)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handle_global_exceptionfunctionname-exceptionobject","title":"<code>handle_global_exception(functionName, exceptionObject)</code>","text":"<p>Logs and emails details of any exception that occurs. <pre><code>def handle_global_exception(functionName, exceptionObject):\n    emailBody = f\"Function Name: {functionName}; Exception Description: {exceptionObject}\"\n    send_email(recipients=EMAIL_TO_SEND_EXCEPTIONS,\n               subject='Exception occured in code', \n               plain_message=emailBody)\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#get_access_token_api_access_aadscopesnone","title":"<code>get_access_token_API_Access_AAD(scopes=None)</code>","text":"<p>Obtains an access token for Microsoft Graph API using MSAL or Azure Identity. (Example implementation:) <pre><code>def get_access_token_API_Access_AAD(scopes=None):\n    if scopes is None:\n        scopes = ['https://graph.microsoft.com/.default']\n    app = ConfidentialClientApplication(\n        client_id=AZURE_CONFIDENTIAL_APP_ID,\n        authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n        client_credential=AZURE_CONFIDENTIAL_SECRET\n    )\n    result = app.acquire_token_for_client(scopes=scopes)\n    return result[\"access_token\"]\n</code></pre></p>"},{"location":"sharepoint-site-library-enumeration/#handling-large-sharepoint-environments","title":"Handling Large SharePoint Environments","text":"<p>Important: Large tenants with many sites, subsites, and document libraries can have tens or hundreds of thousands of files. Processing all of them can take significant time and resources.</p>"},{"location":"sharepoint-site-library-enumeration/#best-practices-for-large-document-libraries","title":"Best Practices for Large Document Libraries","text":"<ul> <li>Run in Azure: For large environments, it is highly recommended to run this inventory code in an Azure Container Instance or Azure VM. This ensures:</li> <li>Sufficient compute and memory resources.</li> <li>Proximity to Microsoft 365 services for faster API calls.</li> <li>Ability to scale or schedule the job as needed.</li> <li>Batch Processing: The code is designed to process and write data in batches, minimizing memory usage and allowing for partial progress in case of interruptions.</li> <li>Progress Notifications: The function sends email notifications after each document library and when all sites are complete, so you can monitor long-running jobs.</li> <li>Error Handling: All exceptions are logged and reported, ensuring that issues with individual sites or libraries do not halt the entire process.</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#example-end-to-end-inventory-flow","title":"Example: End-to-End Inventory Flow","text":"<ol> <li>Discover Sites:</li> <li><code>get_all_sp_sites()</code> \u2192 returns all root sites.</li> <li>Expand to Subsites:</li> <li><code>fetch_all_sites_including_subsites()</code> \u2192 returns all sites and subsites.</li> <li>Process Each Library:</li> <li>For each site, enumerate all document libraries and call <code>process_document_library()</code>.</li> <li>Write Results:</li> <li>Metadata for each file is written to a CSV file for further analysis or database import.</li> </ol>"},{"location":"sharepoint-site-library-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API: List SharePoint Sites</li> <li>Microsoft Graph API: List Drive Items</li> <li>Azure Container Instances Documentation</li> </ul>"},{"location":"sharepoint-site-library-enumeration/#conclusion","title":"Conclusion","text":"<p>The <code>get_all_files_from_sp</code> function and its supporting helpers provide a robust, scalable way to inventory all files and folders across a Microsoft 365 tenant's SharePoint environment. For large tenants, running this code in an Azure container or VM is strongly recommended to ensure reliability and performance.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/","title":"Copying Files from SharePoint to Azure File Share at Scale","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#overview","title":"Overview","text":"<p>This article provides a comprehensive, company-agnostic guide for copying large volumes of files from SharePoint Online document libraries into Azure File Shares. The solution is designed for high-throughput, scalable execution (e.g., as an Azure Container Instance), and is suitable for enterprise-scale migrations, backups, or data archiving. The approach leverages multi-threading for performance and handles large files (30\u201350 GB+) efficiently by streaming and chunked uploads.</p> <p>Key Features: - Secure, certificate-based authentication to Microsoft Graph and Azure - Multi-threaded file copy for high throughput - Chunked upload for large files - Robust error handling and progress tracking - All secrets managed via Azure Key Vault</p> <p>Note: This article assumes you have already extracted the list of files and their metadata from SharePoint. For details on how to enumerate SharePoint files and extract metadata, see Extracting SharePoint Document Library Metadata.</p>"},{"location":"sharepoint-site-library-to-azure-fileshare/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Azure File Share and connection string</li> <li>Azure AD App Registration with certificate-based authentication</li> <li>Azure Key Vault for secret management</li> <li>Extracted metadata for all SharePoint files to be copied (see stub above)</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#solution-architecture","title":"Solution Architecture","text":"<ol> <li>Metadata Extraction: Retrieve all file metadata from SharePoint (site ID, drive ID, item ID, file path, size, timestamps, etc.) and store in a database or CSV. (See stub above)</li> <li>File Copy Process: For each file, download from SharePoint using Microsoft Graph and upload to Azure File Share, preserving directory structure and metadata.</li> <li>Multi-threading: Use a thread pool to process multiple files in parallel for maximum throughput.</li> <li>Chunked Upload: For large files, stream and upload in chunks to avoid memory issues and support files up to 100s of GB.</li> <li>Progress Tracking: Log and track progress for monitoring and troubleshooting.</li> </ol>"},{"location":"sharepoint-site-library-to-azure-fileshare/#full-python-code-example","title":"Full Python Code Example","text":"<p>Below is a complete, production-ready script for the file copy process. All company-specific values have been removed. Replace stub values and secret names as appropriate for your environment.</p> <pre><code>\"\"\"\nThis script copies files from SharePoint Online to Azure File Share using multi-threading and chunked uploads.\n- Designed for high-volume, large-file scenarios (30\u201350 GB+)\n- All secrets are retrieved from Azure Key Vault\n- Can be run as an Azure Container Instance (ACI) or VM\n\"\"\"\nimport threading\nimport requests\nfrom queue import Queue\nfrom urllib.parse import unquote\nimport os\nfrom azure.storage.fileshare import ShareFileClient, ShareDirectoryClient\nfrom datetime import datetime, timedelta, timezone, time\nfrom msal import ConfidentialClientApplication\n\nfrom gdepcommon.logger import setup_logger\nfrom gdepcommon.utils import (\n    get_azure_kv_sceret,\n    sql_dbconnection,\n    PFX_CERTIFICATE_NAME,\n    PFX_CERTIFICATE_NAME_TP\n)\nfrom gdepazure.common import (\n    AZURE_CONFIDENTIAL_APP_ID,\n    AZURE_TENANT_ID,\n    AZURE_AUTHORITY_BASE_URL,\n    AZURE_GRAPH_DEFAULT_RESOURCE\n)\n\n# Thread and chunk parameters\nTHREAD_COUNT = 10  # Tune based on environment\nCHUNK_SIZE = 4 * 1024 * 1024  # 4 MB\n\n# Shared progress state\ntotal_files = 0\nprocessed_files = 0\nlock = threading.Lock()\n\n# Token management\naccess_token = None\ntoken_expiry_time = None\n\ndef refresh_access_token(logger):\n    \"\"\"Refreshes the Microsoft Graph access token using certificate-based auth.\"\"\"\n    global access_token, token_expiry_time\n    try:\n        logger.info(\"Refreshing access token...\")\n        with open(f\"certs/{PFX_CERTIFICATE_NAME}.key\", \"r\") as key_file:\n            private_key = key_file.read()\n        app = ConfidentialClientApplication(\n            client_id=AZURE_CONFIDENTIAL_APP_ID,\n            authority=f\"{AZURE_AUTHORITY_BASE_URL}{AZURE_TENANT_ID}\",\n            client_credential={\n                \"thumbprint\": PFX_CERTIFICATE_NAME_TP,\n                \"private_key\": private_key,\n            },\n        )\n        result = app.acquire_token_for_client(scopes=AZURE_GRAPH_DEFAULT_RESOURCE)\n        access_token = result[\"access_token\"]\n        expires_in = result[\"expires_in\"]\n        token_expiry_time = datetime.now(tz=timezone.utc) + timedelta(seconds=expires_in - 60)\n        logger.info(\"Access token refreshed successfully.\")\n    except Exception as e:\n        logger.error(f\"Failed to refresh access token: {e}\")\n        raise Exception(\"Access token refresh failed.\")\n\ndef get_access_token(logger):\n    \"\"\"Returns a valid access token, refreshing if expired.\"\"\"\n    global access_token, token_expiry_time\n    if not access_token or datetime.now(tz=timezone.utc) &gt;= token_expiry_time:\n        for attempt in range(3):\n            try:\n                refresh_access_token(logger)\n                break\n            except Exception as e:\n                if attempt &lt; 2:\n                    logger.warning(f\"Retrying token refresh (attempt {attempt + 1}/3)...\")\n                else:\n                    raise e\n    return access_token\n\ndef ensure_directory_path_exists(azure_conn_str, share_name, directory_path, cache=None):\n    \"\"\"Ensures the full directory path exists in Azure File Share.\"\"\"\n    if cache is None:\n        cache = set()\n    parts = directory_path.strip('/').split('/')\n    current_path = ''\n    for part in parts:\n        current_path = f\"{current_path}/{part}\" if current_path else part\n        if current_path in cache:\n            continue\n        dir_client = ShareDirectoryClient.from_connection_string(\n            conn_str=azure_conn_str,\n            share_name=share_name,\n            directory_path=current_path\n        )\n        try:\n            dir_client.create_directory()\n            cache.add(current_path)\n        except Exception as ex:\n            if \"ResourceAlreadyExists\" in str(ex):\n                cache.add(current_path)\n            else:\n                raise\n\ndef copy_file_from_sp_to_azure(site_id, drive_id, item_id, azure_file_client, total_file_size_in_bytes, created_date=None, modified_date=None, logger=None):\n    \"\"\"Streams a file from SharePoint and uploads to Azure File Share in chunks.\"\"\"\n    access_token = get_access_token(logger)\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/items/{item_id}/content\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    retries = 3\n    for attempt in range(retries):\n        try:\n            logger.info(f\"Starting file copy for item_id: {item_id}\")\n            with requests.get(url, headers=headers, stream=True) as response:\n                response.raise_for_status()\n                azure_file_client.create_file(size=total_file_size_in_bytes)\n                offset = 0\n                for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n                    azure_file_client.upload_range(data=chunk, offset=offset, length=len(chunk))\n                    offset += len(chunk)\n                    # Log progress for large files (&gt;5GB) every 10%\n                    if total_file_size_in_bytes &gt; 5 * 1024 * 1024 * 1024:\n                        percent_complete = (offset / total_file_size_in_bytes) * 100\n                        if int(offset / CHUNK_SIZE) % int((total_file_size_in_bytes / CHUNK_SIZE) * 0.05) == 0:\n                            logger.info(f\"File {item_id}: {percent_complete:.2f}% complete\")\n            # Set file properties for created/modified dates\n            if created_date or modified_date:\n                file_properties = {}\n                if created_date:\n                    created_datetime = datetime.combine(created_date, time())\n                    file_properties['file_creation_time'] = created_datetime\n                if modified_date:\n                    modified_datetime = datetime.combine(modified_date, time())\n                    file_properties['file_last_write_time'] = modified_datetime\n                from azure.storage.fileshare import ContentSettings\n                content_settings = ContentSettings(content_type=\"application/octet-stream\")\n                azure_file_client.set_http_headers(file_attributes=\"none\", content_settings=content_settings, **file_properties)\n            logger.info(f\"Successfully copied file for item_id: {item_id}\")\n            return\n        except requests.exceptions.RequestException as e:\n            if attempt &lt; retries - 1:\n                logger.warning(f\"Retrying file copy for item_id {item_id} (attempt {attempt + 1}/{retries})...\")\n            else:\n                logger.error(f\"Failed to copy file for item_id: {item_id}. Error: {e}\")\n                raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during file copy for item_id {item_id}: {e}\")\n\ndef worker(queue, azure_conn_str, share_name, created_dirs, logger):\n    \"\"\"Thread worker function: processes files from the queue.\"\"\"\n    global processed_files\n    while not queue.empty():\n        try:\n            file_record = queue.get()\n            site_id = file_record['site_id']\n            drive_id = file_record['drive_id']\n            item_id = file_record['item_id']\n            total_file_size_in_bytes = file_record['length']\n            created_date = file_record['created_date']\n            modified_date = file_record['modified_date']\n            decoded_path = file_record['decoded_path']\n            # Ensure directory exists\n            azure_directory_path = os.path.dirname(decoded_path)\n            if azure_directory_path:\n                ensure_directory_path_exists(azure_conn_str, share_name, azure_directory_path, created_dirs)\n            file_client = ShareFileClient.from_connection_string(\n                conn_str=azure_conn_str,\n                share_name=share_name,\n                file_path=decoded_path\n            )\n            copy_file_from_sp_to_azure(site_id, drive_id, item_id, file_client, total_file_size_in_bytes, created_date, modified_date, logger)\n            # Mark as copied in DB\n            with sql_dbconnection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"UPDATE [dbo].[Fact_Document_Library_Details] SET [copied] = 1 WHERE [unique_id] = ?\", file_record['unique_id'])\n                conn.commit()\n            with lock:\n                global processed_files\n                processed_files += 1\n                overall_progress = (processed_files / total_files) * 100\n                logger.info(f\"Overall Progress: {overall_progress:.2f}% ({processed_files}/{total_files} files complete)\")\n            queue.task_done()\n        except Exception as e:\n            logger.error(f\"Error processing file: {e}\")\n\ndef main(logger):\n    \"\"\"\n    Main entry point: loads file metadata, initializes threads, and starts the copy process.\n    - Loads Azure File Share connection string from Key Vault\n    - Loads file metadata (site_id, drive_id, item_id, file path, size, timestamps, etc.)\n    - Spawns worker threads to process the file queue\n    - Tracks and logs progress\n    \"\"\"\n    global total_files\n    try:\n        azure_conn_str = get_azure_kv_sceret('your-azure-file-connection-string-secret')\n        share_name = \"your-azure-file-share-name\"\n        site_id = \"your-sharepoint-site-id\"\n        drive_id = \"your-sharepoint-drive-id\"\n        created_dirs = set()\n        # Fetch files to copy (replace with your DB or CSV logic)\n        with sql_dbconnection() as sqlConnection:\n            cursor = sqlConnection.cursor()\n            cursor.execute(\"SELECT * FROM [dbo].[Fact_Document_Library_Details] WHERE [type] = 'file' AND [copied] = 0\")\n            results = cursor.fetchall()\n        total_files = len(results)\n        if total_files == 0:\n            logger.info(\"No files to process.\")\n            return\n        queue = Queue()\n        for row in results:\n            sp_relative_url = row.server_relative_url\n            decoded_path = unquote(sp_relative_url[len(\"/sites/YourSite/YourLibrary\"):].lstrip('/'))\n            queue.put({\n                'site_id': site_id,\n                'drive_id': drive_id,\n                'item_id': row.unique_id,\n                'unique_id': row.unique_id,\n                'length': row.length,\n                'created_date': row.time_created,\n                'modified_date': row.time_last_modified,\n                'decoded_path': decoded_path\n            })\n        threads = []\n        for _ in range(THREAD_COUNT):\n            thread = threading.Thread(target=worker, args=(queue, azure_conn_str, share_name, created_dirs, logger))\n            thread.start()\n            threads.append(thread)\n        for thread in threads:\n            thread.join()\n        logger.info(\"All files have been processed successfully.\")\n    except Exception as e:\n        logger.error(f\"Error in main function: {e}\")\n\nif __name__ == \"__main__\":\n    logger = setup_logger(\"sp2azfileshare\", \"/mnt/azure/logs/sp2azfileshare.log\")\n    main(logger)\n</code></pre>"},{"location":"sharepoint-site-library-to-azure-fileshare/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"sharepoint-site-library-to-azure-fileshare/#1-authentication-and-secret-management","title":"1. Authentication and Secret Management","text":"<ul> <li>All secrets (Azure connection string, certificate thumbprint, etc.) are retrieved from Azure Key Vault using a utility function (<code>get_azure_kv_sceret</code>).</li> <li>Microsoft Graph authentication uses certificate-based credentials for security and automation.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#2-multi-threaded-file-copy","title":"2. Multi-Threaded File Copy","text":"<ul> <li>The script uses a thread pool (<code>THREAD_COUNT</code>) and a <code>Queue</code> to distribute file copy tasks across multiple threads.</li> <li>Each thread processes files independently, ensuring high throughput and efficient use of resources.</li> <li>Thread-safe progress tracking is implemented using a <code>Lock</code>.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#3-chunked-upload-for-large-files","title":"3. Chunked Upload for Large Files","text":"<ul> <li>Files are streamed from SharePoint and uploaded to Azure File Share in 4 MB chunks.</li> <li>This approach supports very large files (30\u201350 GB+) without excessive memory usage.</li> <li>Progress for large files is logged every 10% (configurable).</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#4-directory-structure-and-metadata-preservation","title":"4. Directory Structure and Metadata Preservation","text":"<ul> <li>The script ensures that the full directory path exists in Azure File Share before uploading each file.</li> <li>File creation and modification timestamps are preserved if available.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#5-database-integration-and-idempotency","title":"5. Database Integration and Idempotency","text":"<ul> <li>The script marks each file as copied in the database after successful upload, ensuring idempotency and resumability.</li> <li>You can adapt this logic to use a CSV or other metadata store as needed.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#scaling-and-running-in-azure","title":"Scaling and Running in Azure","text":"<ul> <li>This script is designed to run as an Azure Container Instance (ACI), but can also be run on VMs or Kubernetes.</li> <li>Tune <code>THREAD_COUNT</code> based on available CPU and network bandwidth.</li> <li>For very large migrations, consider splitting the workload across multiple containers or jobs.</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#related-articles","title":"Related Articles","text":"<ul> <li>Extracting SharePoint Document Library Metadata for Automation</li> <li>Automating Secure Secret Management with Azure Key Vault</li> </ul>"},{"location":"sharepoint-site-library-to-azure-fileshare/#conclusion","title":"Conclusion","text":"<p>By following this guide and using the provided code, you can efficiently and securely copy massive volumes of files from SharePoint Online to Azure File Share, with full support for large files, multi-threaded performance, and robust error handling. All sensitive information is managed via Azure Key Vault, ensuring compliance and security for enterprise automation scenarios.</p>"},{"location":"sharepoint-sites-enumeration/","title":"How to Retrieve All SharePoint Sites in Your Microsoft 365 Tenant","text":""},{"location":"sharepoint-sites-enumeration/#introduction","title":"Introduction","text":"<p>Retrieving a complete list of SharePoint sites in your Microsoft 365 (M365) tenant is essential for IT automation, reporting, and governance. This article provides a detailed, company-agnostic, step-by-step guide to programmatically enumerate all SharePoint sites using Python and the Microsoft Graph API. All code samples are generic and ready to use in any tenant.</p>"},{"location":"sharepoint-sites-enumeration/#prerequisites","title":"Prerequisites","text":""},{"location":"sharepoint-sites-enumeration/#1-azure-entra-application-registration","title":"1. Azure Entra Application Registration","text":"<ul> <li>Register an application in Azure Entra (Azure AD).</li> <li>Assign the following Microsoft Graph API permissions:</li> <li><code>Sites.Read.All</code> (Application permission)</li> <li><code>Sites.ReadWrite.All</code> (if you need to write/update)</li> <li>Grant admin consent for these permissions.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#2-certificate-based-authentication","title":"2. Certificate-Based Authentication","text":"<ul> <li>Upload a certificate to your Azure Entra application.</li> <li>Use the certificate thumbprint and private key for authentication.</li> <li>For a detailed guide and code on certificate-based authentication, see: Certificate Auth for Microsoft Graph API</li> </ul>"},{"location":"sharepoint-sites-enumeration/#3-python-environment","title":"3. Python Environment","text":"<ul> <li>Install the required packages:   <pre><code>pip install requests msal\n</code></pre></li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-1-authenticate-and-get-an-access-token","title":"Step 1: Authenticate and Get an Access Token","text":"<p>You need to authenticate as your Azure Entra application and obtain an access token for Microsoft Graph. This is best done using certificate-based authentication for security.</p> <p>Below is a full, reusable function for certificate-based authentication. (Replace the placeholders with your actual values.)</p> <pre><code>import msal\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    # Replace these with your app's values\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n</code></pre> <p>See this blog post for a full explanation and troubleshooting tips for certificate-based authentication.</p>"},{"location":"sharepoint-sites-enumeration/#step-2-query-the-microsoft-graph-api-for-sharepoint-sites","title":"Step 2: Query the Microsoft Graph API for SharePoint Sites","text":"<p>The Microsoft Graph API endpoint to list all sites is:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites?search=*\n</code></pre> <p>This returns a paginated list of root SharePoint sites in your tenant.</p>"},{"location":"sharepoint-sites-enumeration/#helper-function-execute-odata-query","title":"Helper Function: Execute OData Query","text":"<pre><code>import requests\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#retrieve-all-sites-with-pagination","title":"Retrieve All Sites (with Pagination)","text":"<pre><code>def get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation","title":"Explanation:","text":"<ul> <li><code>get_all_sp_sites</code> starts with the root search URL.</li> <li>It uses the access token for authentication.</li> <li>It loops through all pages using the <code>@odata.nextLink</code> property for pagination.</li> <li>All sites are collected in the <code>sites</code> list.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-3-retrieve-subsites-for-each-site","title":"Step 3: Retrieve Subsites for Each Site","text":"<p>To enumerate subsites for a given site, use:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{site-id}/sites\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#function-to-get-subsites","title":"Function to Get Subsites","text":"<pre><code>def get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#explanation_1","title":"Explanation:","text":"<ul> <li>For each site, call <code>get_sp_subsites(site_id)</code> to get its direct subsites.</li> <li>You can recursively call this function to build a full site tree.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#step-4-full-example-enumerate-all-sites-and-subsites","title":"Step 4: Full Example - Enumerate All Sites and Subsites","text":"<p>Here is a complete script you can copy, edit, and run in your own environment:</p> <pre><code>import msal\nimport requests\nimport json\nimport os\n\ndef get_access_token_API_Access_AAD(resource_list=None):\n    TENANT_ID = \"&lt;YOUR_TENANT_ID&gt;\"\n    CLIENT_ID = \"&lt;YOUR_CLIENT_ID&gt;\"\n    AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n    CERT_THUMBPRINT = \"&lt;YOUR_CERT_THUMBPRINT&gt;\"\n    CERT_PRIVATE_KEY_PATH = \"&lt;PATH_TO_YOUR_PRIVATE_KEY&gt;.pem\"\n    if resource_list is None:\n        resource_list = [\"https://graph.microsoft.com/.default\"]\n    with open(CERT_PRIVATE_KEY_PATH, \"r\") as f:\n        private_key = f.read()\n    app = msal.ConfidentialClientApplication(\n        client_id=CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential={\n            \"thumbprint\": CERT_THUMBPRINT,\n            \"private_key\": private_key\n        }\n    )\n    result = app.acquire_token_for_client(scopes=resource_list)\n    if \"access_token\" in result:\n        return result[\"access_token\"]\n    else:\n        raise Exception(f\"Could not obtain access token: {result}\")\n\ndef execute_odata_query_get(url, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_all_sp_sites():\n    url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    sites = []\n    next_url = url\n    while next_url:\n        data = execute_odata_query_get(next_url, token)\n        sites.extend(data.get(\"value\", []))\n        next_url = data.get(\"@odata.nextLink\")\n    return sites\n\ndef get_sp_subsites(site_id):\n    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/sites\"\n    token = get_access_token_API_Access_AAD([\"https://graph.microsoft.com/.default\"])\n    data = execute_odata_query_get(url, token)\n    return data.get(\"value\", [])\n\ndef enumerate_all_sites_and_subsites():\n    all_sites = get_all_sp_sites()\n    all_sites_with_subsites = []\n    for site in all_sites:\n        site_id = site['id']\n        subsites = get_sp_subsites(site_id)\n        site['subsites'] = subsites\n        all_sites_with_subsites.append(site)\n    return all_sites_with_subsites\n\nif __name__ == \"__main__\":\n    all_sites = enumerate_all_sites_and_subsites()\n    print(json.dumps(all_sites, indent=2))\n</code></pre>"},{"location":"sharepoint-sites-enumeration/#step-by-step-code-walkthrough","title":"Step-by-Step Code Walkthrough","text":"<ol> <li>get_access_token_API_Access_AAD: Authenticates using your Azure Entra app and certificate, returning a valid access token for Microsoft Graph.</li> <li>execute_odata_query_get: Sends a GET request to the specified Microsoft Graph endpoint using the access token, returning the parsed JSON response.</li> <li>get_all_sp_sites: Uses the <code>/sites?search=*</code> endpoint to retrieve all root SharePoint sites, handling pagination.</li> <li>get_sp_subsites: For each site, retrieves its direct subsites.</li> <li>enumerate_all_sites_and_subsites: Combines the above to build a list of all sites and their subsites.</li> <li>Main block: Runs the enumeration and prints the result as formatted JSON.</li> </ol>"},{"location":"sharepoint-sites-enumeration/#required-permissions-recap","title":"Required Permissions Recap","text":"<ul> <li><code>Sites.Read.All</code> (Application permission, admin consent required)</li> <li>The Azure Entra app must be granted consent by a tenant admin</li> <li>The app must authenticate using a certificate or secret (certificate recommended)</li> </ul>"},{"location":"sharepoint-sites-enumeration/#troubleshooting-and-tips","title":"Troubleshooting and Tips","text":"<ul> <li>If you get a 403 error, check that your app registration has admin consent for <code>Sites.Read.All</code>.</li> <li>If you get a 401 error, check your certificate and app credentials.</li> <li>The <code>search=*</code> parameter is required to enumerate all sites, not just the root site.</li> <li>For large tenants, always handle pagination using <code>@odata.nextLink</code>.</li> <li>You can extend the code to recursively enumerate subsites to any depth.</li> </ul>"},{"location":"sharepoint-sites-enumeration/#references","title":"References","text":"<ul> <li>Microsoft Graph API - List sites</li> <li>Microsoft Graph API - List subsites</li> <li>Microsoft Graph permissions reference</li> <li>Register an application with the Microsoft identity platform</li> <li>Certificate credentials for application authentication</li> <li>MSAL for Python documentation</li> <li>Microsoft Graph Explorer</li> </ul>"},{"location":"sharepoint-sites-enumeration/#summary","title":"Summary","text":"<ul> <li>Register an Azure Entra application and grant it <code>Sites.Read.All</code> permission</li> <li>Authenticate using a certificate (see this blog post)</li> <li>Use the Microsoft Graph API <code>/sites?search=*</code> endpoint to enumerate all SharePoint sites</li> <li>Use <code>/sites/{site-id}/sites</code> to enumerate subsites</li> <li>Handle pagination using <code>@odata.nextLink</code></li> </ul> <p>This approach is secure, scalable, and works in any Microsoft 365 tenant. You can now automate SharePoint site inventory, reporting, or governance tasks in your own environment.</p>"},{"location":"teams-user-number-assignment/","title":"Retrieving Teams Phone Number Assignments","text":""},{"location":"teams-user-number-assignment/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to programmatically retrieve Microsoft Teams phone number assignments for users in Entra (Azure AD), combining Python and PowerShell. The approach is modular, production-ready, and company-agnostic. All code is explained step by step, with constants and endpoints included for clarity.</p>"},{"location":"teams-user-number-assignment/#why-use-powershell-for-teams-phone-assignments","title":"Why Use PowerShell for Teams Phone Assignments?","text":"<p>Some Microsoft Teams telephony data, such as direct phone number assignments, is not always available via the Microsoft Graph API or may require additional permissions and modules. PowerShell modules (such as <code>MicrosoftTeams</code> or <code>SkypeOnlineConnector</code>) provide richer access to Teams telephony configuration and are often used in enterprise automation for this purpose.</p>"},{"location":"teams-user-number-assignment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>The following Python packages:</li> <li><code>subprocess</code> (standard library)</li> <li><code>requests</code></li> <li>PowerShell installed on the system (with the required Teams modules)</li> <li>An Azure AD application (service principal) with permissions to read user information</li> <li>Secure storage for credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"teams-user-number-assignment/#step-1-retrieve-teams-phone-number-assignments","title":"Step 1: Retrieve Teams Phone Number Assignments","text":""},{"location":"teams-user-number-assignment/#function-get_teams_phone_number_assignments","title":"Function: <code>get_teams_phone_number_assignments</code>","text":"<p>This function combines user data from Entra (Azure AD) with Teams phone number assignments, which are typically retrieved via a PowerShell script/module.</p> <pre><code>def get_teams_phone_number_assignments(aad_users):\n    try:\n        user_dict = {user['id']: user for user in aad_users}  # Dictionary for faster lookups\n        phone_assignments = get_teams_phone_numbers()  # Calls PowerShell to get assignments\n        teams_phone_assignments = []\n        for assignment in phone_assignments:\n            user_id = assignment['AssignedPstnTargetId']\n            if user_id in user_dict:\n                formatted_assignment = {\n                    'assigned_to_entra_account': 'yes',\n                    'entra_user': user_dict[user_id].get('displayName','') or '',\n                    'teams_telephoneNumber': assignment.get('TelephoneNumber','') or '',\n                    'entra_businessPhones': user_dict[user_id].get('businessPhones','') or '',\n                    'entra_mobilePhone': user_dict[user_id].get('mobilePhone','') or '',\n                    'entra_accountEnabled': user_dict[user_id].get('accountEnabled','') or '',\n                    'entra_employeeType': user_dict[user_id].get('employeeType','') or '',\n                    'entra_city': user_dict[user_id].get('city','') or '',\n                    'entra_officeLocation': user_dict[user_id].get('officeLocation','') or '',\n                    'teams_assignmentCategory': assignment.get('AssignmentCategory','') or '',\n                    'teams_city': assignment.get('City','') or '',\n                    'teams_isosubdivision': assignment.get('IsoSubdivision','') or '',\n                    'teams_numbertype': assignment.get('NumberType','') or '',\n                    'teams_pstnassignmentstatus': assignment.get('PstnAssignmentStatus','') or '',\n                }\n            else:\n                formatted_assignment = {\n                    'assigned_to_entra_account': 'no',\n                    'entra_user': '',\n                    'teams_telephoneNumber': assignment.get('TelephoneNumber','') or '',\n                    'entra_businessPhones': '',\n                    'entra_mobilePhone': '',\n                    'entra_accountEnabled': '',\n                    'entra_employeeType': '',\n                    'entra_city': '',\n                    'entra_officeLocation': '',\n                    'teams_assignmentCategory': assignment.get('AssignmentCategory','') or '',\n                    'teams_city': assignment.get('City','') or '',\n                    'teams_isosubdivision': assignment.get('IsoSubdivision','') or '',\n                    'teams_numbertype': assignment.get('NumberType','') or '',\n                    'teams_pstnassignmentstatus': assignment.get('PstnAssignmentStatus','') or '',\n                }\n            teams_phone_assignments.append(formatted_assignment)\n        return teams_phone_assignments\n    except Exception as error:\n        handle_global_exception(sys._getframe().f_code.co_name, error)\n</code></pre> <p>Explanation: - Builds a dictionary of users for fast lookup. - Calls <code>get_teams_phone_numbers</code> to retrieve phone assignments (see next step). - Combines user and phone assignment data into a unified list for reporting or export.</p>"},{"location":"teams-user-number-assignment/#step-2-retrieve-teams-phone-numbers-via-powershell","title":"Step 2: Retrieve Teams Phone Numbers via PowerShell","text":""},{"location":"teams-user-number-assignment/#function-get_teams_phone_numbers","title":"Function: <code>get_teams_phone_numbers</code>","text":"<p>This function (not shown in full here) typically uses Python's <code>subprocess</code> module to invoke a PowerShell script or command that retrieves Teams phone number assignments. The PowerShell script should output data in a format that Python can parse (e.g., JSON or CSV).</p> <p>Example (conceptual):</p> <pre><code>import subprocess\nimport json\n\ndef get_teams_phone_numbers():\n    # Example PowerShell command to get Teams phone assignments as JSON\n    ps_command = [\n        'pwsh', '-Command',\n        'Import-Module MicrosoftTeams; Get-CsPhoneNumberAssignment | ConvertTo-Json'\n    ]\n    result = subprocess.run(ps_command, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"PowerShell error: {result.stderr}\")\n    return json.loads(result.stdout)\n</code></pre> <p>Explanation: - Uses PowerShell to access Teams telephony data not available via Graph API. - Returns a list of phone assignment dictionaries for further processing in Python.</p>"},{"location":"teams-user-number-assignment/#step-3-export-or-store-the-results","title":"Step 3: Export or Store the Results","text":"<p>After combining the data, you can export the results to CSV or store them in a database for reporting or compliance purposes.</p>"},{"location":"teams-user-number-assignment/#conclusion","title":"Conclusion","text":"<p>By following this approach, you can programmatically retrieve and correlate Teams phone number assignments with Entra (Azure AD) user data, using a combination of Python and PowerShell. This enables automated reporting, compliance, and inventory workflows for Teams telephony in your organization.</p> <p>For more details, see the Microsoft Teams PowerShell documentation and Microsoft Graph API documentation.</p>"},{"location":"ukg-api-employee-verification/","title":"Validating Employee Data Consistency Between UKG and Your HR System","text":""},{"location":"ukg-api-employee-verification/#introduction","title":"Introduction","text":"<p>When integrating HR data between systems such as ADP (or any HRIS) and UKG Dimensions, it's critical to ensure that the data loaded into UKG matches the source-of-truth in your HR system. This is especially important when using middleware (like Dell Boomi) for automated data loads. This article provides a step-by-step, company-agnostic guide to:</p> <ul> <li>Connect securely to the UKG API</li> <li>Retrieve employee data from UKG</li> <li>Retrieve employee data from your HR system (e.g., ADP)</li> <li>Compare the two datasets for validation</li> <li>Report discrepancies for remediation</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment.</p>"},{"location":"ukg-api-employee-verification/#why-validate-data-after-integration","title":"Why Validate Data After Integration?","text":"<p>Automated integrations (e.g., via Dell Boomi) can occasionally result in mismatches due to mapping errors, transformation issues, or upstream data changes. Validating data post-load ensures:</p> <ul> <li>Data integrity between systems</li> <li>Early detection of integration or mapping issues</li> <li>Compliance with audit requirements</li> <li>Improved trust in downstream business processes</li> </ul>"},{"location":"ukg-api-employee-verification/#solution-overview","title":"Solution Overview","text":"<ol> <li>Connect to UKG API: Use secure credentials (ideally from Azure Key Vault or similar) to authenticate and retrieve employee data from UKG.</li> <li>Connect to HR System: Query your HR system (e.g., ADP) for the same set of employees.</li> <li>Compare Data: Match employees by a unique identifier (e.g., email or employee ID) and compare key fields.</li> <li>Report Results: Output a report of discrepancies for review and correction.</li> </ol>"},{"location":"ukg-api-employee-verification/#step-1-securely-connect-to-the-ukg-api","title":"Step 1: Securely Connect to the UKG API","text":"<p>UKG Dimensions provides a REST API for programmatic access. Authentication typically uses OAuth2 with client credentials. Credentials should be stored securely (e.g., Azure Key Vault).</p> <pre><code>import requests\nimport urllib.parse\nfrom your_utils_module import get_azure_kv_sceret  # Replace with your actual secret retrieval function\n\ndef get_ukg_environment_credentials(environment):\n    if environment == 'PROD':\n        return (\n            get_azure_kv_sceret('ukg-base-uri'),\n            get_azure_kv_sceret('ukg-api-username'),\n            get_azure_kv_sceret('ukg-api-password'),\n            get_azure_kv_sceret('ukg-api-key'),\n            get_azure_kv_sceret('ukg-api-client-id'),\n            get_azure_kv_sceret('ukg-api-client-secret')\n        )\n    else:\n        # Use your non-production secrets\n        ...\n\ndef get_token_apikey_and_uri(environment):\n    base_uri, username, password, api_key, client_id, client_secret = get_ukg_environment_credentials(environment)\n    access_token_uri = base_uri + 'api/authentication/access_token'\n    payload = (\n        f\"username={urllib.parse.quote(username)}&amp;\"\n        f\"password={urllib.parse.quote(password)}&amp;\"\n        f\"client_id={urllib.parse.quote(client_id)}&amp;\"\n        f\"client_secret={urllib.parse.quote(client_secret)}&amp;\"\n        \"grant_type=password&amp;auth_chain=OAuthLdapService\"\n    )\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'appkey': api_key\n    }\n    response = requests.post(access_token_uri, headers=headers, data=payload)\n    response.raise_for_status()\n    response_json = response.json()\n    return response_json[\"access_token\"], base_uri, api_key\n</code></pre> <p>Explanation: - Credentials are retrieved securely. - The function requests an OAuth2 access token from UKG. - The token is used for all subsequent API calls.</p>"},{"location":"ukg-api-employee-verification/#step-2-retrieve-employee-data-from-ukg","title":"Step 2: Retrieve Employee Data from UKG","text":"<p>Once authenticated, you can call the UKG API to retrieve employee details. The following function demonstrates how to fetch all employees and their details:</p> <pre><code>import requests\n\ndef get_all_employees(environment):\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    employees = []\n    # Example endpoint for listing employees (adjust as needed for your UKG tenant)\n    url = base_uri + 'api/v1/commons/persons'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    employees = response.json().get('persons', [])\n    return employees\n</code></pre> <p>Explanation: - Calls the UKG API endpoint to list all employees. - Returns a list of employee records as dictionaries.</p>"},{"location":"ukg-api-employee-verification/#step-3-retrieve-employee-data-from-your-hr-system-eg-adp","title":"Step 3: Retrieve Employee Data from Your HR System (e.g., ADP)","text":"<p>Assuming you have a database or API access to your HR system, you can retrieve employee data for comparison. Here is a sample function for fetching from a SQL database:</p> <pre><code>def fetch_users_from_hr():\n    sql_query = \"\"\"\n        SELECT email, position_id, status, associate_id, last_name, first_name, location, pay_rate_code, reports_to\n        FROM hr_employees\n    \"\"\"\n    # Replace with your actual DB query logic\n    results = execute_Select_SQL_statement(sql_query)[0]\n    return {\n        row[0].lower(): {\n            \"position_id\": row[1],\n            \"status\": row[2],\n            \"associate_id\": row[3],\n            \"last_name\": row[4],\n            \"first_name\": row[5],\n            \"location\": row[6],\n            \"pay_rate_code\": row[7],\n            \"reports_to\": row[8]\n        } for row in results\n    }\n</code></pre> <p>Explanation: - Queries the HR system for employee data. - Returns a dictionary keyed by email for easy lookup.</p>"},{"location":"ukg-api-employee-verification/#step-4-compare-and-validate-employee-data","title":"Step 4: Compare and Validate Employee Data","text":"<p>Now, compare the two datasets and report any discrepancies. Here is a function that does this and saves the results to a CSV file:</p> <pre><code>def compare_ukg_and_hr_employees(ukg_employees, hr_employees):\n    processed_details = []\n    for employee in ukg_employees:\n        username = employee.get('user', {}).get('userAccount', {}).get('userName', None)\n        if username and username.lower() in hr_employees:\n            hr_user = hr_employees[username.lower()]\n            # Compare fields as needed\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': hr_user.get('status'),\n                # Add more fields as needed\n            })\n        else:\n            processed_details.append({\n                'ukg_username': username,\n                'ukg_status': employee.get('status'),\n                'hr_status': 'Not Found',\n            })\n    # Save to CSV for review\n    save_list_to_csv(processed_details, 'ukg_hr_comparison.csv')\n</code></pre> <p>Explanation: - For each UKG employee, attempts to find a match in the HR system by email/username. - Compares relevant fields and records the results. - Outputs a CSV file for review.</p>"},{"location":"ukg-api-employee-verification/#step-5-full-example-putting-it-all-together","title":"Step 5: Full Example \u2013 Putting It All Together","text":"<p>Here is a complete example that ties all the steps together:</p> <pre><code>def main(environment='PROD'):\n    ukg_employees = get_all_employees(environment)\n    hr_employees = fetch_users_from_hr()\n    compare_ukg_and_hr_employees(ukg_employees, hr_employees)\n    print(\"Comparison complete. Results saved to ukg_hr_comparison.csv.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-api-employee-verification/#benefits-of-post-load-validation","title":"Benefits of Post-Load Validation","text":"<ul> <li>Data Quality: Ensures that the data loaded into UKG matches your HR system.</li> <li>Early Issue Detection: Quickly identifies mapping or integration errors.</li> <li>Audit Readiness: Provides evidence of data integrity for compliance.</li> <li>Continuous Improvement: Enables ongoing monitoring and process improvement.</li> </ul>"},{"location":"ukg-api-employee-verification/#conclusion","title":"Conclusion","text":"<p>By following this guide, you can automate the validation of employee data between UKG and your HR system, regardless of your integration platform. This approach is scalable, secure, and adaptable to any enterprise environment.</p> <p>For further enhancements, consider automating the process to run after each integration cycle and integrating with your alerting or ticketing system for proactive remediation.</p>"},{"location":"ukg-api-integration-and-dataview/","title":"Automating UKG Dimensions Integrations and DataView Exports with Python","text":""},{"location":"ukg-api-integration-and-dataview/#introduction","title":"Introduction","text":"<p>UKG Dimensions (formerly Kronos) provides powerful APIs for automating data extraction and integration tasks. This article demonstrates how to:</p> <ul> <li>Programmatically execute a predefined integration (such as a payroll export) in UKG Dimensions and retrieve the output file.</li> <li>Programmatically extract data from an existing DataView using a Hyperfind query.</li> </ul> <p>All code is provided in Python, and the approach is suitable for any enterprise environment. This guide is company-agnostic and can be adapted to your own UKG tenant.</p>"},{"location":"ukg-api-integration-and-dataview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to UKG Dimensions APIs (with appropriate permissions)</li> <li>A predefined integration (e.g., Payroll Export) already set up in your UKG tenant</li> <li>An existing DataView in UKG Dimensions</li> <li>Python 3.8+ and the <code>requests</code> library</li> <li>Secure storage for API credentials (e.g., Azure Key Vault)</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#1-executing-a-predefined-integration-in-ukg-dimensions","title":"1. Executing a Predefined Integration in UKG Dimensions","text":"<p>UKG Dimensions allows you to define integrations (such as payroll exports) via the UI. These integrations can be triggered and monitored via API, and the resulting files can be downloaded programmatically.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_payroll_hours","title":"Python Function: <code>fetch_and_store_payroll_hours</code>","text":"<pre><code>def fetch_and_store_payroll_hours(environment, week_start, week_end, week_start_datetime, week_end_datetime):\n    import uuid, json, time, csv\n    from io import StringIO\n    # ... import your secret and DB utilities ...\n    unique_id = 'Automation-' + str(uuid.uuid4())\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    # 1. Trigger the integration\n    dimensions_api_uri = base_uri + 'api/v1/platform/integrations/4/execute'\n    payload = json.dumps({\n        \"integrationParameters\": [\n            {\"name\": \"Symbolic Period\", \"value\": {'symbolicPeriod': {'id': '0'}, 'startDate': week_start + '.000Z', 'endDate': week_end + '.000Z'}},\n            {\"name\": \"Summary File Name\", \"value\": \"AutomationPayrollSummaryExport.csv\"},\n            {\"name\": \"Hyperfind ID\", \"value\": {'hyperfind': {'id': '1304'}}},\n            {\"name\": \"Ignore Sign Off\", \"value\": False},\n            {\"name\": \"File Name\", \"value\": \"automationpayrollexport.csv\"}\n        ],\n        \"name\": unique_id\n    })\n    response = requests.post(dimensions_api_uri, headers=headers, data=payload).json()\n    # 2. Poll for completion\n    execution_id = response['id']\n    status_url = base_uri + f'api/v1/platform/integration_executions/{execution_id}'\n    while True:\n        status_response = requests.get(status_url, headers=headers).json()\n        if status_response['status'] == 'Completed':\n            break\n        time.sleep(60)  # Wait before polling again\n    # 3. Download the output file\n    file_url = status_url + '/file'\n    params = {'file_name': 'automationpayrollexport.csv'}\n    file_response = requests.get(file_url, headers=headers, params=params)\n    data_file = StringIO(file_response.text)\n    csv_reader = csv.DictReader(data_file)\n    data_list = list(csv_reader)\n    # ... process and store data as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation","title":"Explanation","text":"<ul> <li>Trigger Integration: The function sends a POST request to the integration execution endpoint, passing required parameters (dates, file names, hyperfind, etc.).</li> <li>Poll for Completion: The function polls the execution status endpoint until the integration is complete.</li> <li>Download Output: Once complete, the output file is downloaded and parsed as CSV.</li> <li>Processing: The data can then be processed or loaded into a database as needed.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 Integrations</p>"},{"location":"ukg-api-integration-and-dataview/#2-extracting-data-from-a-dataview-using-a-hyperfind-query","title":"2. Extracting Data from a DataView Using a Hyperfind Query","text":"<p>DataViews in UKG Dimensions allow you to define custom reports. You can extract data from a DataView programmatically using the API and a Hyperfind query to filter employees.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-fetch_and_store_hours_using_dataview","title":"Python Function: <code>fetch_and_store_hours_using_dataview</code>","text":"<pre><code>def fetch_and_store_hours_using_dataview(environment, week_start_datetime, week_end_datetime):\n    import json, time, csv, os\n    # ... import your secret and DB utilities ...\n    pay_code_translation = { 'Regular': 'REG', 'Overtime 1.5': 'OT', 'Doubletime': 'DBL', ... }\n    access_token, base_uri, api_key = get_token_apikey_and_uri(environment)\n    export_url = base_uri + 'api/v1/commons/exports/async'\n    headers = {\n        'Content-Type': 'application/json',\n        'appkey': api_key,\n        'Authorization': access_token\n    }\n    payload = json.dumps({\n        \"name\": \"UKG DV Export Pay Code\",\n        \"payLoad\": {\n            \"from\": {\n                \"view\": 0,\n                \"employeeSet\": {\n                    \"hyperfind\": {\"id\": \"1304\"},\n                    \"dateRange\": {\n                        \"startDate\": week_start_datetime.strftime(\"%Y-%m-%d\"),\n                        \"endDate\": week_end_datetime.strftime(\"%Y-%m-%d\"),\n                    }\n                },\n                \"viewPresentation\": \"People\"\n            },\n            \"select\": [\n                {\"key\": \"PEOPLE_PERSON_NUMBER\", \"alias\": \"Employee ID\", ...},\n                {\"key\": \"CORE_PAYCODE\", \"alias\": \"Pay Code Name\", ...},\n                {\"key\": \"TIMECARD_TRANS_ACTUAL_HOURS\", \"alias\": \"Actual Hours\", ...},\n                # ... more fields ...\n            ],\n            \"groupBy\": [],\n            \"where\": [],\n        },\n        \"type\": \"DATA\"\n    })\n    # 1. Trigger DataView export\n    response = requests.post(export_url, headers=headers, data=payload)\n    execution_key = response.json()['executionKey']\n    # 2. Wait for export to complete\n    time.sleep(60)\n    # 3. Download the CSV file\n    csv_export_url = base_uri + f'api/v1/commons/exports/{execution_key}/file'\n    response = requests.get(csv_export_url, headers=headers)\n    temp_file_path = '/tmp/paycodeextracttempdetail.csv'\n    with open(temp_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(response.text)\n    # ... process CSV as needed ...\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_1","title":"Explanation","text":"<ul> <li>Trigger DataView Export: Sends a POST request to the DataView export endpoint with the required payload (including Hyperfind and date range).</li> <li>Wait for Completion: Waits for the export to complete (can be improved with polling).</li> <li>Download CSV: Downloads the resulting CSV file for further processing.</li> <li>Processing: The CSV can be parsed and loaded into a database or used for reporting.</li> </ul> <p>Reference: - UKG Dimensions API Documentation \u2013 DataViews</p>"},{"location":"ukg-api-integration-and-dataview/#3-orchestrating-the-process-process_and_validate_payroll_hours","title":"3. Orchestrating the Process: <code>process_and_validate_payroll_hours</code>","text":"<p>This function coordinates the two previous steps, automating the extraction of both payroll export and DataView data for a given period. It uses the <code>fetch_period</code> function to retrieve the start and end dates for both the prior and current pay periods, and then passes these as parameters to the extraction functions.</p>"},{"location":"ukg-api-integration-and-dataview/#python-function-process_and_validate_payroll_hours-with-prior_period-and-current_period-parameters","title":"Python Function: <code>process_and_validate_payroll_hours</code> (with <code>prior_period</code> and <code>current_period</code> parameters)","text":"<pre><code>def process_and_validate_payroll_hours(environment='PROD'):\n    \"\"\"\n    Main function to process and validate payroll hours.\n    Steps:\n    1. Fetches the prior period data and stores payroll hours.\n    2. Fetches and stores hours using dataview for the prior period.\n    3. Fetches the current period data and stores hours using dataview.\n    \"\"\"\n    # Get prior period (returns tuple: start_date, end_date, start_datetime, end_datetime)\n    prior_period = fetch_period('Previous', environment)\n    # Extract and store payroll hours for prior period\n    fetch_and_store_payroll_hours(environment, *prior_period)\n    # Extract and store DataView hours for prior period\n    fetch_and_store_hours_using_dataview(environment, prior_period[2], prior_period[3])\n\n    # Get current period\n    current_period = fetch_period('Current', environment)\n    # Extract and store DataView hours for current period\n    fetch_and_store_hours_using_dataview(environment, current_period[2], current_period[3])\n\n    # Optionally, add validation or reporting here\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#about-prior_period-and-current_period","title":"About <code>prior_period</code> and <code>current_period</code>","text":"<p>The <code>fetch_period</code> function returns a tuple for each period:</p> <ul> <li><code>start_date</code> (str): Start date of the pay period (e.g., '2025-06-01')</li> <li><code>end_date</code> (str): End date of the pay period (e.g., '2025-06-15')</li> <li><code>start_datetime</code> (datetime): Start date as a Python <code>datetime</code> object</li> <li><code>end_datetime</code> (datetime): End date as a Python <code>datetime</code> object</li> </ul> <p>These are used as parameters for the extraction functions:</p> <ul> <li><code>fetch_and_store_payroll_hours(environment, start_date, end_date, start_datetime, end_datetime)</code></li> <li><code>fetch_and_store_hours_using_dataview(environment, start_datetime, end_datetime)</code></li> </ul> <p>This approach ensures that all data extraction is aligned to the correct pay periods, and makes it easy to extend the process for additional periods or custom ranges.</p>"},{"location":"ukg-api-integration-and-dataview/#example-usage","title":"Example Usage","text":"<pre><code>if __name__ == \"__main__\":\n    process_and_validate_payroll_hours(environment=\"PROD\")\n</code></pre>"},{"location":"ukg-api-integration-and-dataview/#explanation_2","title":"Explanation","text":"<ul> <li>Fetch Periods: Retrieves the date ranges for the previous and current pay periods using <code>fetch_period</code>.</li> <li>Extract Data: Calls the two extraction functions for each period, passing the correct parameters.</li> <li>Validation: (Optional) You can add logic to compare and validate the extracted data.</li> </ul>"},{"location":"ukg-api-integration-and-dataview/#conclusion","title":"Conclusion","text":"<p>By leveraging the UKG Dimensions API, you can automate the execution of predefined integrations and the extraction of DataView data. This enables robust, repeatable, and auditable data flows for payroll, compliance, and analytics.</p> <p>For more details, consult the official UKG Dimensions API Documentation or your UKG support representative.</p>"},{"location":"ukg-sftp-file-transfer/","title":"Secure File Transfer with UKG Dimensions SFTP","text":""},{"location":"ukg-sftp-file-transfer/#introduction","title":"Introduction","text":"<p>UKG Dimensions (Kronos) provides SFTP endpoints for secure file exchange. For additional security, files are often encrypted (e.g., with PGP/GPG) before upload and must be decrypted after download. This article demonstrates how to:</p> <ul> <li>Connect to a UKG SFTP server using Python</li> <li>Download and decrypt files from UKG</li> <li>Encrypt and upload files to UKG</li> <li>Use Azure Storage (or any local mount) as your working directory</li> <li>Securely manage all credentials and keys using Azure Key Vault</li> <li>Import and manage GPG keys for file encryption/decryption</li> </ul> <p>We use Python libraries such as <code>pysftp</code>, <code>paramiko</code>, <code>gnupg</code>, and Azure SDKs to accomplish these tasks.</p>"},{"location":"ukg-sftp-file-transfer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>The following Python packages:</li> <li><code>pysftp</code> (SFTP client)</li> <li><code>paramiko</code> (SSH key handling)</li> <li><code>python-gnupg</code> (PGP encryption/decryption)</li> <li><code>azure-identity</code>, <code>azure-keyvault-secrets</code> (Azure Key Vault access)</li> <li>Access to your UKG SFTP credentials and keys (public/private, passphrase)</li> <li>GPG/PGP keys for file encryption/decryption</li> <li>A local directory (e.g., Azure Storage mount) for file staging</li> <li>Azure Key Vault for Secrets: Store all sensitive credentials (SFTP username, private key, passphrase, GPG passphrase, etc.) in Azure Key Vault and retrieve them securely at runtime. This avoids hardcoding secrets in your code or environment variables.</li> </ul> <p>Install dependencies: <pre><code>pip install pysftp paramiko python-gnupg azure-identity azure-keyvault-secrets\n</code></pre></p>"},{"location":"ukg-sftp-file-transfer/#key-constants-and-their-secure-retrieval","title":"Key Constants and Their Secure Retrieval","text":"<p>All SFTP credentials and keys are securely retrieved from Azure Key Vault using the <code>get_azure_kv_sceret</code> function. Here are the main constants and how they are constructed:</p> <ul> <li>SFTP_UKG_DATA_HOST_NAME: The SFTP server hostname (e.g., <code>'your-ukg-sftp-host.com'</code>).</li> <li>SFTP_UKG_DATA_USER_NAME: The SFTP username.</li> <li>SFTP_UKG_PUBLIC_KEY: The SFTP server's public key, retrieved and decoded as bytes:   <pre><code>SFTP_UKG_PUBLIC_KEY = bytes(get_azure_kv_sceret('ukg-sftp-host-public-key'), encoding='utf-8')\n</code></pre></li> <li>SFTP_UKG_PRIVATE_KEY: The private key for SFTP authentication, retrieved as a base64-encoded string from Key Vault, then decoded to a PEM string:   <pre><code>SFTP_UKG_PRIVATE_KEY = base64.b64decode(get_azure_kv_sceret('ukg-sftp-host-private-key')).decode('utf-8')\n</code></pre> Why base64.b64decode? <p>When storing sensitive files like private keys in Azure Key Vault, it is common to first encode them using Base64. This ensures the key is stored as a plain string (since Key Vault secrets are always strings) and avoids issues with special characters or line breaks. When retrieving the key, you must decode it back to its original binary (or PEM) format using <code>base64.b64decode</code>. This allows you to safely store and retrieve binary data (like private keys) in a text-only secret store.</p> </li> <li>SFTP_UKG_PRIVATE_KEY_PASSPHRASE: The passphrase for the private key, also retrieved from Key Vault.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#how-get_azure_kv_sceret-works","title":"How <code>get_azure_kv_sceret</code> Works","text":"<p>This function retrieves secrets from Azure Key Vault using the Azure SDK. It authenticates using environment variables for client ID, client secret, and tenant ID, then fetches the secret value by name:</p> <pre><code>def get_azure_kv_sceret(name):\n    secret = None\n    try:\n        vault_url = \"https://&lt;your-key-vault-name&gt;.vault.azure.net/\"\n        client_id = os.environ.get(\"application_interface_clientid\")\n        client_secret = os.environ.get(\"application_interface_clientsecret\")\n        tenant_id = os.environ.get(\"application_interface_tenantid\")\n        credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)\n        secret_client = SecretClient(vault_url=vault_url, credential=credential)\n        retrieved_secret = secret_client.get_secret(name)\n        secret = retrieved_secret.value\n    except Exception as e:\n        print(\"Error:\", str(e))\n    finally:\n        return secret\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#importing-and-managing-gpg-keys-for-ukg-file-encryptiondecryption","title":"Importing and Managing GPG Keys for UKG File Encryption/Decryption","text":"<p>When your container is first provisioned, you should import the GPG keys required for file encryption and decryption. The following function, typically run at container startup, retrieves the GPG public and private keys from Azure Key Vault, decodes them, saves them to disk, and imports them into the GPG keyring:</p> <pre><code>def download_and_import_gpg_keys():\n    try:\n        ukg_sftp_public_encrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-public-encrypt')).decode('utf-8')\n        ukg_sftp_private_decrypt = base64.b64decode(get_azure_kv_sceret('ukg-sftp-private-decrypt')).decode('utf-8')\n        # Save public key\n        public_key_file = \"certs/public_ukg_encrypt.asc\"\n        with open(public_key_file, \"w\") as file:\n            file.write(ukg_sftp_public_encrypt)\n        # Save private key\n        private_key_file = \"certs/private_ukg_decrypt.asc\"\n        with open(private_key_file, \"w\") as file:\n            file.write(ukg_sftp_private_decrypt)\n        # Import the public key\n        subprocess.run([\"gpg\", \"--batch\", \"--yes\", \"--import\", public_key_file], check=True)\n        # Import the private key with passphrase\n        subprocess.run([\n            \"gpg\", \"--batch\", \"--yes\", \"--pinentry-mode=loopback\",\n            \"--passphrase\", os.environ[\"ukg_encrypt_passphrase\"],\n            \"--import\", private_key_file\n        ], check=True)\n        os.remove(public_key_file)\n        os.remove(private_key_file)\n    except Exception as e:\n        print(f\"Failed to import GPG keys: {e}\")\n</code></pre> <p>When to run this: - Run this function once at container startup (or as part of your provisioning script) to ensure the GPG keys are available for all encryption/decryption operations in your UKG SFTP workflows. - This ensures that all subsequent file transfers (upload/download) can use GPG seamlessly at the OS level.</p>"},{"location":"ukg-sftp-file-transfer/#sftp-connection-function-with-explanation","title":"SFTP Connection Function (with Explanation)","text":"<p>The following function establishes a secure SFTP connection to the UKG server using all the above constants. It supports both production and non-production environments:</p> <pre><code>import pysftp\nimport paramiko\nimport base64\nimport io\nimport warnings\n\ndef get_sftp_connection(environment: str) -&gt; Optional[pysftp.Connection]:\n    \"\"\"\n    Establish an SFTP connection to the UKG server.\n    Returns a pysftp.Connection object if successful, otherwise None.\n    \"\"\"\n    localConnection: Optional[pysftp.Connection] = None\n    try:\n        hostname = (SFTP_UKG_DATA_HOST_NAME if environment == 'PROD' else SFTP_UKG_DATA_HOST_NAME_NON_PROD)\n        username = (SFTP_UKG_DATA_USER_NAME if environment == 'PROD' else SFTP_UKG_DATA_USER_NAME_NON_PROD)\n        hostkey = (SFTP_UKG_PUBLIC_KEY if environment == 'PROD' else SFTP_UKG_PUBLIC_KEY_NON_PROD)\n        warnings.filterwarnings('ignore', '.*Failed to load HostKeys.*')\n        hostKey = paramiko.RSAKey(data=base64.decodebytes(hostkey))\n        cnopts = pysftp.CnOpts()\n        cnopts.hostkeys.add(hostname, 'ssh-rsa', hostKey)\n        # Convert private key string to file-like object\n        with io.StringIO(SFTP_UKG_PRIVATE_KEY) as private_key_file:\n            private_key = paramiko.RSAKey.from_private_key(private_key_file, password=SFTP_UKG_PRIVATE_KEY_PASSPHRASE)\n            localConnection = pysftp.Connection(host=hostname, username=username, private_key=private_key, cnopts=cnopts)\n    except Exception as e:\n        print(f\"SFTP connection failed: {e}\")\n    return localConnection\n</code></pre> <p>This function: - Retrieves all connection parameters and keys from Azure Key Vault. - Decodes and loads the SFTP server's public key for host verification. - Loads the private key and passphrase for authentication. - Returns a live SFTP connection object for use in upload/download operations.</p>"},{"location":"ukg-sftp-file-transfer/#downloading-and-decrypting-files-from-ukg","title":"Downloading and Decrypting Files from UKG","text":"<p>UKG may deliver files encrypted with PGP/GPG. Use <code>python-gnupg</code> to decrypt after download.</p> <pre><code>import gnupg\n\ngpg = gnupg.GPG()\nLOCAL_DOWNLOAD_DIR = '/mnt/azure/UKG/Download/'  # Example: Azure Storage mount\nREMOTE_UKG_FOLDER = './Outbound/'\n\ndef download_and_decrypt_files():\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(REMOTE_UKG_FOLDER)\n        for filename in sftp.listdir():\n            if filename.endswith('.gpg'):\n                local_path = os.path.join(LOCAL_DOWNLOAD_DIR, filename)\n                sftp.get(filename, local_path)\n                print(f\"Downloaded: {filename}\")\n                # Decrypt the file\n                with open(local_path, 'rb') as f:\n                    decrypted_data = gpg.decrypt_file(f, passphrase=os.environ[\"ukg_encrypt_passphrase\"])\n                if decrypted_data.ok:\n                    decrypted_path = local_path.replace('.gpg', '')\n                    with open(decrypted_path, 'w', encoding='utf-8') as out:\n                        out.write(str(decrypted_data))\n                    print(f\"Decrypted: {decrypted_path}\")\n                else:\n                    print(f\"Decryption failed for {filename}: {decrypted_data.status}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#encrypting-and-uploading-files-to-ukg","title":"Encrypting and Uploading Files to UKG","text":"<p>Before uploading, files must be encrypted with UKG's public key.</p> <pre><code>def encrypt_and_upload_file(local_file, remote_folder='./Inbound/'):\n    with open(local_file, 'rb') as f:\n        encrypted_data = gpg.encrypt_file(\n            f,\n            recipients=['UKG_PUBLIC_KEY_NAME'],  # Replace with UKG's GPG key name\n            always_trust=True\n        )\n    if not encrypted_data.ok:\n        print(f\"Encryption failed: {encrypted_data.status}\")\n        return\n    encrypted_file = local_file + '.gpg'\n    with open(encrypted_file, 'wb') as ef:\n        ef.write(encrypted_data.data)\n    print(f\"Encrypted: {encrypted_file}\")\n    with get_sftp_connection('PROD') as sftp:\n        sftp.cwd(remote_folder)\n        sftp.put(encrypted_file)\n        print(f\"Uploaded: {encrypted_file} to {remote_folder}\")\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#putting-it-all-together","title":"Putting It All Together","text":"<p>You can automate the full workflow:</p> <pre><code>def main():\n    download_and_import_gpg_keys()  # Ensure GPG keys are imported at container startup\n    download_and_decrypt_files()\n    file_to_upload = '/mnt/azure/UKG/Upload/myfile.csv'\n    encrypt_and_upload_file(file_to_upload)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ukg-sftp-file-transfer/#key-points-and-best-practices","title":"Key Points and Best Practices","text":"<ul> <li>Key Management: Never hardcode sensitive keys or passphrases in your code. Use environment variables or a secure vault.</li> <li>File Cleanup: Remove decrypted/encrypted files after processing if not needed.</li> <li>Error Handling: Add robust error handling for production use.</li> <li>Azure Storage: If using Azure Files, ensure your container mounts the share with correct permissions.</li> <li>Security: Only trust files from known sources and validate signatures if possible.</li> <li>GPG Key Import: Always import GPG keys at the OS level before running any file encryption/decryption operations.</li> </ul>"},{"location":"ukg-sftp-file-transfer/#references","title":"References","text":"<ul> <li>pysftp Documentation</li> <li>python-gnupg Documentation</li> <li>UKG Dimensions Integration Guides</li> <li>Azure Key Vault Documentation</li> </ul>"}]}